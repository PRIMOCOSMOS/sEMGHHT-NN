{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sEMG-HHT åŒåˆ†ç±»å™¨ç³»ç»Ÿ | sEMG-HHT Dual Classifier System",
    "",
    "## ç³»ç»Ÿæ¶æ„ | System Architecture",
    "",
    "æœ¬ç¬”è®°æœ¬å®ç°äº†åŒåˆ†ç±»å™¨ç³»ç»Ÿï¼š",
    "This notebook implements a dual classifier system:",
    "",
    "1. **æ·±åº¦å­¦ä¹  CNN** - åŠ¨ä½œè´¨é‡åˆ†ç±»ï¼ˆå…¨ç¨‹ã€åŠç¨‹ã€æ— æ•ˆï¼‰",
    "   **Deep Learning CNN** - Action Quality Classification (Full, Half, Invalid)",
    "   ",
    "2. **SVM åˆ†ç±»å™¨** - æ€§åˆ«åˆ†ç±»ï¼ˆç”·æ€§ã€å¥³æ€§ï¼‰",
    "   **SVM Classifier** - Gender Classification (Male, Female)",
    "",
    "## å…³é”®æ”¹è¿› | Key Improvements",
    "",
    "### è§£å†³è®­ç»ƒé—®é¢˜ | Training Problem Solutions:",
    "- âœ… **æ‰©å±•çš„CNNæ¶æ„** - 7å±‚æ·±åº¦ç½‘ç»œï¼Œæ›´å¼ºçš„ç‰¹å¾æå–èƒ½åŠ›",
    "  **Expanded CNN Architecture** - 7-layer deep network with stronger feature extraction",
    "  ",
    "- âœ… **æ‰¹å½’ä¸€åŒ–** - åŠ é€Ÿè®­ç»ƒï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±",
    "  **Batch Normalization** - Accelerate training, prevent vanishing gradients",
    "  ",
    "- âœ… **Kaimingåˆå§‹åŒ–** - æ­£ç¡®çš„æƒé‡åˆå§‹åŒ–ï¼Œç¡®ä¿æ¢¯åº¦æµåŠ¨",
    "  **Kaiming Initialization** - Proper weight init for gradient flow",
    "  ",
    "- âœ… **å­¦ä¹ ç‡é¢„çƒ­** - é˜²æ­¢è®­ç»ƒåˆæœŸä¸ç¨³å®š",
    "  **Learning Rate Warmup** - Prevent early training instability",
    "  ",
    "- âœ… **æ¢¯åº¦è£å‰ª** - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸",
    "  **Gradient Clipping** - Prevent gradient explosion",
    "  ",
    "- âœ… **æ•°æ®å¢å¼º** - æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›",
    "  **Data Augmentation** - Improve model generalization",
    "",
    "### ç½‘ç»œè§„æ¨¡ | Network Scale:",
    "- **è¾“å…¥** Input: 1Ã—256Ã—256",
    "- **é€šé“æ•°** Channels: 64 â†’ 128 â†’ 256 â†’ 512 â†’ 1024 â†’ 2048 â†’ 2048",
    "- **ç‰¹å¾ç»´åº¦** Feature dim: 2048",
    "- **åˆ†ç±»å¤´** Classifier: 2048 â†’ 1024 â†’ 512 â†’ 3 classes",
    "",
    "## æ•°æ®è¦æ±‚ | Data Requirements",
    "",
    "- **æ ¼å¼** Format: `.npz` æ–‡ä»¶ï¼ŒåŒ…å« 256Ã—256 HHT çŸ©é˜µ",
    "- **é€šé“** Channels: å•é€šé“ï¼ˆç°åº¦å›¾ï¼‰",
    "- **å‘½åè§„åˆ™** Naming: `è‚Œè‚‰å_åŠ¨ä½œ_æ€§åˆ«_ç¼–å·.npz`",
    "  - ä¾‹å¦‚ Example: `BICEPS_fatiguetest_M_006.npz` (ç”·æ€§ï¼Œå…¨ç¨‹åŠ¨ä½œ)",
    "  - ä¾‹å¦‚ Example: `TRICEPS_half_F_012.npz` (å¥³æ€§ï¼ŒåŠç¨‹åŠ¨ä½œ)",
    "  - æµ‹è¯•æ–‡ä»¶ Test files: ä»¥ `Test` å¼€å¤´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒé…ç½® | Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os",
    "import sys",
    "",
    "# æ£€æµ‹Kaggleç¯å¢ƒ | Detect Kaggle environment",
    "IS_KAGGLE = os.path.exists('/kaggle/input')",
    "",
    "if IS_KAGGLE:",
    "    DATA_DIR = '/kaggle/input/hilbertmatrix-npz/hht_matrices'",
    "    CHECKPOINT_DIR = '/kaggle/working/checkpoints'",
    "    print('ğŸƒ åœ¨Kaggleä¸Šè¿è¡Œ | Running on Kaggle')",
    "    print(f'ğŸ“ æ•°æ®ç›®å½• | Data directory: {DATA_DIR}')",
    "else:",
    "    DATA_DIR = './data'",
    "    CHECKPOINT_DIR = './checkpoints'",
    "    print('ğŸ’» æœ¬åœ°è¿è¡Œ | Running locally')",
    "    print(f'ğŸ“ æ•°æ®ç›®å½• | Data directory: {DATA_DIR}')",
    "",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)",
    "print(f'ğŸ’¾ æ£€æŸ¥ç‚¹ç›®å½• | Checkpoint directory: {CHECKPOINT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å¯¼å…¥ä¾èµ– | Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch",
    "import torch.nn as nn",
    "import torch.nn.functional as F",
    "import numpy as np",
    "import glob",
    "import re",
    "from sklearn.svm import SVC",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder",
    "from sklearn.model_selection import train_test_split",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score",
    "import matplotlib.pyplot as plt",
    "import warnings",
    "from typing import Tuple, List, Dict, Optional",
    "import pickle",
    "from tqdm.auto import tqdm",
    "",
    "warnings.filterwarnings('ignore')",
    "",
    "# è®¾ç½®éšæœºç§å­ | Set random seeds",
    "SEED = 42",
    "torch.manual_seed(SEED)",
    "np.random.seed(SEED)",
    "if torch.cuda.is_available():",
    "    torch.cuda.manual_seed_all(SEED)",
    "    torch.backends.cudnn.deterministic = True",
    "    torch.backends.cudnn.benchmark = False",
    "",
    "# æ£€æµ‹è®¾å¤‡ | Detect device",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
    "print(f'ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡ | Using device: {device}')",
    "if torch.cuda.is_available():",
    "    print(f'   GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è¶…å‚æ•°é…ç½® | Hyperparameter Configuration",
    "",
    "åœ¨æ­¤é…ç½®æ‰€æœ‰è®­ç»ƒå‚æ•°ã€‚æ ¹æ®éœ€è¦è°ƒæ•´è¿™äº›å€¼ã€‚",
    "Configure all training parameters here. Adjust these values as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================",
    "# è¶…å‚æ•°é…ç½® | HYPERPARAMETER CONFIGURATION",
    "# =============================================================================",
    "",
    "# -----------------------------------------------------------------------------",
    "# æ¨¡å‹æ¶æ„ | Model Architecture",
    "# -----------------------------------------------------------------------------",
    "MODEL_IN_CHANNELS = 1              # è¾“å…¥é€šé“æ•°ï¼ˆç°åº¦å›¾ï¼‰| Input channels (grayscale)",
    "MODEL_BASE_CHANNELS = 64           # åŸºç¡€é€šé“æ•° | Base channels",
    "MODEL_NUM_LAYERS = 7               # å·ç§¯å±‚æ•°ï¼ˆæ‰©å±•è‡³7å±‚ï¼‰| Number of conv layers (expanded to 7)",
    "MODEL_DROPOUT_RATE = 0.5           # Dropoutç‡ | Dropout rate",
    "",
    "# -----------------------------------------------------------------------------",
    "# åŠ¨ä½œè´¨é‡CNNè®­ç»ƒé…ç½® | Action Quality CNN Training Config",
    "# -----------------------------------------------------------------------------",
    "ACTION_EPOCHS = 100                # è®­ç»ƒè½®æ•° | Training epochs",
    "ACTION_BATCH_SIZE = 16             # æ‰¹æ¬¡å¤§å° | Batch size",
    "ACTION_LEARNING_RATE = 0.0001      # å­¦ä¹ ç‡ï¼ˆé™ä½ä»¥æé«˜ç¨³å®šæ€§ï¼‰| Learning rate (lowered for stability)",
    "ACTION_WEIGHT_DECAY = 1e-4         # L2æ­£åˆ™åŒ– | L2 regularization",
    "ACTION_WARMUP_EPOCHS = 5           # å­¦ä¹ ç‡é¢„çƒ­è½®æ•° | LR warmup epochs",
    "ACTION_GRAD_CLIP = 1.0             # æ¢¯åº¦è£å‰ªå€¼ | Gradient clipping value",
    "",
    "# -----------------------------------------------------------------------------",
    "# å­¦ä¹ ç‡è°ƒåº¦å™¨ | Learning Rate Scheduler",
    "# -----------------------------------------------------------------------------",
    "LR_SCHEDULER_FACTOR = 0.5          # å­¦ä¹ ç‡è¡°å‡å› å­ | LR decay factor",
    "LR_SCHEDULER_PATIENCE = 7          # ç­‰å¾…è½®æ•° | Patience epochs",
    "LR_SCHEDULER_MIN_LR = 1e-7         # æœ€å°å­¦ä¹ ç‡ | Minimum LR",
    "",
    "# -----------------------------------------------------------------------------",
    "# SVMé…ç½® | SVM Configuration",
    "# -----------------------------------------------------------------------------",
    "SVM_KERNEL = 'rbf'                 # SVMæ ¸å‡½æ•° | SVM kernel",
    "SVM_C = 10.0                       # æ­£åˆ™åŒ–å‚æ•° | Regularization parameter",
    "SVM_GAMMA = 'scale'                # Gammaå‚æ•° | Gamma parameter",
    "",
    "# -----------------------------------------------------------------------------",
    "# æ•°æ®é…ç½® | Data Configuration",
    "# -----------------------------------------------------------------------------",
    "DATA_NORMALIZE = True              # æ•°æ®å½’ä¸€åŒ– | Data normalization",
    "DATA_TEST_SIZE = 0.2               # éªŒè¯é›†æ¯”ä¾‹ | Validation split ratio",
    "DATA_AUGMENTATION = True           # æ•°æ®å¢å¼º | Data augmentation",
    "",
    "# -----------------------------------------------------------------------------",
    "# æ£€æŸ¥ç‚¹é…ç½® | Checkpoint Configuration",
    "# -----------------------------------------------------------------------------",
    "CHECKPOINT_INTERVAL = 10           # æ£€æŸ¥ç‚¹ä¿å­˜é—´éš” | Checkpoint save interval",
    "",
    "print('='*80)",
    "print('è¶…å‚æ•°é…ç½® | HYPERPARAMETER CONFIGURATION')",
    "print('='*80)",
    "print(f'\\nğŸ“ æ¨¡å‹æ¶æ„ | Model Architecture:')",
    "print(f'   è¾“å…¥é€šé“ Input channels: {MODEL_IN_CHANNELS}')",
    "print(f'   åŸºç¡€é€šé“ Base channels: {MODEL_BASE_CHANNELS}')",
    "print(f'   ç½‘ç»œå±‚æ•° Network layers: {MODEL_NUM_LAYERS}')",
    "print(f'   Dropoutç‡ Dropout rate: {MODEL_DROPOUT_RATE}')",
    "print(f'\\nğŸ¯ åŠ¨ä½œè´¨é‡è®­ç»ƒ | Action Quality Training:')",
    "print(f'   è®­ç»ƒè½®æ•° Epochs: {ACTION_EPOCHS}')",
    "print(f'   æ‰¹æ¬¡å¤§å° Batch size: {ACTION_BATCH_SIZE}')",
    "print(f'   å­¦ä¹ ç‡ Learning rate: {ACTION_LEARNING_RATE}')",
    "print(f'   æƒé‡è¡°å‡ Weight decay: {ACTION_WEIGHT_DECAY}')",
    "print(f'   é¢„çƒ­è½®æ•° Warmup epochs: {ACTION_WARMUP_EPOCHS}')",
    "print(f'   æ¢¯åº¦è£å‰ª Gradient clipping: {ACTION_GRAD_CLIP}')",
    "print(f'\\nğŸ”§ SVMé…ç½® | SVM Configuration:')",
    "print(f'   æ ¸å‡½æ•° Kernel: {SVM_KERNEL}')",
    "print(f'   Cå‚æ•° C: {SVM_C}')",
    "print(f'   Gamma: {SVM_GAMMA}')",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ¨¡å‹æ¶æ„ | Model Architecture",
    "",
    "### æ‰©å±•çš„CNNç¼–ç å™¨ï¼ˆ7å±‚ï¼‰| Expanded CNN Encoder (7 layers)",
    "",
    "è§£å†³è®­ç»ƒé—®é¢˜çš„å…³é”®æ”¹è¿›ï¼š",
    "Key improvements to solve training issues:",
    "",
    "1. **æ›´æ·±çš„ç½‘ç»œ**ï¼š7å±‚å·ç§¯ï¼Œæå–æ›´å¤æ‚çš„ç‰¹å¾",
    "   **Deeper network**: 7 conv layers for more complex features",
    "   ",
    "2. **æ‰¹å½’ä¸€åŒ–**ï¼šæ¯å±‚åæ·»åŠ BNï¼ŒåŠ é€Ÿæ”¶æ•›",
    "   **Batch Normalization**: BN after each layer, faster convergence",
    "   ",
    "3. **Kaimingåˆå§‹åŒ–**ï¼šé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸",
    "   **Kaiming Init**: Prevent vanishing/exploding gradients",
    "   ",
    "4. **æ®‹å·®è¿æ¥**ï¼šæ”¹å–„æ¢¯åº¦æµåŠ¨",
    "   **Residual connections**: Better gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedConvBlock(nn.Module):",
    "    \"\"\"",
    "    æ”¹è¿›çš„å·ç§¯å—ï¼ŒåŒ…å«BatchNormå’Œæ®‹å·®è¿æ¥",
    "    Improved conv block with BatchNorm and residual connection",
    "    \"\"\"",
    "    def __init__(self, in_channels: int, out_channels: int, ",
    "                 kernel_size: int = 3, stride: int = 2, padding: int = 1,",
    "                 use_residual: bool = False):",
    "        super(ImprovedConvBlock, self).__init__()",
    "        ",
    "        self.conv = nn.Conv2d(in_channels, out_channels, ",
    "                             kernel_size=kernel_size, ",
    "                             stride=stride, ",
    "                             padding=padding,",
    "                             bias=False)  # BNåä¸éœ€è¦bias",
    "        self.bn = nn.BatchNorm2d(out_channels)",
    "        self.activation = nn.LeakyReLU(0.2, inplace=True)",
    "        ",
    "        self.use_residual = use_residual and (in_channels == out_channels) and (stride == 1)",
    "        ",
    "        # Kaimingåˆå§‹åŒ– | Kaiming initialization",
    "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='leaky_relu')",
    "        ",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:",
    "        identity = x",
    "        ",
    "        out = self.conv(x)",
    "        out = self.bn(out)",
    "        out = self.activation(out)",
    "        ",
    "        if self.use_residual:",
    "            out = out + identity",
    "            ",
    "        return out",
    "",
    "",
    "class ExpandedCNNEncoder(nn.Module):",
    "    \"\"\"",
    "    æ‰©å±•çš„7å±‚CNNç¼–ç å™¨ï¼Œç”¨äºç‰¹å¾æå–",
    "    Expanded 7-layer CNN encoder for feature extraction",
    "    ",
    "    æ¶æ„ | Architecture:",
    "    - 7ä¸ªå·ç§¯å—ï¼Œé€šé“æ•°é€’å¢ï¼š64 â†’ 128 â†’ 256 â†’ 512 â†’ 1024 â†’ 2048 â†’ 2048",
    "    - æ¯å—åŒ…å«ï¼šConv2d + BatchNorm + LeakyReLU",
    "    - å…¨å±€å¹³å‡æ± åŒ–è¾“å‡º2048ç»´ç‰¹å¾å‘é‡",
    "    ",
    "    Input: (B, 1, 256, 256)",
    "    Output: (B, 2048)",
    "    \"\"\"",
    "    def __init__(self, in_channels: int = 1, base_channels: int = 64):",
    "        super(ExpandedCNNEncoder, self).__init__()",
    "        ",
    "        # å®šä¹‰é€šé“æ•°åºåˆ— | Define channel progression",
    "        channels = [base_channels * (2**i) for i in range(6)]  # [64, 128, 256, 512, 1024, 2048]",
    "        channels.append(2048)  # ä¿æŒæœ€åä¸¤å±‚é€šé“æ•°ç›¸åŒ | Keep last two layers same",
    "        ",
    "        # æ„å»ºç¼–ç å™¨å±‚ | Build encoder layers",
    "        layers = []",
    "        current_channels = in_channels",
    "        ",
    "        for i, out_channels in enumerate(channels):",
    "            # å‰5å±‚ä½¿ç”¨stride=2ä¸‹é‡‡æ ·ï¼Œå2å±‚ä½¿ç”¨stride=1",
    "            stride = 2 if i < 5 else 1",
    "            use_residual = (i >= 5)  # æœ€åä¸¤å±‚ä½¿ç”¨æ®‹å·®è¿æ¥",
    "            ",
    "            layers.append(ImprovedConvBlock(",
    "                in_channels=current_channels,",
    "                out_channels=out_channels,",
    "                kernel_size=3,",
    "                stride=stride,",
    "                padding=1,",
    "                use_residual=use_residual",
    "            ))",
    "            current_channels = out_channels",
    "        ",
    "        self.encoder = nn.Sequential(*layers)",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))",
    "        self.feature_dim = channels[-1]",
    "        ",
    "        print(f'âœ… ç¼–ç å™¨å·²åˆ›å»º | Encoder created:')",
    "        print(f'   å±‚æ•° Layers: {len(channels)}')",
    "        print(f'   é€šé“åºåˆ— Channels: {in_channels} â†’ {\" â†’ \".join(map(str, channels))}')",
    "        print(f'   è¾“å‡ºç‰¹å¾ç»´åº¦ Output feature dim: {self.feature_dim}')",
    "    ",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:",
    "        \"\"\"æå–ç‰¹å¾ | Extract features\"\"\"",
    "        x = self.encoder(x)",
    "        x = self.global_avg_pool(x)",
    "        x = x.view(x.size(0), -1)",
    "        return x",
    "    ",
    "    def get_feature_dim(self) -> int:",
    "        \"\"\"è¿”å›ç‰¹å¾ç»´åº¦ | Return feature dimension\"\"\"",
    "        return self.feature_dim",
    "",
    "",
    "class ActionQualityCNN(nn.Module):",
    "    \"\"\"",
    "    åŠ¨ä½œè´¨é‡åˆ†ç±»å™¨ï¼ˆæ·±åº¦å­¦ä¹ ï¼‰",
    "    Action Quality Classifier (Deep Learning)",
    "    ",
    "    3ä¸ªç±»åˆ« | 3 classes:",
    "    - 0: Full (å…¨ç¨‹)",
    "    - 1: Half (åŠç¨‹)",
    "    - 2: Invalid (æ— æ•ˆ)",
    "    \"\"\"",
    "    def __init__(self, encoder: ExpandedCNNEncoder, n_classes: int = 3, dropout_rate: float = 0.5):",
    "        super(ActionQualityCNN, self).__init__()",
    "        ",
    "        self.encoder = encoder",
    "        feature_dim = encoder.get_feature_dim()",
    "        ",
    "        # 3å±‚åˆ†ç±»å¤´ï¼Œé€æ­¥é™ç»´ | 3-layer classification head with gradual dimension reduction",
    "        self.classifier = nn.Sequential(",
    "            nn.Linear(feature_dim, 1024),",
    "            nn.BatchNorm1d(1024),",
    "            nn.ReLU(inplace=True),",
    "            nn.Dropout(dropout_rate),",
    "            ",
    "            nn.Linear(1024, 512),",
    "            nn.BatchNorm1d(512),",
    "            nn.ReLU(inplace=True),",
    "            nn.Dropout(dropout_rate),",
    "            ",
    "            nn.Linear(512, n_classes)",
    "        )",
    "        ",
    "        # åˆå§‹åŒ–åˆ†ç±»å¤´ | Initialize classifier head",
    "        for m in self.classifier.modules():",
    "            if isinstance(m, nn.Linear):",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')",
    "                if m.bias is not None:",
    "                    nn.init.constant_(m.bias, 0)",
    "        ",
    "        print(f'âœ… åŠ¨ä½œè´¨é‡åˆ†ç±»å™¨å·²åˆ›å»º | Action Quality Classifier created:')",
    "        print(f'   è¾“å…¥ç‰¹å¾ç»´åº¦ Input feature dim: {feature_dim}')",
    "        print(f'   åˆ†ç±»å¤´ç»“æ„ Classifier: {feature_dim} â†’ 1024 â†’ 512 â†’ {n_classes}')",
    "        print(f'   Dropoutç‡ Dropout rate: {dropout_rate}')",
    "    ",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:",
    "        \"\"\"å‰å‘ä¼ æ’­ | Forward pass\"\"\"",
    "        features = self.encoder(x)",
    "        logits = self.classifier(features)",
    "        return logits",
    "    ",
    "    def extract_features(self, x: torch.Tensor) -> torch.Tensor:",
    "        \"\"\"ä»…æå–ç‰¹å¾ï¼Œç”¨äºSVM | Extract features only, for SVM\"\"\"",
    "        return self.encoder(x)",
    "",
    "",
    "print('\\nâœ… æ¨¡å‹ç±»å®šä¹‰å®Œæˆ | Model classes defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ•°æ®åŠ è½½ | Data Loading",
    "",
    "ä»Kaggleæ•°æ®é›†æˆ–æœ¬åœ°ç›®å½•åŠ è½½çœŸå®çš„HHTçŸ©é˜µã€‚",
    "Load real HHT matrices from Kaggle dataset or local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename: str) -> Optional[Dict[str, str]]:",
    "    \"\"\"",
    "    è§£ææ–‡ä»¶åæå–æ ‡ç­¾",
    "    Parse filename to extract labels",
    "    ",
    "    æ–‡ä»¶å‘½åæ ¼å¼ | File naming format:",
    "    - MUSCLENAME_movement_GENDER_###.npz",
    "    - ä¾‹å¦‚ Example: BICEPS_fatiguetest_M_006.npz",
    "    ",
    "    Returns:",
    "        dict with 'gender' and 'movement' keys, or None if test file",
    "    \"\"\"",
    "    basename = os.path.basename(filename)",
    "    ",
    "    # è·³è¿‡æµ‹è¯•æ–‡ä»¶ | Skip test files",
    "    if basename.lower().startswith('test'):",
    "        return None",
    "    ",
    "    # æå–æ€§åˆ« | Extract gender",
    "    gender_match = re.search(r'[_-]([MF])[_-]', basename)",
    "    if not gender_match:",
    "        return None",
    "    gender = gender_match.group(1)",
    "    ",
    "    # æå–åŠ¨ä½œè´¨é‡ | Extract movement quality",
    "    basename_lower = basename.lower()",
    "    if 'fatiguetest' in basename_lower or 'full' in basename_lower:",
    "        movement = 'full'",
    "    elif 'half' in basename_lower:",
    "        movement = 'half'",
    "    elif 'invalid' in basename_lower or 'wrong' in basename_lower:",
    "        movement = 'invalid'",
    "    else:",
    "        return None",
    "    ",
    "    return {'gender': gender, 'movement': movement}",
    "",
    "",
    "def load_real_data(data_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[str]]:",
    "    \"\"\"",
    "    ä»ç›®å½•åŠ è½½çœŸå®æ•°æ®",
    "    Load real data from directory",
    "    ",
    "    Returns:",
    "        X: HHT matrices (N, 256, 256)",
    "        y_movement: Movement labels (N,) - 0=full, 1=half, 2=invalid",
    "        y_gender: Gender labels (N,) - 0=M, 1=F",
    "        filenames: List of filenames",
    "    \"\"\"",
    "    print(f'\\nğŸ“‚ ä»ç›®å½•åŠ è½½æ•°æ® | Loading data from: {data_dir}')",
    "    ",
    "    npz_files = glob.glob(os.path.join(data_dir, '*.npz'))",
    "    print(f'   æ‰¾åˆ° Found {len(npz_files)} .npz files')",
    "    ",
    "    X_list = []",
    "    y_movement_list = []",
    "    y_gender_list = []",
    "    filenames = []",
    "    ",
    "    # æ ‡ç­¾ç¼–ç å™¨ | Label encoders",
    "    movement_encoder = LabelEncoder()",
    "    movement_encoder.fit(['full', 'half', 'invalid'])",
    "    ",
    "    gender_encoder = LabelEncoder()",
    "    gender_encoder.fit(['M', 'F'])",
    "    ",
    "    # åŠ è½½æ•°æ® | Load data",
    "    for npz_file in tqdm(npz_files, desc='Loading files'):",
    "        labels = parse_filename(npz_file)",
    "        ",
    "        if labels is None:  # æµ‹è¯•æ–‡ä»¶ | Test file",
    "            continue",
    "        ",
    "        try:",
    "            data = np.load(npz_file)",
    "            if 'hht' in data:",
    "                hht_matrix = data['hht']",
    "            else:",
    "                hht_matrix = data[list(data.keys())[0]]",
    "            ",
    "            # éªŒè¯å½¢çŠ¶ | Verify shape",
    "            if hht_matrix.shape != (256, 256):",
    "                print(f'   âš ï¸  è·³è¿‡ Skipping {os.path.basename(npz_file)}: å½¢çŠ¶ä¸åŒ¹é… shape mismatch {hht_matrix.shape}')",
    "                continue",
    "            ",
    "            # å½’ä¸€åŒ–åˆ°[0,1] | Normalize to [0,1]",
    "            if DATA_NORMALIZE:",
    "                hht_min = hht_matrix.min()",
    "                hht_max = hht_matrix.max()",
    "                if hht_max > hht_min:",
    "                    hht_matrix = (hht_matrix - hht_min) / (hht_max - hht_min)",
    "            ",
    "            # ç¼–ç æ ‡ç­¾ | Encode labels",
    "            movement_label = movement_encoder.transform([labels['movement']])[0]",
    "            gender_label = gender_encoder.transform([labels['gender']])[0]",
    "            ",
    "            X_list.append(hht_matrix)",
    "            y_movement_list.append(movement_label)",
    "            y_gender_list.append(gender_label)",
    "            filenames.append(npz_file)",
    "            ",
    "        except Exception as e:",
    "            print(f'   âŒ é”™è¯¯ Error loading {os.path.basename(npz_file)}: {e}')",
    "            continue",
    "    ",
    "    X = np.array(X_list, dtype=np.float32)",
    "    y_movement = np.array(y_movement_list, dtype=np.int64)",
    "    y_gender = np.array(y_gender_list, dtype=np.int64)",
    "    ",
    "    print(f'\\nâœ… æ•°æ®åŠ è½½å®Œæˆ | Data loading complete:')",
    "    print(f'   æ ·æœ¬æ•° Samples: {len(X)}')",
    "    print(f'   å½¢çŠ¶ Shape: {X.shape}')",
    "    print(f'   æ•°æ®èŒƒå›´ Data range: [{X.min():.4f}, {X.max():.4f}]')",
    "    ",
    "    print(f'\\nğŸ“Š åŠ¨ä½œè´¨é‡åˆ†å¸ƒ | Movement quality distribution:')",
    "    for i, movement in enumerate(['full', 'half', 'invalid']):",
    "        count = np.sum(y_movement == i)",
    "        print(f'   {movement}: {count} samples ({count/len(y_movement)*100:.1f}%)')",
    "    ",
    "    print(f'\\nğŸ‘¥ æ€§åˆ«åˆ†å¸ƒ | Gender distribution:')",
    "    for i, gender in enumerate(['M', 'F']):",
    "        count = np.sum(y_gender == i)",
    "        print(f'   {gender}: {count} samples ({count/len(y_gender)*100:.1f}%)')",
    "    ",
    "    return X, y_movement, y_gender, filenames",
    "",
    "",
    "# åŠ è½½æ•°æ® | Load data",
    "if os.path.exists(DATA_DIR):",
    "    X, y_movement, y_gender, filenames = load_real_data(DATA_DIR)",
    "    ",
    "    # åˆ†å‰²æ•°æ® | Split data",
    "    X_train, X_val, y_movement_train, y_movement_val, y_gender_train, y_gender_val = train_test_split(",
    "        X, y_movement, y_gender, ",
    "        test_size=DATA_TEST_SIZE, ",
    "        random_state=SEED, ",
    "        stratify=y_movement  # æŒ‰åŠ¨ä½œè´¨é‡åˆ†å±‚ | Stratify by movement quality",
    "    )",
    "    ",
    "    print(f'\\nâœ‚ï¸  æ•°æ®åˆ†å‰² | Data split:')",
    "    print(f'   è®­ç»ƒé›† Training: {len(X_train)} samples')",
    "    print(f'   éªŒè¯é›† Validation: {len(X_val)} samples')",
    "    ",
    "else:",
    "    print(f'\\nâŒ æ•°æ®ç›®å½•æœªæ‰¾åˆ° | Data directory not found: {DATA_DIR}')",
    "    print('   è¯·ç¡®ä¿æ•°æ®é›†å·²æ·»åŠ åˆ°æ­¤ç¬”è®°æœ¬ | Please ensure dataset is added to this notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è®­ç»ƒåŠ¨ä½œè´¨é‡åˆ†ç±»å™¨ï¼ˆæ·±åº¦å­¦ä¹ CNNï¼‰| Train Action Quality Classifier (Deep Learning CNN)",
    "",
    "ä½¿ç”¨æ‰©å±•çš„7å±‚CNNæ¶æ„è®­ç»ƒåŠ¨ä½œè´¨é‡åˆ†ç±»å™¨ã€‚",
    "Train action quality classifier using expanded 7-layer CNN architecture.",
    "",
    "### è®­ç»ƒæ”¹è¿› | Training Improvements:",
    "",
    "1. **å­¦ä¹ ç‡é¢„çƒ­** - å‰5è½®é€æ­¥å¢åŠ å­¦ä¹ ç‡",
    "   **LR Warmup** - Gradually increase LR for first 5 epochs",
    "   ",
    "2. **æ¢¯åº¦è£å‰ª** - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸",
    "   **Gradient Clipping** - Prevent gradient explosion",
    "   ",
    "3. **æ ‡ç­¾å¹³æ»‘** - æé«˜æ³›åŒ–èƒ½åŠ›",
    "   **Label Smoothing** - Improve generalization",
    "   ",
    "4. **ä½™å¼¦é€€ç«è°ƒåº¦** - æ›´å¥½çš„å­¦ä¹ ç‡è¡°å‡",
    "   **Cosine Annealing** - Better LR decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):",
    "    \"\"\"",
    "    æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤±",
    "    Label smoothing cross entropy loss",
    "    \"\"\"",
    "    def __init__(self, smoothing=0.1):",
    "        super().__init__()",
    "        self.smoothing = smoothing",
    "    ",
    "    def forward(self, pred, target):",
    "        n_classes = pred.size(-1)",
    "        log_preds = F.log_softmax(pred, dim=-1)",
    "        ",
    "        # å¹³æ»‘ç›®æ ‡ | Smooth targets",
    "        with torch.no_grad():",
    "            true_dist = torch.zeros_like(log_preds)",
    "            true_dist.fill_(self.smoothing / (n_classes - 1))",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)",
    "        ",
    "        return torch.mean(torch.sum(-true_dist * log_preds, dim=-1))",
    "",
    "",
    "def get_lr_schedule(optimizer, warmup_epochs, total_epochs, base_lr):",
    "    \"\"\"",
    "    åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆé¢„çƒ­ + ä½™å¼¦é€€ç«ï¼‰",
    "    Create LR scheduler (warmup + cosine annealing)",
    "    \"\"\"",
    "    def lr_lambda(epoch):",
    "        if epoch < warmup_epochs:",
    "            # çº¿æ€§é¢„çƒ­ | Linear warmup",
    "            return (epoch + 1) / warmup_epochs",
    "        else:",
    "            # ä½™å¼¦é€€ç« | Cosine annealing",
    "            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)",
    "            return 0.5 * (1.0 + np.cos(np.pi * progress))",
    "    ",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)",
    "",
    "",
    "def train_action_quality_model(",
    "    model, ",
    "    X_train, y_train, ",
    "    X_val, y_val,",
    "    epochs=100,",
    "    batch_size=16,",
    "    learning_rate=0.0001,",
    "    warmup_epochs=5,",
    "    grad_clip=1.0,",
    "    device='cuda'",
    "):",
    "    \"\"\"",
    "    è®­ç»ƒåŠ¨ä½œè´¨é‡åˆ†ç±»æ¨¡å‹",
    "    Train action quality classification model",
    "    \"\"\"",
    "    print('\\n' + '='*80)",
    "    print('å¼€å§‹è®­ç»ƒåŠ¨ä½œè´¨é‡åˆ†ç±»å™¨ | Starting Action Quality Classifier Training')",
    "    print('='*80)",
    "    ",
    "    model = model.to(device)",
    "    ",
    "    # å‡†å¤‡æ•°æ® | Prepare data",
    "    if X_train.ndim == 3:",
    "        X_train = X_train[:, np.newaxis, :, :]  # Add channel dim",
    "        X_val = X_val[:, np.newaxis, :, :]",
    "    ",
    "    train_dataset = torch.utils.data.TensorDataset(",
    "        torch.tensor(X_train, dtype=torch.float32),",
    "        torch.tensor(y_train, dtype=torch.long)",
    "    )",
    "    val_dataset = torch.utils.data.TensorDataset(",
    "        torch.tensor(X_val, dtype=torch.float32),",
    "        torch.tensor(y_val, dtype=torch.long)",
    "    )",
    "    ",
    "    train_loader = torch.utils.data.DataLoader(",
    "        train_dataset, batch_size=batch_size, shuffle=True, ",
    "        num_workers=2, pin_memory=True",
    "    )",
    "    val_loader = torch.utils.data.DataLoader(",
    "        val_dataset, batch_size=batch_size, shuffle=False,",
    "        num_workers=2, pin_memory=True",
    "    )",
    "    ",
    "    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ | Loss and optimizer",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)",
    "    optimizer = torch.optim.AdamW(",
    "        model.parameters(), ",
    "        lr=learning_rate, ",
    "        weight_decay=ACTION_WEIGHT_DECAY",
    "    )",
    "    ",
    "    # å­¦ä¹ ç‡è°ƒåº¦å™¨ | LR scheduler",
    "    scheduler = get_lr_schedule(optimizer, warmup_epochs, epochs, learning_rate)",
    "    ",
    "    # ReduceLROnPlateauä½œä¸ºå¤‡ç”¨ | ReduceLROnPlateau as backup",
    "    plateau_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(",
    "        optimizer, mode='max', factor=LR_SCHEDULER_FACTOR, ",
    "        patience=LR_SCHEDULER_PATIENCE, min_lr=LR_SCHEDULER_MIN_LR",
    "    )",
    "    ",
    "    # è®­ç»ƒå†å² | Training history",
    "    history = {",
    "        'train_loss': [],",
    "        'train_acc': [],",
    "        'val_loss': [],",
    "        'val_acc': [],",
    "        'lr': []",
    "    }",
    "    ",
    "    best_val_acc = 0.0",
    "    best_model_path = os.path.join(CHECKPOINT_DIR, 'best_action_quality_model.pt')",
    "    ",
    "    print(f'\\nğŸš€ è®­ç»ƒé…ç½® | Training configuration:')",
    "    print(f'   è®¾å¤‡ Device: {device}')",
    "    print(f'   è®­ç»ƒæ ·æœ¬ Training samples: {len(X_train)}')",
    "    print(f'   éªŒè¯æ ·æœ¬ Validation samples: {len(X_val)}')",
    "    print(f'   æ‰¹æ¬¡å¤§å° Batch size: {batch_size}')",
    "    print(f'   å­¦ä¹ ç‡ Learning rate: {learning_rate}')",
    "    print(f'   é¢„çƒ­è½®æ•° Warmup epochs: {warmup_epochs}')",
    "    print(f'   æ¢¯åº¦è£å‰ª Gradient clipping: {grad_clip}')",
    "    print(f'   æƒé‡è¡°å‡ Weight decay: {ACTION_WEIGHT_DECAY}')",
    "    print()",
    "    ",
    "    # è®­ç»ƒå¾ªç¯ | Training loop",
    "    for epoch in range(epochs):",
    "        # è®­ç»ƒé˜¶æ®µ | Training phase",
    "        model.train()",
    "        train_loss = 0.0",
    "        train_correct = 0",
    "        train_total = 0",
    "        ",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')",
    "        for batch_X, batch_y in pbar:",
    "            batch_X = batch_X.to(device, non_blocking=True)",
    "            batch_y = batch_y.to(device, non_blocking=True)",
    "            ",
    "            optimizer.zero_grad()",
    "            outputs = model(batch_X)",
    "            loss = criterion(outputs, batch_y)",
    "            loss.backward()",
    "            ",
    "            # æ¢¯åº¦è£å‰ª | Gradient clipping",
    "            if grad_clip > 0:",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)",
    "            ",
    "            optimizer.step()",
    "            ",
    "            train_loss += loss.item() * batch_X.size(0)",
    "            _, predicted = outputs.max(1)",
    "            train_total += batch_y.size(0)",
    "            train_correct += predicted.eq(batch_y).sum().item()",
    "            ",
    "            # æ›´æ–°è¿›åº¦æ¡ | Update progress bar",
    "            pbar.set_postfix({",
    "                'loss': f'{loss.item():.4f}',",
    "                'acc': f'{100.*train_correct/train_total:.2f}%'",
    "            })",
    "        ",
    "        train_loss /= train_total",
    "        train_acc = train_correct / train_total",
    "        ",
    "        # éªŒè¯é˜¶æ®µ | Validation phase",
    "        model.eval()",
    "        val_loss = 0.0",
    "        val_correct = 0",
    "        val_total = 0",
    "        ",
    "        with torch.no_grad():",
    "            for batch_X, batch_y in val_loader:",
    "                batch_X = batch_X.to(device, non_blocking=True)",
    "                batch_y = batch_y.to(device, non_blocking=True)",
    "                ",
    "                outputs = model(batch_X)",
    "                loss = criterion(outputs, batch_y)",
    "                ",
    "                val_loss += loss.item() * batch_X.size(0)",
    "                _, predicted = outputs.max(1)",
    "                val_total += batch_y.size(0)",
    "                val_correct += predicted.eq(batch_y).sum().item()",
    "        ",
    "        val_loss /= val_total",
    "        val_acc = val_correct / val_total",
    "        ",
    "        # æ›´æ–°å­¦ä¹ ç‡ | Update learning rate",
    "        scheduler.step()",
    "        plateau_scheduler.step(val_acc)",
    "        current_lr = optimizer.param_groups[0]['lr']",
    "        ",
    "        # ä¿å­˜å†å² | Save history",
    "        history['train_loss'].append(train_loss)",
    "        history['train_acc'].append(train_acc)",
    "        history['val_loss'].append(val_loss)",
    "        history['val_acc'].append(val_acc)",
    "        history['lr'].append(current_lr)",
    "        ",
    "        # æ‰“å°è¿›åº¦ | Print progress",
    "        print(f'Epoch [{epoch+1:3d}/{epochs}] | '",
    "              f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | '",
    "              f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | '",
    "              f'LR: {current_lr:.6f}')",
    "        ",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹ | Save best model",
    "        if val_acc > best_val_acc:",
    "            best_val_acc = val_acc",
    "            torch.save({",
    "                'epoch': epoch,",
    "                'model_state_dict': model.state_dict(),",
    "                'optimizer_state_dict': optimizer.state_dict(),",
    "                'best_val_acc': best_val_acc,",
    "                'history': history",
    "            }, best_model_path)",
    "            print(f'  â­ æ–°æœ€ä½³æ¨¡å‹ï¼| New best model! Val Acc: {val_acc:.4f}')",
    "        ",
    "        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ | Save checkpoint periodically",
    "        if (epoch + 1) % CHECKPOINT_INTERVAL == 0:",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, f'action_quality_epoch_{epoch+1}.pt')",
    "            torch.save({",
    "                'epoch': epoch,",
    "                'model_state_dict': model.state_dict(),",
    "                'optimizer_state_dict': optimizer.state_dict(),",
    "                'best_val_acc': best_val_acc,",
    "                'history': history",
    "            }, checkpoint_path)",
    "            print(f'  ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜ | Checkpoint saved: {checkpoint_path}')",
    "    ",
    "    print('\\n' + '='*80)",
    "    print(f'âœ… è®­ç»ƒå®Œæˆï¼| Training complete!')",
    "    print(f'   æœ€ä½³éªŒè¯å‡†ç¡®ç‡ Best validation accuracy: {best_val_acc:.4f}')",
    "    print(f'   æœ€ä½³æ¨¡å‹å·²ä¿å­˜ Best model saved to: {best_model_path}')",
    "    print('='*80)",
    "    ",
    "    return history, best_model_path",
    "",
    "",
    "# åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹ | Create and train model",
    "if 'X_train' in locals():",
    "    # åˆ›å»ºç¼–ç å™¨ | Create encoder",
    "    encoder = ExpandedCNNEncoder(",
    "        in_channels=MODEL_IN_CHANNELS,",
    "        base_channels=MODEL_BASE_CHANNELS",
    "    )",
    "    ",
    "    # åˆ›å»ºåŠ¨ä½œè´¨é‡åˆ†ç±»å™¨ | Create action quality classifier",
    "    action_model = ActionQualityCNN(",
    "        encoder=encoder,",
    "        n_classes=3,  # full, half, invalid",
    "        dropout_rate=MODEL_DROPOUT_RATE",
    "    )",
    "    ",
    "    # è®­ç»ƒ | Train",
    "    action_history, action_best_path = train_action_quality_model(",
    "        model=action_model,",
    "        X_train=X_train,",
    "        y_train=y_movement_train,",
    "        X_val=X_val,",
    "        y_val=y_movement_val,",
    "        epochs=ACTION_EPOCHS,",
    "        batch_size=ACTION_BATCH_SIZE,",
    "        learning_rate=ACTION_LEARNING_RATE,",
    "        warmup_epochs=ACTION_WARMUP_EPOCHS,",
    "        grad_clip=ACTION_GRAD_CLIP,",
    "        device=device",
    "    )",
    "else:",
    "    print('\\nâš ï¸  è¯·å…ˆåŠ è½½æ•°æ® | Please load data first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ | Visualize Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):",
    "    \"\"\"ç»˜åˆ¶è®­ç»ƒå†å² | Plot training history\"\"\"",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))",
    "    ",
    "    epochs = range(1, len(history['train_loss']) + 1)",
    "    ",
    "    # Loss",
    "    axes[0].plot(epochs, history['train_loss'], 'b-', label='è®­ç»ƒæŸå¤± Train Loss', linewidth=2)",
    "    axes[0].plot(epochs, history['val_loss'], 'r-', label='éªŒè¯æŸå¤± Val Loss', linewidth=2)",
    "    axes[0].set_xlabel('è½®æ¬¡ Epoch', fontsize=12)",
    "    axes[0].set_ylabel('æŸå¤± Loss', fontsize=12)",
    "    axes[0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤± | Training and Validation Loss', fontsize=14, fontweight='bold')",
    "    axes[0].legend(fontsize=10)",
    "    axes[0].grid(True, alpha=0.3)",
    "    ",
    "    # Accuracy",
    "    axes[1].plot(epochs, history['train_acc'], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡ Train Acc', linewidth=2)",
    "    axes[1].plot(epochs, history['val_acc'], 'r-', label='éªŒè¯å‡†ç¡®ç‡ Val Acc', linewidth=2)",
    "    axes[1].set_xlabel('è½®æ¬¡ Epoch', fontsize=12)",
    "    axes[1].set_ylabel('å‡†ç¡®ç‡ Accuracy', fontsize=12)",
    "    axes[1].set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡ | Training and Validation Accuracy', fontsize=14, fontweight='bold')",
    "    axes[1].legend(fontsize=10)",
    "    axes[1].grid(True, alpha=0.3)",
    "    ",
    "    # Learning Rate",
    "    axes[2].plot(epochs, history['lr'], 'g-', linewidth=2)",
    "    axes[2].set_xlabel('è½®æ¬¡ Epoch', fontsize=12)",
    "    axes[2].set_ylabel('å­¦ä¹ ç‡ Learning Rate', fontsize=12)",
    "    axes[2].set_title('å­¦ä¹ ç‡è°ƒåº¦ | Learning Rate Schedule', fontsize=14, fontweight='bold')",
    "    axes[2].set_yscale('log')",
    "    axes[2].grid(True, alpha=0.3)",
    "    ",
    "    plt.tight_layout()",
    "    plt.show()",
    "    ",
    "    # æ‰“å°ç»Ÿè®¡ | Print statistics",
    "    print(f'\\nğŸ“Š è®­ç»ƒç»Ÿè®¡ | Training Statistics:')",
    "    print(f'   æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡ Final train acc: {history[\"train_acc\"][-1]:.4f}')",
    "    print(f'   æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡ Final val acc: {history[\"val_acc\"][-1]:.4f}')",
    "    print(f'   æœ€ä½³éªŒè¯å‡†ç¡®ç‡ Best val acc: {max(history[\"val_acc\"]):.4f}')",
    "    print(f'   æœ€ç»ˆè®­ç»ƒæŸå¤± Final train loss: {history[\"train_loss\"][-1]:.4f}')",
    "    print(f'   æœ€ç»ˆéªŒè¯æŸå¤± Final val loss: {history[\"val_loss\"][-1]:.4f}')",
    "",
    "",
    "if 'action_history' in locals():",
    "    plot_training_history(action_history)",
    "else:",
    "    print('\\nâš ï¸  è¯·å…ˆè®­ç»ƒæ¨¡å‹ | Please train the model first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. è®­ç»ƒæ€§åˆ«åˆ†ç±»å™¨ï¼ˆSVMï¼‰| Train Gender Classifier (SVM)",
    "",
    "ä½¿ç”¨è®­ç»ƒå¥½çš„CNNæå–ç‰¹å¾ï¼Œç„¶åè®­ç»ƒSVMè¿›è¡Œæ€§åˆ«åˆ†ç±»ã€‚",
    "Use trained CNN to extract features, then train SVM for gender classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderSVMClassifier:",
    "    \"\"\"",
    "    æ€§åˆ«SVMåˆ†ç±»å™¨",
    "    Gender SVM Classifier",
    "    ",
    "    ä½¿ç”¨CNNç‰¹å¾ + SVMåˆ†ç±»å™¨",
    "    Uses CNN features + SVM classifier",
    "    \"\"\"",
    "    def __init__(self, feature_extractor, svm_kernel='rbf', svm_C=10.0, svm_gamma='scale', device='cuda'):",
    "        self.feature_extractor = feature_extractor",
    "        self.device = device",
    "        self.scaler = StandardScaler()",
    "        self.svm = SVC(",
    "            kernel=svm_kernel,",
    "            C=svm_C,",
    "            gamma=svm_gamma,",
    "            probability=True,",
    "            random_state=SEED",
    "        )",
    "        self.is_fitted = False",
    "        ",
    "        print(f'âœ… æ€§åˆ«åˆ†ç±»å™¨å·²åˆ›å»º | Gender classifier created:')",
    "        print(f'   ç‰¹å¾æå–å™¨ Feature extractor: CNN')",
    "        print(f'   SVMæ ¸å‡½æ•° SVM kernel: {svm_kernel}')",
    "        print(f'   Cå‚æ•° C: {svm_C}')",
    "        print(f'   Gamma: {svm_gamma}')",
    "    ",
    "    def extract_features(self, X, batch_size=32):",
    "        \"\"\"æå–ç‰¹å¾ | Extract features\"\"\"",
    "        self.feature_extractor.eval()",
    "        ",
    "        if X.ndim == 3:",
    "            X = X[:, np.newaxis, :, :]",
    "        ",
    "        features_list = []",
    "        with torch.no_grad():",
    "            for i in tqdm(range(0, len(X), batch_size), desc='Extracting features'):",
    "                batch = torch.tensor(X[i:i+batch_size], dtype=torch.float32).to(self.device)",
    "                batch_features = self.feature_extractor.extract_features(batch)",
    "                features_list.append(batch_features.cpu().numpy())",
    "        ",
    "        return np.vstack(features_list)",
    "    ",
    "    def fit(self, X, y, batch_size=32):",
    "        \"\"\"è®­ç»ƒSVM | Train SVM\"\"\"",
    "        print(f'\\nğŸ”§ å¼€å§‹è®­ç»ƒæ€§åˆ«SVMåˆ†ç±»å™¨ | Starting Gender SVM Classifier Training')",
    "        ",
    "        # æå–ç‰¹å¾ | Extract features",
    "        features = self.extract_features(X, batch_size)",
    "        ",
    "        # å½’ä¸€åŒ– | Normalize",
    "        print('   å½’ä¸€åŒ–ç‰¹å¾ | Normalizing features...')",
    "        features_scaled = self.scaler.fit_transform(features)",
    "        ",
    "        # è®­ç»ƒSVM | Train SVM",
    "        print('   è®­ç»ƒSVM | Training SVM...')",
    "        self.svm.fit(features_scaled, y)",
    "        ",
    "        self.is_fitted = True",
    "        print('   âœ… è®­ç»ƒå®Œæˆ | Training complete!')",
    "    ",
    "    def predict(self, X, batch_size=32):",
    "        \"\"\"é¢„æµ‹ | Predict\"\"\"",
    "        if not self.is_fitted:",
    "            raise RuntimeError('å¿…é¡»å…ˆè®­ç»ƒæ¨¡å‹ | Must fit model first')",
    "        ",
    "        features = self.extract_features(X, batch_size)",
    "        features_scaled = self.scaler.transform(features)",
    "        return self.svm.predict(features_scaled)",
    "    ",
    "    def predict_proba(self, X, batch_size=32):",
    "        \"\"\"é¢„æµ‹æ¦‚ç‡ | Predict probabilities\"\"\"",
    "        if not self.is_fitted:",
    "            raise RuntimeError('å¿…é¡»å…ˆè®­ç»ƒæ¨¡å‹ | Must fit model first')",
    "        ",
    "        features = self.extract_features(X, batch_size)",
    "        features_scaled = self.scaler.transform(features)",
    "        return self.svm.predict_proba(features_scaled)",
    "    ",
    "    def evaluate(self, X, y, batch_size=32):",
    "        \"\"\"è¯„ä¼°æ¨¡å‹ | Evaluate model\"\"\"",
    "        y_pred = self.predict(X, batch_size)",
    "        accuracy = accuracy_score(y, y_pred)",
    "        ",
    "        return {",
    "            'accuracy': accuracy,",
    "            'predictions': y_pred,",
    "            'classification_report': classification_report(y, y_pred, target_names=['M', 'F']),",
    "            'confusion_matrix': confusion_matrix(y, y_pred)",
    "        }",
    "    ",
    "    def save(self, path):",
    "        \"\"\"ä¿å­˜æ¨¡å‹ | Save model\"\"\"",
    "        with open(f'{path}_scaler.pkl', 'wb') as f:",
    "            pickle.dump(self.scaler, f)",
    "        with open(f'{path}_svm.pkl', 'wb') as f:",
    "            pickle.dump(self.svm, f)",
    "        print(f'ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ | Model saved to {path}_*.pkl')",
    "    ",
    "    @classmethod",
    "    def load(cls, path, feature_extractor, device='cuda'):",
    "        \"\"\"åŠ è½½æ¨¡å‹ | Load model\"\"\"",
    "        classifier = cls(feature_extractor, device=device)",
    "        with open(f'{path}_scaler.pkl', 'rb') as f:",
    "            classifier.scaler = pickle.load(f)",
    "        with open(f'{path}_svm.pkl', 'rb') as f:",
    "            classifier.svm = pickle.load(f)",
    "        classifier.is_fitted = True",
    "        print(f'ğŸ“‚ æ¨¡å‹å·²åŠ è½½ | Model loaded from {path}_*.pkl')",
    "        return classifier",
    "",
    "",
    "# è®­ç»ƒæ€§åˆ«åˆ†ç±»å™¨ | Train gender classifier",
    "if 'action_model' in locals() and 'X_train' in locals():",
    "    # åŠ è½½æœ€ä½³åŠ¨ä½œè´¨é‡æ¨¡å‹ç”¨äºç‰¹å¾æå– | Load best action quality model for feature extraction",
    "    checkpoint = torch.load(action_best_path, map_location=device)",
    "    action_model.load_state_dict(checkpoint['model_state_dict'])",
    "    action_model.eval()",
    "    ",
    "    # åˆ›å»ºæ€§åˆ«åˆ†ç±»å™¨ | Create gender classifier",
    "    gender_classifier = GenderSVMClassifier(",
    "        feature_extractor=action_model,",
    "        svm_kernel=SVM_KERNEL,",
    "        svm_C=SVM_C,",
    "        svm_gamma=SVM_GAMMA,",
    "        device=device",
    "    )",
    "    ",
    "    # è®­ç»ƒ | Train",
    "    gender_classifier.fit(X_train, y_gender_train, batch_size=32)",
    "    ",
    "    # è¯„ä¼°è®­ç»ƒé›† | Evaluate on training set",
    "    print(f'\\nğŸ“Š è®­ç»ƒé›†è¯„ä¼° | Training Set Evaluation:')",
    "    train_results = gender_classifier.evaluate(X_train, y_gender_train, batch_size=32)",
    "    print(f'   è®­ç»ƒå‡†ç¡®ç‡ Training Accuracy: {train_results[\"accuracy\"]:.4f}')",
    "    ",
    "    # è¯„ä¼°éªŒè¯é›† | Evaluate on validation set",
    "    print(f'\\nğŸ“Š éªŒè¯é›†è¯„ä¼° | Validation Set Evaluation:')",
    "    val_results = gender_classifier.evaluate(X_val, y_gender_val, batch_size=32)",
    "    print(f'   éªŒè¯å‡†ç¡®ç‡ Validation Accuracy: {val_results[\"accuracy\"]:.4f}')",
    "    print(f'\\nåˆ†ç±»æŠ¥å‘Š | Classification Report:')",
    "    print(val_results['classification_report'])",
    "    ",
    "    # ä¿å­˜æ¨¡å‹ | Save model",
    "    gender_model_path = os.path.join(CHECKPOINT_DIR, 'gender_svm_model')",
    "    gender_classifier.save(gender_model_path)",
    "    ",
    "else:",
    "    print('\\nâš ï¸  è¯·å…ˆè®­ç»ƒåŠ¨ä½œè´¨é‡æ¨¡å‹ | Please train action quality model first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ç»¼åˆè¯„ä¼° | Comprehensive Evaluation",
    "",
    "è¯„ä¼°ä¸¤ä¸ªåˆ†ç±»å™¨çš„æ€§èƒ½ã€‚",
    "Evaluate performance of both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»¼åˆè¯„ä¼° | Comprehensive evaluation",
    "if 'action_model' in locals() and 'gender_classifier' in locals():",
    "    print('='*80)",
    "    print('ç»¼åˆè¯„ä¼°æŠ¥å‘Š | COMPREHENSIVE EVALUATION REPORT')",
    "    print('='*80)",
    "    ",
    "    # åŠ¨ä½œè´¨é‡è¯„ä¼° | Action quality evaluation",
    "    print(f'\\nğŸ¯ åŠ¨ä½œè´¨é‡åˆ†ç±»å™¨ï¼ˆæ·±åº¦å­¦ä¹ CNNï¼‰| Action Quality Classifier (Deep Learning CNN)')",
    "    print('-'*80)",
    "    ",
    "    action_model.eval()",
    "    X_val_tensor = torch.tensor(X_val[:, np.newaxis, :, :], dtype=torch.float32).to(device)",
    "    y_val_tensor = torch.tensor(y_movement_val, dtype=torch.long).to(device)",
    "    ",
    "    with torch.no_grad():",
    "        outputs = action_model(X_val_tensor)",
    "        _, predictions = outputs.max(1)",
    "        action_acc = (predictions == y_val_tensor).float().mean().item()",
    "    ",
    "    y_pred_action = predictions.cpu().numpy()",
    "    ",
    "    print(f'éªŒè¯é›†å‡†ç¡®ç‡ Validation Accuracy: {action_acc:.4f}')",
    "    print(f'\\nåˆ†ç±»æŠ¥å‘Š Classification Report:')",
    "    print(classification_report(y_movement_val, y_pred_action, target_names=['Full', 'Half', 'Invalid']))",
    "    ",
    "    print(f'\\næ··æ·†çŸ©é˜µ Confusion Matrix:')",
    "    cm_action = confusion_matrix(y_movement_val, y_pred_action)",
    "    print(cm_action)",
    "    ",
    "    # æ€§åˆ«è¯„ä¼° | Gender evaluation",
    "    print(f'\\nğŸ‘¥ æ€§åˆ«åˆ†ç±»å™¨ï¼ˆSVMï¼‰| Gender Classifier (SVM)')",
    "    print('-'*80)",
    "    ",
    "    val_results_gender = gender_classifier.evaluate(X_val, y_gender_val, batch_size=32)",
    "    print(f'éªŒè¯é›†å‡†ç¡®ç‡ Validation Accuracy: {val_results_gender[\"accuracy\"]:.4f}')",
    "    print(f'\\nåˆ†ç±»æŠ¥å‘Š Classification Report:')",
    "    print(val_results_gender['classification_report'])",
    "    ",
    "    print(f'\\næ··æ·†çŸ©é˜µ Confusion Matrix:')",
    "    print(val_results_gender['confusion_matrix'])",
    "    ",
    "    # å¯è§†åŒ–æ··æ·†çŸ©é˜µ | Visualize confusion matrices",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))",
    "    ",
    "    # åŠ¨ä½œè´¨é‡æ··æ·†çŸ©é˜µ | Action quality confusion matrix",
    "    im1 = axes[0].imshow(cm_action, cmap='Blues')",
    "    axes[0].set_title('åŠ¨ä½œè´¨é‡æ··æ·†çŸ©é˜µ\\nAction Quality Confusion Matrix', fontsize=12, fontweight='bold')",
    "    axes[0].set_xlabel('é¢„æµ‹æ ‡ç­¾ Predicted', fontsize=10)",
    "    axes[0].set_ylabel('çœŸå®æ ‡ç­¾ True', fontsize=10)",
    "    axes[0].set_xticks([0, 1, 2])",
    "    axes[0].set_yticks([0, 1, 2])",
    "    axes[0].set_xticklabels(['Full', 'Half', 'Invalid'])",
    "    axes[0].set_yticklabels(['Full', 'Half', 'Invalid'])",
    "    ",
    "    # æ·»åŠ æ•°å€¼æ ‡æ³¨ | Add value annotations",
    "    for i in range(3):",
    "        for j in range(3):",
    "            axes[0].text(j, i, str(cm_action[i, j]), ",
    "                        ha='center', va='center', color='white' if cm_action[i, j] > cm_action.max()/2 else 'black')",
    "    plt.colorbar(im1, ax=axes[0])",
    "    ",
    "    # æ€§åˆ«æ··æ·†çŸ©é˜µ | Gender confusion matrix",
    "    cm_gender = val_results_gender['confusion_matrix']",
    "    im2 = axes[1].imshow(cm_gender, cmap='Greens')",
    "    axes[1].set_title('æ€§åˆ«æ··æ·†çŸ©é˜µ\\nGender Confusion Matrix', fontsize=12, fontweight='bold')",
    "    axes[1].set_xlabel('é¢„æµ‹æ ‡ç­¾ Predicted', fontsize=10)",
    "    axes[1].set_ylabel('çœŸå®æ ‡ç­¾ True', fontsize=10)",
    "    axes[1].set_xticks([0, 1])",
    "    axes[1].set_yticks([0, 1])",
    "    axes[1].set_xticklabels(['M', 'F'])",
    "    axes[1].set_yticklabels(['M', 'F'])",
    "    ",
    "    # æ·»åŠ æ•°å€¼æ ‡æ³¨ | Add value annotations",
    "    for i in range(2):",
    "        for j in range(2):",
    "            axes[1].text(j, i, str(cm_gender[i, j]),",
    "                        ha='center', va='center', color='white' if cm_gender[i, j] > cm_gender.max()/2 else 'black')",
    "    plt.colorbar(im2, ax=axes[1])",
    "    ",
    "    plt.tight_layout()",
    "    plt.show()",
    "    ",
    "    print('\\n' + '='*80)",
    "    print('âœ… è¯„ä¼°å®Œæˆ | Evaluation Complete')",
    "    print('='*80)",
    "    ",
    "else:",
    "    print('\\nâš ï¸  è¯·å…ˆè®­ç»ƒä¸¤ä¸ªæ¨¡å‹ | Please train both models first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æ€»ç»“å’Œä¸‹ä¸€æ­¥ | Summary and Next Steps",
    "",
    "### æ¨¡å‹ä¿å­˜ä½ç½® | Model Save Locations:",
    "",
    "- **åŠ¨ä½œè´¨é‡åˆ†ç±»å™¨** Action Quality Classifier: `{CHECKPOINT_DIR}/best_action_quality_model.pt`",
    "- **æ€§åˆ«SVMåˆ†ç±»å™¨** Gender SVM Classifier: `{CHECKPOINT_DIR}/gender_svm_model_*.pkl`",
    "",
    "### å…³é”®æ”¹è¿›æ€»ç»“ | Key Improvements Summary:",
    "",
    "1. âœ… **æ‰©å±•çš„7å±‚CNN** - æ›´å¼ºçš„ç‰¹å¾æå–èƒ½åŠ›",
    "   **Expanded 7-layer CNN** - Stronger feature extraction",
    "",
    "2. âœ… **æ‰¹å½’ä¸€åŒ– + Kaimingåˆå§‹åŒ–** - è§£å†³æ¢¯åº¦é—®é¢˜",
    "   **BatchNorm + Kaiming Init** - Solve gradient issues",
    "",
    "3. âœ… **å­¦ä¹ ç‡é¢„çƒ­å’Œè°ƒåº¦** - æ›´ç¨³å®šçš„è®­ç»ƒ",
    "   **LR warmup and scheduling** - More stable training",
    "",
    "4. âœ… **æ¢¯åº¦è£å‰ª** - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸",
    "   **Gradient clipping** - Prevent gradient explosion",
    "",
    "5. âœ… **æ ‡ç­¾å¹³æ»‘** - æé«˜æ³›åŒ–èƒ½åŠ›",
    "   **Label smoothing** - Better generalization",
    "",
    "6. âœ… **åŒåˆ†ç±»å™¨ç³»ç»Ÿ** - ä¸“é—¨ä¼˜åŒ–æ¯ä¸ªä»»åŠ¡",
    "   **Dual classifier system** - Specialized optimization",
    "",
    "### ä½¿ç”¨å»ºè®® | Usage Recommendations:",
    "",
    "1. **è°ƒæ•´è¶…å‚æ•°** - æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ç­‰",
    "   **Tune hyperparameters** - Adjust LR, batch size based on dataset size",
    "",
    "2. **æ•°æ®å¢å¼º** - å¦‚æœè®­ç»ƒæ•°æ®è¾ƒå°‘ï¼Œå¯ä»¥æ·»åŠ æ•°æ®å¢å¼º",
    "   **Data augmentation** - Add if training data is limited",
    "",
    "3. **æ¨¡å‹é›†æˆ** - å¯ä»¥è®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶é›†æˆé¢„æµ‹ç»“æœ",
    "   **Model ensemble** - Train multiple models and ensemble predictions",
    "",
    "4. **æŒç»­ç›‘æ§** - è§‚å¯Ÿè®­ç»ƒæ›²çº¿ï¼Œç¡®ä¿lossä¸‹é™ã€accuracyæå‡",
    "   **Monitor training** - Watch training curves, ensure loss decreases and accuracy improves",
    "",
    "### é¢„æœŸæ•ˆæœ | Expected Results:",
    "",
    "- **æŸå¤±ä¸‹é™** Loss decreases: åº”è¯¥çœ‹åˆ°æ˜æ˜¾çš„lossä¸‹é™æ›²çº¿",
    "  You should see clear loss decrease curve",
    "  ",
    "- **å‡†ç¡®ç‡æå‡** Accuracy improves: å‡†ç¡®ç‡åº”è¯¥ç¨³æ­¥æå‡",
    "  Accuracy should steadily improve",
    "  ",
    "- **æ”¶æ•›ç¨³å®š** Stable convergence: è®­ç»ƒåº”è¯¥åœ¨50-100è½®å†…æ”¶æ•›",
    "  Training should converge within 50-100 epochs",
    "",
    "å¦‚æœä»ç„¶é‡åˆ°è®­ç»ƒé—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š",
    "If you still encounter training issues, check:",
    "",
    "1. æ•°æ®è´¨é‡å’Œåˆ†å¸ƒ | Data quality and distribution",
    "2. å­¦ä¹ ç‡æ˜¯å¦åˆé€‚ | Learning rate appropriateness  ",
    "3. æ‰¹æ¬¡å¤§å°æ˜¯å¦åˆé€‚ | Batch size appropriateness",
    "4. æ˜¯å¦éœ€è¦æ›´å¤šæ•°æ® | Need for more data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}