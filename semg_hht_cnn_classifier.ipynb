{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sEMG-HHT CNN Classifier for Movement Quality and Fatigue Classification\n",
    "\n",
    "This notebook implements a Convolutional Neural Network (CNN) encoder with SVM classifier for classifying surface electromyography (sEMG) signals based on movement quality and fatigue levels.\n",
    "\n",
    "## Architecture Overview\n",
    "- **Input**: 256×256 matrix from HHT (Hilbert-Huang Transform) of sEMG signals\n",
    "- **Encoder**: 3-layer CNN with Conv2D + InstanceNorm + LeakyReLU\n",
    "- **Pooling**: Global Average Pooling\n",
    "- **Classifier**: SVM for multi-class classification\n",
    "\n",
    "## Requirements\n",
    "- PyTorch\n",
    "- scikit-learn\n",
    "- NumPy\n",
    "- Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Integration | Kaggle 集成\n\n",
    "This notebook is configured to work with the **HILBERTMATRIX_NPZ** dataset on Kaggle.\n\n",
    "本笔记本配置为使用 Kaggle 上的 **HILBERTMATRIX_NPZ** 数据集。\n\n",
    "Data path: `/kaggle/input/hilbertmatrix-npz/hht_matrices/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sEMG-HHT CNN Classifier - Kaggle Training\n",
    "# sEMG-HHT CNN 分类器 - Kaggle 训练\n",
    "\n",
    "This notebook trains a CNN-SVM classifier on sEMG Hilbert-Huang Transform data for 6-class classification:\n",
    "本笔记本训练一个 CNN-SVM 分类器，用于 sEMG 希尔伯特-黄变换数据的 6 类分类：\n",
    "\n",
    "- **Gender (性别)**: Male (男性, M), Female (女性, F)\n",
    "- **Movement Quality (动作质量)**: Full (完整), Half (半程), Invalid (无效)\n",
    "\n",
    "## Dataset Integration | 数据集集成\n",
    "\n",
    "This notebook uses the **HILBERTMATRIX_NPZ** dataset from Kaggle.\n",
    "本笔记本使用 Kaggle 上的 **HILBERTMATRIX_NPZ** 数据集。\n",
    "\n",
    "Data location: `/kaggle/input/hilbertmatrix-npz/hht_matrices/`\n",
    "数据位置：`/kaggle/input/hilbertmatrix-npz/hht_matrices/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data paths for Kaggle\n",
    "# 配置 Kaggle 数据路径\n",
    "import os\n",
    "\n",
    "# Check if running on Kaggle\n",
    "# 检查是否在 Kaggle 上运行\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    DATA_DIR = '/kaggle/input/hilbertmatrix-npz/hht_matrices'\n",
    "    CHECKPOINT_DIR = '/kaggle/working/checkpoints'\n",
    "    print(f'Running on Kaggle | 在 Kaggle 上运行')\n",
    "    print(f'Data directory | 数据目录: {DATA_DIR}')\n",
    "else:\n",
    "    DATA_DIR = './data'\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    print(f'Running locally | 本地运行')\n",
    "    print(f'Data directory | 数据目录: {DATA_DIR}')\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f'Checkpoint directory | 检查点目录: {CHECKPOINT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if running on Kaggle or fresh environment)\n",
    "# !pip install torch torchvision scikit-learn numpy matplotlib scipy PyEMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from typing import Tuple, Optional\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings that are not critical for this demo\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CNN Encoder Architecture\n",
    "\n",
    "The encoder consists of 3 convolutional layers, each with:\n",
    "- Conv2D (kernel=3, stride=2, padding=1)\n",
    "- Instance Normalization\n",
    "- LeakyReLU activation\n",
    "\n",
    "This progressively reduces the spatial dimensions while extracting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block with Conv2D, InstanceNorm, and LeakyReLU.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, \n",
    "                 kernel_size: int = 3, stride: int = 2, padding: int = 1,\n",
    "                 leaky_slope: float = 0.2):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
    "                              kernel_size=kernel_size, \n",
    "                              stride=stride, \n",
    "                              padding=padding)\n",
    "        self.instance_norm = nn.InstanceNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_slope)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.instance_norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class sEMGHHTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN Encoder for sEMG-HHT matrix classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: 1×256×256 (single-channel HHT matrix)\n",
    "    - 3 ConvBlocks with increasing channels\n",
    "    - Global Average Pooling\n",
    "    - Output: Feature vector for SVM classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 1, \n",
    "                 base_channels: int = 64,\n",
    "                 num_layers: int = 3,\n",
    "                 leaky_slope: float = 0.2):\n",
    "        super(sEMGHHTEncoder, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Build convolutional layers\n",
    "        layers = []\n",
    "        current_channels = in_channels\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            out_channels = base_channels * (2 ** i)\n",
    "            layers.append(ConvBlock(\n",
    "                in_channels=current_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                leaky_slope=leaky_slope\n",
    "            ))\n",
    "            current_channels = out_channels\n",
    "        \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Calculate output feature dimension\n",
    "        self.feature_dim = base_channels * (2 ** (num_layers - 1))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, channels, height, width)\n",
    "        \n",
    "        Returns:\n",
    "            Feature tensor of shape (batch, feature_dim)\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch, feature_dim)\n",
    "        return x\n",
    "    \n",
    "    def get_feature_dim(self) -> int:\n",
    "        \"\"\"Return the output feature dimension.\"\"\"\n",
    "        return self.feature_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete Classification Pipeline\n",
    "\n",
    "This class combines the CNN encoder with an SVM classifier for end-to-end classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sEMGHHTClassifier:\n",
    "    \"\"\"\n",
    "    Complete classification pipeline combining CNN encoder and SVM classifier.\n",
    "    \n",
    "    The pipeline:\n",
    "    1. Extracts features using CNN encoder\n",
    "    2. Normalizes features using StandardScaler\n",
    "    3. Classifies using SVM (supports multi-class)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 encoder: Optional[sEMGHHTEncoder] = None,\n",
    "                 svm_kernel: str = 'rbf',\n",
    "                 svm_C: float = 1.0,\n",
    "                 svm_gamma: str = 'scale',\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Args:\n",
    "            encoder: Pre-trained or new CNN encoder (creates default if None)\n",
    "            svm_kernel: SVM kernel type ('rbf', 'linear', 'poly')\n",
    "            svm_C: SVM regularization parameter\n",
    "            svm_gamma: SVM gamma parameter\n",
    "            device: Device to run the encoder on\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize encoder\n",
    "        if encoder is None:\n",
    "            self.encoder = sEMGHHTEncoder(\n",
    "                in_channels=1, \n",
    "                base_channels=64, \n",
    "                num_layers=3\n",
    "            )\n",
    "        else:\n",
    "            self.encoder = encoder\n",
    "        \n",
    "        self.encoder.to(self.device)\n",
    "        \n",
    "        # Initialize scaler and SVM\n",
    "        self.scaler = StandardScaler()\n",
    "        self.svm = SVC(\n",
    "            kernel=svm_kernel,\n",
    "            C=svm_C,\n",
    "            gamma=svm_gamma,\n",
    "            decision_function_shape='ovr',  # One-vs-Rest for multi-class\n",
    "            probability=True  # Enable probability estimates\n",
    "        )\n",
    "        \n",
    "        self._is_fitted = False\n",
    "    \n",
    "    def extract_features(self, X: np.ndarray, batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract features from HHT matrices using the CNN encoder.\n",
    "        \n",
    "        Args:\n",
    "            X: Input array of shape (n_samples, height, width) or (n_samples, 1, height, width)\n",
    "            batch_size: Batch size for processing\n",
    "        \n",
    "        Returns:\n",
    "            Feature array of shape (n_samples, feature_dim)\n",
    "        \"\"\"\n",
    "        self.encoder.eval()\n",
    "        \n",
    "        # Ensure correct shape\n",
    "        if X.ndim == 3:\n",
    "            X = X[:, np.newaxis, :, :]  # Add channel dimension\n",
    "        \n",
    "        features_list = []\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch = torch.tensor(X[i:i+batch_size], dtype=torch.float32).to(self.device)\n",
    "                batch_features = self.encoder(batch)\n",
    "                features_list.append(batch_features.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(features_list)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Fit the classifier (extract features and train SVM).\n",
    "        \n",
    "        Args:\n",
    "            X: Training HHT matrices of shape (n_samples, height, width)\n",
    "            y: Training labels of shape (n_samples,)\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \"\"\"\n",
    "        print(\"Extracting features from training data...\")\n",
    "        features = self.extract_features(X, batch_size)\n",
    "        \n",
    "        print(\"Normalizing features...\")\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        \n",
    "        print(\"Training SVM classifier...\")\n",
    "        self.svm.fit(features_scaled, y)\n",
    "        \n",
    "        self._is_fitted = True\n",
    "        print(\"Training complete!\")\n",
    "    \n",
    "    def predict(self, X: np.ndarray, batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class labels for samples.\n",
    "        \n",
    "        Args:\n",
    "            X: Input HHT matrices of shape (n_samples, height, width)\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \n",
    "        Returns:\n",
    "            Predicted labels of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        if not self._is_fitted:\n",
    "            raise RuntimeError(\"Classifier must be fitted before predicting\")\n",
    "        \n",
    "        features = self.extract_features(X, batch_size)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        return self.svm.predict(features_scaled)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray, batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class probabilities for samples.\n",
    "        \n",
    "        Args:\n",
    "            X: Input HHT matrices of shape (n_samples, height, width)\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \n",
    "        Returns:\n",
    "            Probability array of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        if not self._is_fitted:\n",
    "            raise RuntimeError(\"Classifier must be fitted before predicting\")\n",
    "        \n",
    "        features = self.extract_features(X, batch_size)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        return self.svm.predict_proba(features_scaled)\n",
    "    \n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray, \n",
    "                 batch_size: int = 32) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate the classifier on test data.\n",
    "        \n",
    "        Args:\n",
    "            X: Test HHT matrices\n",
    "            y: True labels\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing accuracy, predictions, and classification report\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X, batch_size)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': y_pred,\n",
    "            'classification_report': classification_report(y, y_pred),\n",
    "            'confusion_matrix': confusion_matrix(y, y_pred)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. End-to-End Training with Encoder Fine-tuning (Optional)\n",
    "\n",
    "For better performance, you can fine-tune the encoder alongside training a neural network classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sEMGHHTEndToEndClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    End-to-end trainable classifier with CNN encoder and linear classification head.\n",
    "    \n",
    "    This version allows the encoder to be fine-tuned during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_classes: int = 4,\n",
    "                 in_channels: int = 1,\n",
    "                 base_channels: int = 64,\n",
    "                 num_encoder_layers: int = 3,\n",
    "                 dropout_rate: float = 0.5):\n",
    "        super(sEMGHHTEndToEndClassifier, self).__init__()\n",
    "        \n",
    "        self.encoder = sEMGHHTEncoder(\n",
    "            in_channels=in_channels,\n",
    "            base_channels=base_channels,\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        \n",
    "        feature_dim = self.encoder.get_feature_dim()\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.encoder(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "    \n",
    "    def get_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Extract features without classification.\"\"\"\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_end_to_end(model: nn.Module,\n",
    "                     X_train: np.ndarray,\n",
    "                     y_train: np.ndarray,\n",
    "                     X_val: np.ndarray,\n",
    "                     y_val: np.ndarray,\n",
    "                     epochs: int = 50,\n",
    "                     batch_size: int = 16,\n",
    "                     learning_rate: float = 0.001,\n",
    "                     device: torch.device = torch.device('cpu')) -> dict:\n",
    "    \"\"\"\n",
    "    Train the end-to-end model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        X_train, y_train: Training data and labels\n",
    "        X_val, y_val: Validation data and labels\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        learning_rate: Learning rate\n",
    "        device: Device to train on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing training history\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Ensure correct shape\n",
    "    if X_train.ndim == 3:\n",
    "        X_train = X_train[:, np.newaxis, :, :]\n",
    "        X_val = X_val[:, np.newaxis, :, :]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.long)\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * batch_X.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += batch_y.size(0)\n",
    "            train_correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                val_loss += loss.item() * batch_X.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += batch_y.size(0)\n",
    "                val_correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_svm_classifier(classifier: sEMGHHTClassifier, path: str):\n",
    "    \"\"\"Save the SVM-based classifier to disk.\"\"\"\n",
    "    # Save encoder\n",
    "    torch.save(classifier.encoder.state_dict(), f\"{path}_encoder.pt\")\n",
    "    \n",
    "    # Save scaler and SVM\n",
    "    with open(f\"{path}_scaler.pkl\", 'wb') as f:\n",
    "        pickle.dump(classifier.scaler, f)\n",
    "    \n",
    "    with open(f\"{path}_svm.pkl\", 'wb') as f:\n",
    "        pickle.dump(classifier.svm, f)\n",
    "    \n",
    "    print(f\"Classifier saved to {path}_*.pt/pkl\")\n",
    "\n",
    "def load_svm_classifier(path: str, device: torch.device = torch.device('cpu')) -> sEMGHHTClassifier:\n",
    "    \"\"\"Load a saved SVM-based classifier.\"\"\"\n",
    "    classifier = sEMGHHTClassifier(device=device)\n",
    "    \n",
    "    # Load encoder\n",
    "    classifier.encoder.load_state_dict(torch.load(f\"{path}_encoder.pt\", map_location=device))\n",
    "    \n",
    "    # Load scaler and SVM\n",
    "    with open(f\"{path}_scaler.pkl\", 'rb') as f:\n",
    "        classifier.scaler = pickle.load(f)\n",
    "    \n",
    "    with open(f\"{path}_svm.pkl\", 'rb') as f:\n",
    "        classifier.svm = pickle.load(f)\n",
    "    \n",
    "    classifier._is_fitted = True\n",
    "    print(f\"Classifier loaded from {path}_*.pt/pkl\")\n",
    "    return classifier\n",
    "\n",
    "def save_e2e_model(model: nn.Module, path: str):\n",
    "    \"\"\"Save the end-to-end model.\"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_e2e_model(path: str, n_classes: int = 4, \n",
    "                   device: torch.device = torch.device('cpu')) -> sEMGHHTEndToEndClassifier:\n",
    "    \"\"\"Load a saved end-to-end model.\"\"\"\n",
    "    model = sEMGHHTEndToEndClassifier(n_classes=n_classes)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data Training | 真实数据训练\n",
    "\n",
    "Train the classifier on real sEMG HHT data from the Kaggle dataset.\n",
    "在 Kaggle 数据集的真实 sEMG HHT 数据上训练分类器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "# 导入所需模块\n",
    "import glob\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse filename to extract labels.\n",
    "    解析文件名以提取标签。\n",
    "    \n",
    "    Returns None if filename starts with 'Test' (unlabeled test data)\n",
    "    如果文件名以 'Test' 开头则返回 None（未标记的测试数据）\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    # Skip files that start with 'Test'\n",
    "    # 跳过以 'Test' 开头的文件\n",
    "    if basename.lower().startswith('test'):\n",
    "        return None\n",
    "    \n",
    "    # Extract gender (M or F)\n",
    "    # 提取性别（M 或 F）\n",
    "    gender_match = re.search(r'[_-]([MF])[_-]', basename)\n",
    "    if not gender_match:\n",
    "        return None\n",
    "    gender = gender_match.group(1)\n",
    "    \n",
    "    # Extract movement quality\n",
    "    # 提取动作质量\n",
    "    basename_lower = basename.lower()\n",
    "    if 'fatiguetest' in basename_lower or 'full' in basename_lower:\n",
    "        movement = 'full'\n",
    "    elif 'half' in basename_lower:\n",
    "        movement = 'half'\n",
    "    elif 'invalid' in basename_lower or 'wrong' in basename_lower:\n",
    "        movement = 'invalid'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return {'gender': gender, 'movement': movement}\n",
    "\n",
    "def load_data_from_directory(data_dir):\n",
    "    \"\"\"\n",
    "    Load HHT matrices from npz files.\n",
    "    从 npz 文件加载 HHT 矩阵。\n",
    "    \"\"\"\n",
    "    npz_files = glob.glob(os.path.join(data_dir, '*.npz'))\n",
    "    \n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    filenames = []\n",
    "    test_files = []\n",
    "    \n",
    "    # Create label encoder\n",
    "    # 创建标签编码器\n",
    "    all_classes = ['M_full', 'M_half', 'M_invalid', 'F_full', 'F_half', 'F_invalid']\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_classes)\n",
    "    \n",
    "    for npz_file in npz_files:\n",
    "        labels = parse_filename(npz_file)\n",
    "        \n",
    "        if labels is None:\n",
    "            test_files.append(npz_file)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            data = np.load(npz_file)\n",
    "            if 'hht' in data:\n",
    "                hht_matrix = data['hht']\n",
    "            else:\n",
    "                hht_matrix = data[list(data.keys())[0]]\n",
    "            \n",
    "            if hht_matrix.shape != (256, 256):\n",
    "                continue\n",
    "            \n",
    "            # Create combined label\n",
    "            # 创建组合标签\n",
    "            combined = f\"{labels['gender']}_{labels['movement']}\"\n",
    "            label = label_encoder.transform([combined])[0]\n",
    "            \n",
    "            X_list.append(hht_matrix)\n",
    "            y_list.append(label)\n",
    "            filenames.append(npz_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error loading {npz_file}: {e}')\n",
    "            continue\n",
    "    \n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    print(f'\\nLoaded {len(X)} training samples | 加载了 {len(X)} 个训练样本')\n",
    "    print(f'Found {len(test_files)} test files | 找到 {len(test_files)} 个测试文件')\n",
    "    print(f'\\nClass distribution | 类别分布:')\n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        count = np.sum(y == i)\n",
    "        print(f'  {class_name}: {count} samples')\n",
    "    \n",
    "    return X, y, filenames, test_files, label_encoder\n",
    "\n",
    "# Load data from Kaggle dataset\n",
    "# 从 Kaggle 数据集加载数据\n",
    "if os.path.exists(DATA_DIR):\n",
    "    X, y, filenames, test_files, label_encoder = load_data_from_directory(DATA_DIR)\n",
    "else:\n",
    "    print(f'Data directory not found | 数据目录未找到: {DATA_DIR}')\n",
    "    print('Please ensure the HILBERTMATRIX_NPZ dataset is added to this notebook.')\n",
    "    print('请确保 HILBERTMATRIX_NPZ 数据集已添加到此笔记本。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real HHT Transformation (For Reference)\n",
    "\n",
    "When you have real sEMG data, you can use the following functions to perform HHT transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier | 训练分类器\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if 'X' in locals() and len(X) > 0:\n",
    "    # Split data | 分割数据\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f'Training set | 训练集: {X_train.shape[0]} samples')\n",
    "    print(f'Validation set | 验证集: {X_val.shape[0]} samples')\n",
    "    \n",
    "    # Initialize classifier | 初始化分类器\n",
    "    classifier = sEMGHHTClassifier(\n",
    "        encoder=None,\n",
    "        svm_kernel='rbf',\n",
    "        svm_C=10.0,\n",
    "        svm_gamma='scale',\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Train | 训练\n",
    "    print('\\n' + '='*60)\n",
    "    print('Training SVM Classifier | 训练 SVM 分类器')\n",
    "    print('='*60)\n",
    "    classifier.fit(X_train, y_train, batch_size=32)\n",
    "    \n",
    "    # Evaluate on training set | 在训练集上评估\n",
    "    train_results = classifier.evaluate(X_train, y_train, batch_size=32)\n",
    "    print(f'\\nTraining Accuracy | 训练准确率: {train_results[\"accuracy\"]:.4f}')\n",
    "    \n",
    "    # Evaluate on validation set | 在验证集上评估\n",
    "    val_results = classifier.evaluate(X_val, y_val, batch_size=32)\n",
    "    print(f'Validation Accuracy | 验证准确率: {val_results[\"accuracy\"]:.4f}')\n",
    "    print('\\nClassification Report | 分类报告:')\n",
    "    print(val_results['classification_report'])\n",
    "else:\n",
    "    print('No data loaded. Please check the data directory.')\n",
    "    print('未加载数据。请检查数据目录。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hht_matrix(signal: np.ndarray, \n",
    "                       fs: float, \n",
    "                       matrix_size: int = 256,\n",
    "                       max_imf: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute HHT (Hilbert-Huang Transform) matrix from a signal.\n",
    "    \n",
    "    This is a reference implementation. For real usage, you may need to install:\n",
    "    pip install PyEMD scipy\n",
    "    \n",
    "    Args:\n",
    "        signal: 1D input signal\n",
    "        fs: Sampling frequency\n",
    "        matrix_size: Output matrix size (matrix_size × matrix_size)\n",
    "        max_imf: Maximum number of IMFs to extract\n",
    "    \n",
    "    Returns:\n",
    "        HHT matrix of shape (matrix_size, matrix_size)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from PyEMD import EMD\n",
    "        from scipy.signal import hilbert\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install PyEMD and scipy: pip install PyEMD scipy\")\n",
    "    \n",
    "    # Perform EMD\n",
    "    emd = EMD()\n",
    "    imfs = emd(signal, max_imf=max_imf)\n",
    "    \n",
    "    # Compute Hilbert transform for each IMF\n",
    "    n_samples = len(signal)\n",
    "    t = np.arange(n_samples) / fs\n",
    "    \n",
    "    # Initialize time-frequency matrix\n",
    "    freq_bins = np.linspace(0, fs/2, matrix_size)\n",
    "    time_bins = np.linspace(0, t[-1], matrix_size)\n",
    "    hht_matrix = np.zeros((matrix_size, matrix_size))\n",
    "    \n",
    "    for imf in imfs:\n",
    "        # Compute analytic signal\n",
    "        analytic = hilbert(imf)\n",
    "        amplitude = np.abs(analytic)\n",
    "        phase = np.unwrap(np.angle(analytic))\n",
    "        \n",
    "        # Compute instantaneous frequency\n",
    "        inst_freq = np.diff(phase) / (2 * np.pi) * fs\n",
    "        inst_freq = np.concatenate([inst_freq, [inst_freq[-1]]])\n",
    "        inst_freq = np.clip(inst_freq, 0, fs/2)\n",
    "        \n",
    "        # Map to time-frequency matrix\n",
    "        for i, (ti, fi, ai) in enumerate(zip(t, inst_freq, amplitude)):\n",
    "            t_idx = int(ti / t[-1] * (matrix_size - 1))\n",
    "            f_idx = int(fi / (fs/2) * (matrix_size - 1))\n",
    "            \n",
    "            t_idx = np.clip(t_idx, 0, matrix_size - 1)\n",
    "            f_idx = np.clip(f_idx, 0, matrix_size - 1)\n",
    "            \n",
    "            hht_matrix[f_idx, t_idx] += ai\n",
    "    \n",
    "    # Normalize\n",
    "    if hht_matrix.max() > 0:\n",
    "        hht_matrix = hht_matrix / hht_matrix.max()\n",
    "    \n",
    "    return hht_matrix.astype(np.float32)\n",
    "\n",
    "\n",
    "# Example usage (uncomment when you have real data):\n",
    "# signal = np.random.randn(1000)  # Replace with real sEMG signal\n",
    "# fs = 1000  # Sampling frequency in Hz\n",
    "# hht_matrix = compute_hht_matrix(signal, fs, matrix_size=256)\n",
    "# plt.imshow(hht_matrix, aspect='auto', cmap='hot', origin='lower')\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test files | 对测试文件进行推理\n",
    "if 'classifier' in locals() and len(test_files) > 0:\n",
    "    print('\\n' + '='*60)\n",
    "    print('Running Inference on Test Files | 对测试文件进行推理')\n",
    "    print('='*60)\n",
    "    \n",
    "    X_test_list = []\n",
    "    valid_test_files = []\n",
    "    \n",
    "    for test_file in test_files[:10]:  # Limit to first 10 for demo\n",
    "        try:\n",
    "            data = np.load(test_file)\n",
    "            if 'hht' in data:\n",
    "                hht_matrix = data['hht']\n",
    "            else:\n",
    "                hht_matrix = data[list(data.keys())[0]]\n",
    "            \n",
    "            if hht_matrix.shape == (256, 256):\n",
    "                X_test_list.append(hht_matrix)\n",
    "                valid_test_files.append(test_file)\n",
    "        except Exception as e:\n",
    "            print(f'Error loading {test_file}: {e}')\n",
    "    \n",
    "    if len(X_test_list) > 0:\n",
    "        X_test = np.array(X_test_list, dtype=np.float32)\n",
    "        y_test_pred = classifier.predict(X_test, batch_size=32)\n",
    "        y_test_proba = classifier.predict_proba(X_test, batch_size=32)\n",
    "        \n",
    "        svm_classes = classifier.svm.classes_\n",
    "        \n",
    "        print(f'\\nPredictions for {len(valid_test_files)} test files:')\n",
    "        print(f'前 {len(valid_test_files)} 个测试文件的预测结果：\\n')\n",
    "        for i, (filename, pred, proba) in enumerate(zip(valid_test_files, y_test_pred, y_test_proba)):\n",
    "            pred_idx = np.where(svm_classes == pred)[0][0]\n",
    "            class_name = label_encoder.classes_[pred]\n",
    "            confidence = proba[pred_idx]\n",
    "            print(f'{os.path.basename(filename)}: {class_name} (confidence: {confidence:.4f})')\n",
    "else:\n",
    "    print('No test files to process or classifier not trained.')\n",
    "    print('没有测试文件或分类器未训练。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "1. **CNN Encoder Architecture**: 3-layer convolutional network with Instance Normalization and LeakyReLU\n",
    "2. **SVM Classifier**: Multi-class classification using extracted CNN features\n",
    "3. **End-to-End Model**: Optional fully trainable model with neural network classifier\n",
    "4. **Real Data Training**: Load and train on actual HHT matrices from Kaggle dataset\n",
    "5. **HHT Computation**: Reference implementation for real sEMG signals\n",
    "\n",
    "### Hyperparameter Tuning:\n",
    "- `base_channels`: Number of channels in first conv layer (default: 64)\n",
    "- `svm_C`: SVM regularization parameter\n",
    "- `svm_kernel`: SVM kernel type ('rbf', 'linear', 'poly')\n",
    "- `learning_rate`: Learning rate for end-to-end training\n",
    "- `dropout_rate`: Dropout rate in end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"sEMG-HHT CNN Classifier - Ready for Use\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDevice: {device}\")\n",
    "print(f\"Encoder feature dimension: {encoder.get_feature_dim()}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"\\nClass names:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}