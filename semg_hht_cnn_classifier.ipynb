{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sEMG-HHT CNN Classifier for Movement Quality and Fatigue Classification\n",
    "\n",
    "This notebook implements a Convolutional Neural Network (CNN) encoder with SVM classifier for classifying surface electromyography (sEMG) signals based on movement quality and fatigue levels.\n",
    "\n",
    "## Architecture Overview\n",
    "- **Input**: 256×256 matrix from HHT (Hilbert-Huang Transform) of sEMG signals\n",
    "- **Encoder**: 3-layer CNN with Conv2D + InstanceNorm + LeakyReLU\n",
    "- **Pooling**: Global Average Pooling\n",
    "- **Classifier**: SVM for multi-class classification\n",
    "\n",
    "## Requirements\n",
    "- PyTorch\n",
    "- scikit-learn\n",
    "- NumPy\n",
    "- Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if running on Kaggle or fresh environment)\n",
    "# !pip install torch torchvision scikit-learn numpy matplotlib scipy PyEMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from typing import Tuple, Optional\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings that are not critical for this demo\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CNN Encoder Architecture\n",
    "\n",
    "The encoder consists of 3 convolutional layers, each with:\n",
    "- Conv2D (kernel=3, stride=2, padding=1)\n",
    "- Instance Normalization\n",
    "- LeakyReLU activation\n",
    "\n",
    "This progressively reduces the spatial dimensions while extracting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block with Conv2D, InstanceNorm, and LeakyReLU.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, \n",
    "                 kernel_size: int = 3, stride: int = 2, padding: int = 1,\n",
    "                 leaky_slope: float = 0.2):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
    "                              kernel_size=kernel_size, \n",
    "                              stride=stride, \n",
    "                              padding=padding)\n",
    "        self.instance_norm = nn.InstanceNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_slope)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.instance_norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class sEMGHHTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN Encoder for sEMG-HHT matrix classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: 1×256×256 (single-channel HHT matrix)\n",
    "    - 3 ConvBlocks with increasing channels\n",
    "    - Global Average Pooling\n",
    "    - Output: Feature vector for SVM classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 1, \n",
    "                 base_channels: int = 64,\n",
    "                 num_layers: int = 3,\n",
    "                 leaky_slope: float = 0.2):\n",
    "        super(sEMGHHTEncoder, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Build convolutional layers\n",
    "        layers = []\n",
    "        current_channels = in_channels\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            out_channels = base_channels * (2 ** i)\n",
    "            layers.append(ConvBlock(\n",
    "                in_channels=current_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                leaky_slope=leaky_slope\n",
    "            ))\n",
    "            current_channels = out_channels\n",
    "        \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Calculate output feature dimension\n",
    "        self.feature_dim = base_channels * (2 ** (num_layers - 1))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, channels, height, width)\n",
    "        \n",
    "        Returns:\n",
    "            Feature tensor of shape (batch, feature_dim)\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch, feature_dim)\n",
    "        return x\n",
    "    \n",
    "    def get_feature_dim(self) -> int:\n",
    "        \"\"\"Return the output feature dimension.\"\"\"\n",
    "        return self.feature_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the architecture\n",
    "encoder = sEMGHHTEncoder(in_channels=1, base_channels=64, num_layers=3)\n",
    "print(\"Encoder Architecture:\")\n",
    "print(encoder)\n",
    "print(f\"\\nOutput feature dimension: {encoder.get_feature_dim()}\")\n",
    "\n",
    "# Test with sample input\n",
    "sample_input = torch.randn(4, 1, 256, 256)  # batch=4, channels=1, height=256, width=256\n",
    "sample_output = encoder(sample_input)\n",
    "print(f\"\\nInput shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {sample_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete Classification Pipeline\n",
    "\n",
    "This class combines the CNN encoder with an SVM classifier for end-to-end classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sEMGHHTClassifier:\n",
    "    \"\"\"\n",
    "    Complete classification pipeline combining CNN encoder and SVM classifier.\n",
    "    \n",
    "    The pipeline:\n",
    "    1. Extracts features using CNN encoder\n",
    "    2. Normalizes features using StandardScaler\n",
    "    3. Classifies using SVM (supports multi-class)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 encoder: Optional[sEMGHHTEncoder] = None,\n",
    "                 svm_kernel: str = 'rbf',\n",
    "                 svm_C: float = 1.0,\n",
    "                 svm_gamma: str = 'scale',\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Args:\n",
    "            encoder: Pre-trained or new CNN encoder (creates default if None)\n",
    "            svm_kernel: SVM kernel type ('rbf', 'linear', 'poly')\n",
    "            svm_C: SVM regularization parameter\n",
    "            svm_gamma: SVM gamma parameter\n",
    "            device: Device to run the encoder on\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize encoder\n",
    "        if encoder is None:\n",
    "            self.encoder = sEMGHHTEncoder(\n",
    "                in_channels=1, \n",
    "                base_channels=64, \n",
    "                num_layers=3\n",
    "            )\n",
    "        else:\n",
    "            self.encoder = encoder\n",
    "        \n",
    "        self.encoder.to(self.device)\n",
    "        \n",
    "        # Initialize scaler and SVM\n",
    "        self.scaler = StandardScaler()\n",
    "        self.svm = SVC(\n",
    "            kernel=svm_kernel,\n",
    "            C=svm_C,\n",
    "            gamma=svm_gamma,\n",
    "            decision_function_shape='ovr',  # One-vs-Rest for multi-class\n",
    "            probability=True  # Enable probability estimates\n",
    "        )\n",
    "        \n",
    "        self._is_fitted = False\n",
    "    \n",
    "    def extract_features(self, X: np.ndarray, batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract features from HHT matrices using the CNN encoder.\n",
    "        \n",
    "        Args:\n",
    "            X: Input array of shape (n_samples, height, width) or (n_samples, 1, height, width)\n",
    "            batch_size: Batch size for processing\n",
    "        \n",
    "        Returns:\n",
    "            Feature array of shape (n_samples, feature_dim)\n",
    "        \"\"\"\n",
    "        self.encoder.eval()\n",
    "        \n",
    "        # Ensure correct shape\n",
    "        if X.ndim == 3:\n",
    "            X = X[:, np.newaxis, :, :]  # Add channel dimension\n",
    "        \n",
    "        features_list = []\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch = torch.tensor(X[i:i+batch_size], dtype=torch.float32).to(self.device)\n",
    "                batch_features = self.encoder(batch)\n",
    "                features_list.append(batch_features.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(features_list)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Fit the classifier (extract features and train SVM).\n",
    "        \n",
    "        Args:\n",
    "            X: Training HHT matrices of shape (n_samples, height, width)\n",
    "            y: Training labels of shape (n_samples,)\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \"\"\"\n",
    "        print(\"Extracting features from training data...\")\n",
    "        features = self.extract_features(X, batch_size)\n",
    "        \n",
    "        print(\"Normalizing features...\")\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        \n",
    "        print(\"Training SVM classifier...\")\n",
    "        self.svm.fit(features_scaled, y)\n",
    "        \n",
    "        self._is_fitted = True\n",
    "        print(\"Training complete!\")\n",
    "    \n",
    "    def predict(self, X: np.ndarray, batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class labels for samples.\n",
    "        \n",
    "        Args:\n",
    "            X: Input HHT matrices of shape (n_samples, height, width)\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \n",
    "        Returns:\n",
    "            Predicted labels of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        if not self._is_fitted:\n",
    "            raise RuntimeError(\"Classifier must be fitted before predicting\")\n",
    "        \n",
    "        features = self.extract_features(X, batch_size)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        return self.svm.predict(features_scaled)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray, batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class probabilities for samples.\n",
    "        \n",
    "        Args:\n",
    "            X: Input HHT matrices of shape (n_samples, height, width)\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \n",
    "        Returns:\n",
    "            Probability array of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        if not self._is_fitted:\n",
    "            raise RuntimeError(\"Classifier must be fitted before predicting\")\n",
    "        \n",
    "        features = self.extract_features(X, batch_size)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        return self.svm.predict_proba(features_scaled)\n",
    "    \n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray, \n",
    "                 batch_size: int = 32) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate the classifier on test data.\n",
    "        \n",
    "        Args:\n",
    "            X: Test HHT matrices\n",
    "            y: True labels\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing accuracy, predictions, and classification report\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X, batch_size)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': y_pred,\n",
    "            'classification_report': classification_report(y, y_pred),\n",
    "            'confusion_matrix': confusion_matrix(y, y_pred)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Generation and HHT Simulation\n",
    "\n",
    "For demonstration purposes, we create synthetic HHT matrices representing different movement quality and fatigue levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_hht_data(n_samples_per_class: int = 100,\n",
    "                                 n_classes: int = 4,\n",
    "                                 matrix_size: int = 256,\n",
    "                                 random_state: int = 42) -> Tuple[np.ndarray, np.ndarray, list]:\n",
    "    \"\"\"\n",
    "    Generate synthetic HHT matrices for demonstration.\n",
    "    \n",
    "    Creates HHT-like matrices with different frequency and amplitude patterns\n",
    "    to simulate different movement quality and fatigue levels:\n",
    "    - Class 0: High quality, Low fatigue (strong, clear patterns)\n",
    "    - Class 1: High quality, High fatigue (strong patterns, more noise)\n",
    "    - Class 2: Low quality, Low fatigue (weaker patterns, clear)\n",
    "    - Class 3: Low quality, High fatigue (weak patterns, noisy)\n",
    "    \n",
    "    Args:\n",
    "        n_samples_per_class: Number of samples per class\n",
    "        n_classes: Number of classes\n",
    "        matrix_size: Size of the HHT matrix (matrix_size × matrix_size)\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        X: Array of HHT matrices (n_samples, matrix_size, matrix_size)\n",
    "        y: Array of labels\n",
    "        class_names: List of class descriptions\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    class_names = [\n",
    "        'High Quality, Low Fatigue',\n",
    "        'High Quality, High Fatigue',\n",
    "        'Low Quality, Low Fatigue',\n",
    "        'Low Quality, High Fatigue'\n",
    "    ]\n",
    "    \n",
    "    # Parameters for each class\n",
    "    class_params = [\n",
    "        {'amplitude': 1.0, 'noise': 0.1, 'freq_spread': 0.3},  # High Q, Low F\n",
    "        {'amplitude': 1.0, 'noise': 0.4, 'freq_spread': 0.5},  # High Q, High F\n",
    "        {'amplitude': 0.5, 'noise': 0.1, 'freq_spread': 0.6},  # Low Q, Low F\n",
    "        {'amplitude': 0.5, 'noise': 0.4, 'freq_spread': 0.8},  # Low Q, High F\n",
    "    ]\n",
    "    \n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    # Time and frequency axes\n",
    "    t = np.linspace(0, 1, matrix_size)\n",
    "    f = np.linspace(0, 100, matrix_size)\n",
    "    T, F = np.meshgrid(t, f)\n",
    "    \n",
    "    for class_idx in range(min(n_classes, len(class_params))):\n",
    "        params = class_params[class_idx]\n",
    "        \n",
    "        for _ in range(n_samples_per_class):\n",
    "            # Generate base HHT-like pattern\n",
    "            center_freq = 30 + np.random.uniform(-10, 10)\n",
    "            center_time = 0.5 + np.random.uniform(-0.2, 0.2)\n",
    "            \n",
    "            # Create Gaussian-like energy distribution in time-frequency plane\n",
    "            hht_matrix = params['amplitude'] * np.exp(\n",
    "                -((F - center_freq) ** 2) / (2 * (10 * params['freq_spread']) ** 2)\n",
    "                -((T - center_time) ** 2) / (2 * (0.2) ** 2)\n",
    "            )\n",
    "            \n",
    "            # Add secondary components\n",
    "            for _ in range(np.random.randint(1, 4)):\n",
    "                sec_freq = np.random.uniform(10, 80)\n",
    "                sec_time = np.random.uniform(0.2, 0.8)\n",
    "                sec_amp = params['amplitude'] * np.random.uniform(0.2, 0.5)\n",
    "                \n",
    "                hht_matrix += sec_amp * np.exp(\n",
    "                    -((F - sec_freq) ** 2) / (2 * (8 * params['freq_spread']) ** 2)\n",
    "                    -((T - sec_time) ** 2) / (2 * (0.15) ** 2)\n",
    "                )\n",
    "            \n",
    "            # Add noise\n",
    "            noise = params['noise'] * np.random.randn(matrix_size, matrix_size)\n",
    "            hht_matrix += noise\n",
    "            \n",
    "            # Normalize to [0, 1] range\n",
    "            hht_matrix = (hht_matrix - hht_matrix.min()) / (hht_matrix.max() - hht_matrix.min() + 1e-8)\n",
    "            \n",
    "            X_list.append(hht_matrix)\n",
    "            y_list.append(class_idx)\n",
    "    \n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    # Shuffle data\n",
    "    shuffle_idx = np.random.permutation(len(y))\n",
    "    X = X[shuffle_idx]\n",
    "    y = y[shuffle_idx]\n",
    "    \n",
    "    return X, y, class_names[:n_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "print(\"Generating synthetic HHT data...\")\n",
    "X, y, class_names = generate_synthetic_hht_data(\n",
    "    n_samples_per_class=100,\n",
    "    n_classes=4,\n",
    "    matrix_size=256,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample HHT matrices from each class\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, class_idx in enumerate(range(4)):\n",
    "    # Find a sample from this class\n",
    "    sample_idx = np.where(y == class_idx)[0][0]\n",
    "    \n",
    "    im = axes[i].imshow(X[sample_idx], aspect='auto', cmap='hot', \n",
    "                        extent=[0, 1, 0, 100], origin='lower')\n",
    "    axes[i].set_title(f'Class {class_idx}: {class_names[class_idx]}')\n",
    "    axes[i].set_xlabel('Time (normalized)')\n",
    "    axes[i].set_ylabel('Frequency (Hz)')\n",
    "    plt.colorbar(im, ax=axes[i], label='Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Sample HHT Matrices from Each Class', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the classifier\n",
    "classifier = sEMGHHTClassifier(\n",
    "    encoder=None,  # Use default encoder\n",
    "    svm_kernel='rbf',\n",
    "    svm_C=10.0,\n",
    "    svm_gamma='scale',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Train\n",
    "classifier.fit(X_train, y_train, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "results = classifier.evaluate(X_test, y_test, batch_size=16)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {results['accuracy']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(results['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cm = results['confusion_matrix']\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "# Set labels\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       xticklabels=[f'Class {i}' for i in range(4)],\n",
    "       yticklabels=[f'Class {i}' for i in range(4)],\n",
    "       title='Confusion Matrix',\n",
    "       ylabel='True Label',\n",
    "       xlabel='Predicted Label')\n",
    "\n",
    "# Rotate tick labels\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. End-to-End Training with Encoder Fine-tuning (Optional)\n",
    "\n",
    "For better performance, you can fine-tune the encoder alongside training a neural network classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sEMGHHTEndToEndClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    End-to-end trainable classifier with CNN encoder and linear classification head.\n",
    "    \n",
    "    This version allows the encoder to be fine-tuned during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_classes: int = 4,\n",
    "                 in_channels: int = 1,\n",
    "                 base_channels: int = 64,\n",
    "                 num_encoder_layers: int = 3,\n",
    "                 dropout_rate: float = 0.5):\n",
    "        super(sEMGHHTEndToEndClassifier, self).__init__()\n",
    "        \n",
    "        self.encoder = sEMGHHTEncoder(\n",
    "            in_channels=in_channels,\n",
    "            base_channels=base_channels,\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        \n",
    "        feature_dim = self.encoder.get_feature_dim()\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.encoder(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "    \n",
    "    def get_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Extract features without classification.\"\"\"\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_end_to_end(model: nn.Module,\n",
    "                     X_train: np.ndarray,\n",
    "                     y_train: np.ndarray,\n",
    "                     X_val: np.ndarray,\n",
    "                     y_val: np.ndarray,\n",
    "                     epochs: int = 50,\n",
    "                     batch_size: int = 16,\n",
    "                     learning_rate: float = 0.001,\n",
    "                     device: torch.device = torch.device('cpu')) -> dict:\n",
    "    \"\"\"\n",
    "    Train the end-to-end model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        X_train, y_train: Training data and labels\n",
    "        X_val, y_val: Validation data and labels\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        learning_rate: Learning rate\n",
    "        device: Device to train on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing training history\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Ensure correct shape\n",
    "    if X_train.ndim == 3:\n",
    "        X_train = X_train[:, np.newaxis, :, :]\n",
    "        X_val = X_val[:, np.newaxis, :, :]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.long)\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * batch_X.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += batch_y.size(0)\n",
    "            train_correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                val_loss += loss.item() * batch_X.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += batch_y.size(0)\n",
    "                val_correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train end-to-end model\n",
    "e2e_model = sEMGHHTEndToEndClassifier(\n",
    "    n_classes=4,\n",
    "    in_channels=1,\n",
    "    base_channels=64,\n",
    "    num_encoder_layers=3,\n",
    "    dropout_rate=0.5\n",
    ")\n",
    "\n",
    "print(\"Training end-to-end model...\")\n",
    "history = train_end_to_end(\n",
    "    model=e2e_model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_test,\n",
    "    y_val=y_test,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.001,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy')\n",
    "axes[1].plot(history['val_acc'], label='Val Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_svm_classifier(classifier: sEMGHHTClassifier, path: str):\n",
    "    \"\"\"Save the SVM-based classifier to disk.\"\"\"\n",
    "    # Save encoder\n",
    "    torch.save(classifier.encoder.state_dict(), f\"{path}_encoder.pt\")\n",
    "    \n",
    "    # Save scaler and SVM\n",
    "    with open(f\"{path}_scaler.pkl\", 'wb') as f:\n",
    "        pickle.dump(classifier.scaler, f)\n",
    "    \n",
    "    with open(f\"{path}_svm.pkl\", 'wb') as f:\n",
    "        pickle.dump(classifier.svm, f)\n",
    "    \n",
    "    print(f\"Classifier saved to {path}_*.pt/pkl\")\n",
    "\n",
    "def load_svm_classifier(path: str, device: torch.device = torch.device('cpu')) -> sEMGHHTClassifier:\n",
    "    \"\"\"Load a saved SVM-based classifier.\"\"\"\n",
    "    classifier = sEMGHHTClassifier(device=device)\n",
    "    \n",
    "    # Load encoder\n",
    "    classifier.encoder.load_state_dict(torch.load(f\"{path}_encoder.pt\", map_location=device))\n",
    "    \n",
    "    # Load scaler and SVM\n",
    "    with open(f\"{path}_scaler.pkl\", 'rb') as f:\n",
    "        classifier.scaler = pickle.load(f)\n",
    "    \n",
    "    with open(f\"{path}_svm.pkl\", 'rb') as f:\n",
    "        classifier.svm = pickle.load(f)\n",
    "    \n",
    "    classifier._is_fitted = True\n",
    "    print(f\"Classifier loaded from {path}_*.pt/pkl\")\n",
    "    return classifier\n",
    "\n",
    "def save_e2e_model(model: nn.Module, path: str):\n",
    "    \"\"\"Save the end-to-end model.\"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_e2e_model(path: str, n_classes: int = 4, \n",
    "                   device: torch.device = torch.device('cpu')) -> sEMGHHTEndToEndClassifier:\n",
    "    \"\"\"Load a saved end-to-end model.\"\"\"\n",
    "    model = sEMGHHTEndToEndClassifier(n_classes=n_classes)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Save models (uncomment to use)\n",
    "# save_svm_classifier(classifier, 'semg_hht_classifier')\n",
    "# save_e2e_model(e2e_model, 'semg_hht_e2e_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real HHT Transformation (For Reference)\n",
    "\n",
    "When you have real sEMG data, you can use the following functions to perform HHT transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hht_matrix(signal: np.ndarray, \n",
    "                       fs: float, \n",
    "                       matrix_size: int = 256,\n",
    "                       max_imf: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute HHT (Hilbert-Huang Transform) matrix from a signal.\n",
    "    \n",
    "    This is a reference implementation. For real usage, you may need to install:\n",
    "    pip install PyEMD scipy\n",
    "    \n",
    "    Args:\n",
    "        signal: 1D input signal\n",
    "        fs: Sampling frequency\n",
    "        matrix_size: Output matrix size (matrix_size × matrix_size)\n",
    "        max_imf: Maximum number of IMFs to extract\n",
    "    \n",
    "    Returns:\n",
    "        HHT matrix of shape (matrix_size, matrix_size)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from PyEMD import EMD\n",
    "        from scipy.signal import hilbert\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install PyEMD and scipy: pip install PyEMD scipy\")\n",
    "    \n",
    "    # Perform EMD\n",
    "    emd = EMD()\n",
    "    imfs = emd(signal, max_imf=max_imf)\n",
    "    \n",
    "    # Compute Hilbert transform for each IMF\n",
    "    n_samples = len(signal)\n",
    "    t = np.arange(n_samples) / fs\n",
    "    \n",
    "    # Initialize time-frequency matrix\n",
    "    freq_bins = np.linspace(0, fs/2, matrix_size)\n",
    "    time_bins = np.linspace(0, t[-1], matrix_size)\n",
    "    hht_matrix = np.zeros((matrix_size, matrix_size))\n",
    "    \n",
    "    for imf in imfs:\n",
    "        # Compute analytic signal\n",
    "        analytic = hilbert(imf)\n",
    "        amplitude = np.abs(analytic)\n",
    "        phase = np.unwrap(np.angle(analytic))\n",
    "        \n",
    "        # Compute instantaneous frequency\n",
    "        inst_freq = np.diff(phase) / (2 * np.pi) * fs\n",
    "        inst_freq = np.concatenate([inst_freq, [inst_freq[-1]]])\n",
    "        inst_freq = np.clip(inst_freq, 0, fs/2)\n",
    "        \n",
    "        # Map to time-frequency matrix\n",
    "        for i, (ti, fi, ai) in enumerate(zip(t, inst_freq, amplitude)):\n",
    "            t_idx = int(ti / t[-1] * (matrix_size - 1))\n",
    "            f_idx = int(fi / (fs/2) * (matrix_size - 1))\n",
    "            \n",
    "            t_idx = np.clip(t_idx, 0, matrix_size - 1)\n",
    "            f_idx = np.clip(f_idx, 0, matrix_size - 1)\n",
    "            \n",
    "            hht_matrix[f_idx, t_idx] += ai\n",
    "    \n",
    "    # Normalize\n",
    "    if hht_matrix.max() > 0:\n",
    "        hht_matrix = hht_matrix / hht_matrix.max()\n",
    "    \n",
    "    return hht_matrix.astype(np.float32)\n",
    "\n",
    "\n",
    "# Example usage (uncomment when you have real data):\n",
    "# signal = np.random.randn(1000)  # Replace with real sEMG signal\n",
    "# fs = 1000  # Sampling frequency in Hz\n",
    "# hht_matrix = compute_hht_matrix(signal, fs, matrix_size=256)\n",
    "# plt.imshow(hht_matrix, aspect='auto', cmap='hot', origin='lower')\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "1. **CNN Encoder Architecture**: 3-layer convolutional network with Instance Normalization and LeakyReLU\n",
    "2. **SVM Classifier**: Multi-class classification using extracted CNN features\n",
    "3. **End-to-End Model**: Optional fully trainable model with neural network classifier\n",
    "4. **Data Generation**: Synthetic HHT matrix generation for demonstration\n",
    "5. **HHT Computation**: Reference implementation for real sEMG signals\n",
    "\n",
    "### For Real Data:\n",
    "1. Load your sEMG signals\n",
    "2. Apply HHT transformation using `compute_hht_matrix()`\n",
    "3. Prepare labels for your classification task\n",
    "4. Train the classifier using `sEMGHHTClassifier` or `sEMGHHTEndToEndClassifier`\n",
    "\n",
    "### Hyperparameter Tuning:\n",
    "- `base_channels`: Number of channels in first conv layer (default: 64)\n",
    "- `svm_C`: SVM regularization parameter\n",
    "- `svm_kernel`: SVM kernel type ('rbf', 'linear', 'poly')\n",
    "- `learning_rate`: Learning rate for end-to-end training\n",
    "- `dropout_rate`: Dropout rate in end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"sEMG-HHT CNN Classifier - Ready for Use\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDevice: {device}\")\n",
    "print(f\"Encoder feature dimension: {encoder.get_feature_dim()}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"\\nClass names:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
