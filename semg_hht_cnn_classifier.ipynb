{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sEMG-HHT åŒåˆ†ç±»å™¨ç³»ç»Ÿ | sEMG-HHT Dual Classifier System\n",
    "\n",
    "## ç³»ç»Ÿæ¶æ„ | System Architecture\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬å®ç°äº†åŒåˆ†ç±»å™¨ç³»ç»Ÿï¼š\n",
    "This notebook implements a dual classifier system:\n",
    "\n",
    "1. **æ·±åº¦å­¦ä¹  CNN** - åŠ¨ä½œè´¨é‡åˆ†ç±»ï¼ˆå…¨ç¨‹ã€åŠç¨‹ã€æ— æ•ˆï¼‰\n",
    "   **Deep Learning CNN** - Action Quality Classification (Full, Half, Invalid)\n",
    "   \n",
    "2. **SVM åˆ†ç±»å™¨** - æ€§åˆ«åˆ†ç±»ï¼ˆç”·æ€§ã€å¥³æ€§ï¼‰\n",
    "   **SVM Classifier** - Gender Classification (Male, Female)\n",
    "\n",
    "## å…³é”®æ”¹è¿› | Key Improvements\n",
    "\n",
    "### è§£å†³è®­ç»ƒé—®é¢˜ | Training Problem Solutions:\n",
    "- âœ… **æ‰©å±•çš„CNNæ¶æ„** - 7å±‚æ·±åº¦ç½‘ç»œï¼Œæ›´å¼ºçš„ç‰¹å¾æå–èƒ½åŠ›\n",
    "  **Expanded CNN Architecture** - 7-layer deep network with stronger feature extraction\n",
    "  \n",
    "- âœ… **æ‰¹å½’ä¸€åŒ–** - åŠ é€Ÿè®­ç»ƒï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±\n",
    "  **Batch Normalization** - Accelerate training, prevent vanishing gradients\n",
    "  \n",
    "- âœ… **Kaimingåˆå§‹åŒ–** - æ­£ç¡®çš„æƒé‡åˆå§‹åŒ–ï¼Œç¡®ä¿æ¢¯åº¦æµåŠ¨\n",
    "  **Kaiming Initialization** - Proper weight init for gradient flow\n",
    "  \n",
    "- âœ… **å­¦ä¹ ç‡é¢„çƒ­** - é˜²æ­¢è®­ç»ƒåˆæœŸä¸ç¨³å®š\n",
    "  **Learning Rate Warmup** - Prevent early training instability\n",
    "  \n",
    "- âœ… **æ¢¯åº¦è£å‰ª** - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "  **Gradient Clipping** - Prevent gradient explosion\n",
    "  \n",
    "- âœ… **æ•°æ®å¢å¼º** - æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›\n",
    "  **Data Augmentation** - Improve model generalization\n",
    "\n",
    "### ç½‘ç»œè§„æ¨¡ | Network Scale:\n",
    "- **è¾“å…¥** Input: 1Ã—256Ã—256\n",
    "- **é€šé“æ•°** Channels: 64 â†’ 128 â†’ 256 â†’ 512 â†’ 1024 â†’ 2048 â†’ 2048\n",
    "- **ç‰¹å¾ç»´åº¦** Feature dim: 2048\n",
    "- **åˆ†ç±»å¤´** Classifier: 2048 â†’ 1024 â†’ 512 â†’ 3 classes\n",
    "\n",
    "## æ•°æ®è¦æ±‚ | Data Requirements\n",
    "\n",
    "- **æ ¼å¼** Format: `.npz` æ–‡ä»¶ï¼ŒåŒ…å« 256Ã—256 HHT çŸ©é˜µ\n",
    "- **é€šé“** Channels: å•é€šé“ï¼ˆç°åº¦å›¾ï¼‰\n",
    "- **å‘½åè§„åˆ™** Naming: `è‚Œè‚‰å_åŠ¨ä½œ_æ€§åˆ«_ç¼–å·.npz`\n",
    "  - ä¾‹å¦‚ Example: `BICEPS_fatiguetest_M_006.npz` (ç”·æ€§ï¼Œå…¨ç¨‹åŠ¨ä½œ)\n",
    "  - ä¾‹å¦‚ Example: `TRICEPS_half_F_012.npz` (å¥³æ€§ï¼ŒåŠç¨‹åŠ¨ä½œ)\n",
    "  - æµ‹è¯•æ–‡ä»¶ Test files: ä»¥ `Test` å¼€å¤´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒé…ç½® | Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# æ£€æµ‹Kaggleç¯å¢ƒ | Detect Kaggle environment\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    DATA_DIR = '/kaggle/input/hilbertmatrix-npz/hht_matrices'\n",
    "    CHECKPOINT_DIR = '/kaggle/working/checkpoints'\n",
    "    print('ğŸƒ åœ¨Kaggleä¸Šè¿è¡Œ | Running on Kaggle')\n",
    "    print(f'ğŸ“ æ•°æ®ç›®å½• | Data directory: {DATA_DIR}')\n",
    "else:\n",
    "    DATA_DIR = './data'\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    print('ğŸ’» æœ¬åœ°è¿è¡Œ | Running locally')\n",
    "    print(f'ğŸ“ æ•°æ®ç›®å½• | Data directory: {DATA_DIR}')\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f'ğŸ’¾ æ£€æŸ¥ç‚¹ç›®å½• | Checkpoint directory: {CHECKPOINT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å¯¼å…¥ä¾èµ– | Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ | Set random seeds\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# æ£€æµ‹è®¾å¤‡ | Detect device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡ | Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'   GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è¶…å‚æ•°é…ç½® | Hyperparameter Configuration\n",
    "\n",
    "åœ¨æ­¤é…ç½®æ‰€æœ‰è®­ç»ƒå‚æ•°ã€‚æ ¹æ®éœ€è¦è°ƒæ•´è¿™äº›å€¼ã€‚\n",
    "Configure all training parameters here. Adjust these values as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# è¶…å‚æ•°é…ç½® | HYPERPARAMETER CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# æ¨¡å‹æ¶æ„ | Model Architecture\n",
    "# -----------------------------------------------------------------------------\n",
    "MODEL_IN_CHANNELS = 1              # è¾“å…¥é€šé“æ•°ï¼ˆç°åº¦å›¾ï¼‰| Input channels (grayscale)\n",
    "MODEL_BASE_CHANNELS = 64           # åŸºç¡€é€šé“æ•° | Base channels\n",
    "MODEL_NUM_LAYERS = 7               # å·ç§¯å±‚æ•°ï¼ˆæ‰©å±•è‡³7å±‚ï¼‰| Number of conv layers (expanded to 7)\n",
    "MODEL_DROPOUT_RATE = 0.5           # Dropoutç‡ | Dropout rate\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# åŠ¨ä½œè´¨é‡CNNè®­ç»ƒé…ç½® | Action Quality CNN Training Config\n",
    "# -----------------------------------------------------------------------------\n",
    "ACTION_EPOCHS = 100                # è®­ç»ƒè½®æ•° | Training epochs\n",
    "ACTION_BATCH_SIZE = 16             # æ‰¹æ¬¡å¤§å° | Batch size\n",
    "ACTION_LEARNING_RATE = 0.0001      # å­¦ä¹ ç‡ï¼ˆé™ä½ä»¥æé«˜ç¨³å®šæ€§ï¼‰| Learning rate (lowered for stability)\n",
    "ACTION_WEIGHT_DECAY = 1e-4         # L2æ­£åˆ™åŒ– | L2 regularization\n",
    "ACTION_WARMUP_EPOCHS = 5           # å­¦ä¹ ç‡é¢„çƒ­è½®æ•° | LR warmup epochs\n",
    "ACTION_GRAD_CLIP = 1.0             # æ¢¯åº¦è£å‰ªå€¼ | Gradient clipping value\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# å­¦ä¹ ç‡è°ƒåº¦å™¨ | Learning Rate Scheduler\n",
    "# -----------------------------------------------------------------------------\n",
    "LR_SCHEDULER_FACTOR = 0.5          # å­¦ä¹ ç‡è¡°å‡å› å­ | LR decay factor\n",
    "LR_SCHEDULER_PATIENCE = 7          # ç­‰å¾…è½®æ•° | Patience epochs\n",
    "LR_SCHEDULER_MIN_LR = 1e-7         # æœ€å°å­¦ä¹ ç‡ | Minimum LR\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SVMé…ç½® | SVM Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "SVM_KERNEL = 'rbf'                 # SVMæ ¸å‡½æ•° | SVM kernel\n",
    "SVM_C = 10.0                       # æ­£åˆ™åŒ–å‚æ•° | Regularization parameter\n",
    "SVM_GAMMA = 'scale'                # Gammaå‚æ•° | Gamma parameter\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# æ•°æ®é…ç½® | Data Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "DATA_NORMALIZE = True              # æ•°æ®å½’ä¸€åŒ– | Data normalization\n",
    "DATA_TEST_SIZE = 0.2               # éªŒè¯é›†æ¯”ä¾‹ | Validation split ratio\n",
    "DATA_AUGMENTATION = True           # æ•°æ®å¢å¼º | Data augmentation\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# æ£€æŸ¥ç‚¹é…ç½® | Checkpoint Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "CHECKPOINT_INTERVAL = 10           # æ£€æŸ¥ç‚¹ä¿å­˜é—´éš” | Checkpoint save interval\n",
    "\n",
    "print('='*80)\n",
    "print('è¶…å‚æ•°é…ç½® | HYPERPARAMETER CONFIGURATION')\n",
    "print('='*80)\n",
    "print(f'\\nğŸ“ æ¨¡å‹æ¶æ„ | Model Architecture:')\n",
    "print(f'   è¾“å…¥é€šé“ Input channels: {MODEL_IN_CHANNELS}')\n",
    "print(f'   åŸºç¡€é€šé“ Base channels: {MODEL_BASE_CHANNELS}')\n",
    "print(f'   ç½‘ç»œå±‚æ•° Network layers: {MODEL_NUM_LAYERS}')\n",
    "print(f'   Dropoutç‡ Dropout rate: {MODEL_DROPOUT_RATE}')\n",
    "print(f'\\nğŸ¯ åŠ¨ä½œè´¨é‡è®­ç»ƒ | Action Quality Training:')\n",
    "print(f'   è®­ç»ƒè½®æ•° Epochs: {ACTION_EPOCHS}')\n",
    "print(f'   æ‰¹æ¬¡å¤§å° Batch size: {ACTION_BATCH_SIZE}')\n",
    "print(f'   å­¦ä¹ ç‡ Learning rate: {ACTION_LEARNING_RATE}')\n",
    "print(f'   æƒé‡è¡°å‡ Weight decay: {ACTION_WEIGHT_DECAY}')\n",
    "print(f'   é¢„çƒ­è½®æ•° Warmup epochs: {ACTION_WARMUP_EPOCHS}')\n",
    "print(f'   æ¢¯åº¦è£å‰ª Gradient clipping: {ACTION_GRAD_CLIP}')\n",
    "print(f'\\nğŸ”§ SVMé…ç½® | SVM Configuration:')\n",
    "print(f'   æ ¸å‡½æ•° Kernel: {SVM_KERNEL}')\n",
    "print(f'   Cå‚æ•° C: {SVM_C}')\n",
    "print(f'   Gamma: {SVM_GAMMA}')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ¨¡å‹æ¶æ„ | Model Architecture\n",
    "\n",
    "### æ‰©å±•çš„CNNç¼–ç å™¨ï¼ˆ7å±‚ï¼‰| Expanded CNN Encoder (7 layers)\n",
    "\n",
    "è§£å†³è®­ç»ƒé—®é¢˜çš„å…³é”®æ”¹è¿›ï¼š\n",
    "Key improvements to solve training issues:\n",
    "\n",
    "1. **æ›´æ·±çš„ç½‘ç»œ**ï¼š7å±‚å·ç§¯ï¼Œæå–æ›´å¤æ‚çš„ç‰¹å¾\n",
    "   **Deeper network**: 7 conv layers for more complex features\n",
    "   \n",
    "2. **æ‰¹å½’ä¸€åŒ–**ï¼šæ¯å±‚åæ·»åŠ BNï¼ŒåŠ é€Ÿæ”¶æ•›\n",
    "   **Batch Normalization**: BN after each layer, faster convergence\n",
    "   \n",
    "3. **Kaimingåˆå§‹åŒ–**ï¼šé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸\n",
    "   **Kaiming Init**: Prevent vanishing/exploding gradients\n",
    "   \n",
    "4. **æ®‹å·®è¿æ¥**ï¼šæ”¹å–„æ¢¯åº¦æµåŠ¨\n",
    "   **Residual connections**: Better gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„å·ç§¯å—ï¼ŒåŒ…å«BatchNormå’Œæ®‹å·®è¿æ¥\n",
    "    Improved conv block with BatchNorm and residual connection\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, \n",
    "                 kernel_size: int = 3, stride: int = 2, padding: int = 1,\n",
    "                 use_residual: bool = False):\n",
    "        super(ImprovedConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
    "                             kernel_size=kernel_size, \n",
    "                             stride=stride, \n",
    "                             padding=padding,\n",
    "                             bias=False)  # BNåä¸éœ€è¦bias\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.use_residual = use_residual and (in_channels == out_channels) and (stride == 1)\n",
    "        \n",
    "        # Kaimingåˆå§‹åŒ– | Kaiming initialization\n",
    "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        if self.use_residual:\n",
    "            out = out + identity\n",
    "            \n",
    "        return out\n",
    "\n",
    "\n",
    "class ExpandedCNNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    æ‰©å±•çš„7å±‚CNNç¼–ç å™¨ï¼Œç”¨äºç‰¹å¾æå–\n",
    "    Expanded 7-layer CNN encoder for feature extraction\n",
    "    \n",
    "    æ¶æ„ | Architecture:\n",
    "    - 7ä¸ªå·ç§¯å—ï¼Œé€šé“æ•°é€’å¢ï¼š64 â†’ 128 â†’ 256 â†’ 512 â†’ 1024 â†’ 2048 â†’ 2048\n",
    "    - æ¯å—åŒ…å«ï¼šConv2d + BatchNorm + LeakyReLU\n",
    "    - å…¨å±€å¹³å‡æ± åŒ–è¾“å‡º2048ç»´ç‰¹å¾å‘é‡\n",
    "    \n",
    "    Input: (B, 1, 256, 256)\n",
    "    Output: (B, 2048)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int = 1, base_channels: int = 64):\n",
    "        super(ExpandedCNNEncoder, self).__init__()\n",
    "        \n",
    "        # å®šä¹‰é€šé“æ•°åºåˆ— | Define channel progression\n",
    "        channels = [base_channels * (2**i) for i in range(6)]  # [64, 128, 256, 512, 1024, 2048]\n",
    "        channels.append(2048)  # ä¿æŒæœ€åä¸¤å±‚é€šé“æ•°ç›¸åŒ | Keep last two layers same\n",
    "        \n",
    "        # æ„å»ºç¼–ç å™¨å±‚ | Build encoder layers\n",
    "        layers = []\n",
    "        current_channels = in_channels\n",
    "        \n",
    "        for i, out_channels in enumerate(channels):\n",
    "            # å‰5å±‚ä½¿ç”¨stride=2ä¸‹é‡‡æ ·ï¼Œå2å±‚ä½¿ç”¨stride=1\n",
    "            stride = 2 if i < 5 else 1\n",
    "            use_residual = (i >= 5)  # æœ€åä¸¤å±‚ä½¿ç”¨æ®‹å·®è¿æ¥\n",
    "            \n",
    "            layers.append(ImprovedConvBlock(\n",
    "                in_channels=current_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "                use_residual=use_residual\n",
    "            ))\n",
    "            current_channels = out_channels\n",
    "        \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.feature_dim = channels[-1]\n",
    "        \n",
    "        print(f'âœ… ç¼–ç å™¨å·²åˆ›å»º | Encoder created:')\n",
    "        print(f'   å±‚æ•° Layers: {len(channels)}')\n",
    "        print(f'   é€šé“åºåˆ— Channels: {in_channels} â†’ {\" â†’ \".join(map(str, channels))}')\n",
    "        print(f'   è¾“å‡ºç‰¹å¾ç»´åº¦ Output feature dim: {self.feature_dim}')\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"æå–ç‰¹å¾ | Extract features\"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "    \n",
    "    def get_feature_dim(self) -> int:\n",
    "        \"\"\"è¿”å›ç‰¹å¾ç»´åº¦ | Return feature dimension\"\"\"\n",
    "        return self.feature_dim\n",
    "\n",
    "\n",
    "class ActionQualityCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    åŠ¨ä½œè´¨é‡åˆ†ç±»å™¨ï¼ˆæ·±åº¦å­¦ä¹ ï¼‰\n",
    "    Action Quality Classifier (Deep Learning)\n",
    "    \n",
    "    3ä¸ªç±»åˆ« | 3 classes:\n",
    "    - 0: Full (å…¨ç¨‹)\n",
    "    - 1: Half (åŠç¨‹)\n",
    "    - 2: Invalid (æ— æ•ˆ)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder: ExpandedCNNEncoder, n_classes: int = 3, dropout_rate: float = 0.5):\n",
    "        super(ActionQualityCNN, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        feature_dim = encoder.get_feature_dim()\n",
    "        \n",
    "        # 3å±‚åˆ†ç±»å¤´ï¼Œé€æ­¥é™ç»´ | 3-layer classification head with gradual dimension reduction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(512, n_classes)\n",
    "        )\n",
    "        \n",
    "        # åˆå§‹åŒ–åˆ†ç±»å¤´ | Initialize classifier head\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        print(f'âœ… åŠ¨ä½œè´¨é‡åˆ†ç±»å™¨å·²åˆ›å»º | Action Quality Classifier created:')\n",
    "        print(f'   è¾“å…¥ç‰¹å¾ç»´åº¦ Input feature dim: {feature_dim}')\n",
    "        print(f'   åˆ†ç±»å¤´ç»“æ„ Classifier: {feature_dim} â†’ 1024 â†’ 512 â†’ {n_classes}')\n",
    "        print(f'   Dropoutç‡ Dropout rate: {dropout_rate}')\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"å‰å‘ä¼ æ’­ | Forward pass\"\"\"\n",
    "        features = self.encoder(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "    \n",
    "    def extract_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"ä»…æå–ç‰¹å¾ï¼Œç”¨äºSVM | Extract features only, for SVM\"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "print('\\nâœ… æ¨¡å‹ç±»å®šä¹‰å®Œæˆ | Model classes defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ•°æ®åŠ è½½ | Data Loading\n",
    "\n",
    "ä»Kaggleæ•°æ®é›†æˆ–æœ¬åœ°ç›®å½•åŠ è½½çœŸå®çš„HHTçŸ©é˜µã€‚\n",
    "Load real HHT matrices from Kaggle dataset or local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename: str) -> Optional[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    è§£ææ–‡ä»¶åæå–æ ‡ç­¾\n",
    "    Parse filename to extract labels\n",
    "    \n",
    "    æ–‡ä»¶å‘½åæ ¼å¼ | File naming format:\n",
    "    - MUSCLENAME_movement_GENDER_###.npz\n",
    "    - ä¾‹å¦‚ Example: BICEPS_fatiguetest_M_006.npz\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'gender' and 'movement' keys, or None if test file\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    # è·³è¿‡æµ‹è¯•æ–‡ä»¶ | Skip test files\n",
    "    if basename.lower().startswith('test'):\n",
    "        return None\n",
    "    \n",
    "    # æå–æ€§åˆ« | Extract gender\n",
    "    gender_match = re.search(r'[_-]([MF])[_-]', basename)\n",
    "    if not gender_match:\n",
    "        return None\n",
    "    gender = gender_match.group(1)\n",
    "    \n",
    "    # æå–åŠ¨ä½œè´¨é‡ | Extract movement quality\n",
    "    basename_lower = basename.lower()\n",
    "    if 'fatiguetest' in basename_lower or 'full' in basename_lower:\n",
    "        movement = 'full'\n",
    "    elif 'half' in basename_lower:\n",
    "        movement = 'half'\n",
    "    elif 'invalid' in basename_lower or 'wrong' in basename_lower:\n",
    "        movement = 'invalid'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return {'gender': gender, 'movement': movement}\n",
    "\n",
    "\n",
    "def load_real_data(data_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    ä»ç›®å½•åŠ è½½çœŸå®æ•°æ®\n",
    "    Load real data from directory\n",
    "    \n",
    "    Returns:\n",
    "        X: HHT matrices (N, 256, 256)\n",
    "        y_movement: Movement labels (N,) - 0=full, 1=half, 2=invalid\n",
    "        y_gender: Gender labels (N,) - 0=M, 1=F\n",
    "        filenames: List of filenames\n",
    "    \"\"\"\n",
    "    print(f'\\nğŸ“‚ ä»ç›®å½•åŠ è½½æ•°æ® | Loading data from: {data_dir}')\n",
    "    \n",
    "    npz_files = glob.glob(os.path.join(data_dir, '*.npz'))\n",
    "    print(f'   æ‰¾åˆ° Found {len(npz_files)} .npz files')\n",
    "    \n",
    "    X_list = []\n",
    "    y_movement_list = []\n",
    "    y_gender_list = []\n",
    "    filenames = []\n",
    "    \n",
    "    # æ ‡ç­¾ç¼–ç å™¨ | Label encoders\n",
    "    movement_encoder = LabelEncoder()\n",
    "    movement_encoder.fit(['full', 'half', 'invalid'])\n",
    "    \n",
    "    gender_encoder = LabelEncoder()\n",
    "    gender_encoder.fit(['M', 'F'])\n",
    "    \n",
    "    # åŠ è½½æ•°æ® | Load data\n",
    "    for npz_file in tqdm(npz_files, desc='Loading files'):\n",
    "        labels = parse_filename(npz_file)\n",
    "        \n",
    "        if labels is None:  # æµ‹è¯•æ–‡ä»¶ | Test file\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            data = np.load(npz_file)\n",
    "            if 'hht' in data:\n",
    "                hht_matrix = data['hht']\n",
    "            else:\n",
    "                hht_matrix = data[list(data.keys())[0]]\n",
    "            \n",
    "            # éªŒè¯å½¢çŠ¶ | Verify shape\n",
    "            if hht_matrix.shape != (256, 256):\n",
    "                print(f'   âš ï¸  è·³è¿‡ Skipping {os.path.basename(npz_file)}: å½¢çŠ¶ä¸åŒ¹é… shape mismatch {hht_matrix.shape}')\n",
    "                continue\n",
    "            \n",
    "            # å½’ä¸€åŒ–åˆ°[0,1] | Normalize to [0,1]\n",
    "            if DATA_NORMALIZE:\n",
    "                hht_min = hht_matrix.min()\n",
    "                hht_max = hht_matrix.max()\n",
    "                if hht_max > hht_min:\n",
    "                    hht_matrix = (hht_matrix - hht_min) / (hht_max - hht_min)\n",
    "            \n",
    "            # ç¼–ç æ ‡ç­¾ | Encode labels\n",
    "            movement_label = movement_encoder.transform([labels['movement']])[0]\n",
    "            gender_label = gender_encoder.transform([labels['gender']])[0]\n",
    "            \n",
    "            X_list.append(hht_matrix)\n",
    "            y_movement_list.append(movement_label)\n",
    "            y_gender_list.append(gender_label)\n",
    "            filenames.append(npz_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'   âŒ é”™è¯¯ Error loading {os.path.basename(npz_file)}: {e}')\n",
    "            continue\n",
    "    \n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y_movement = np.array(y_movement_list, dtype=np.int64)\n",
    "    y_gender = np.array(y_gender_list, dtype=np.int64)\n",
    "    \n",
    "    print(f'\\nâœ… æ•°æ®åŠ è½½å®Œæˆ | Data loading complete:')\n",
    "    print(f'   æ ·æœ¬æ•° Samples: {len(X)}')\n",
    "    print(f'   å½¢çŠ¶ Shape: {X.shape}')\n",
    "    print(f'   æ•°æ®èŒƒå›´ Data range: [{X.min():.4f}, {X.max():.4f}]')\n",
    "    \n",
    "    print(f'\\nğŸ“Š åŠ¨ä½œè´¨é‡åˆ†å¸ƒ | Movement quality distribution:')\n",
    "    for i, movement in enumerate(['full', 'half', 'invalid']):\n",
    "        count = np.sum(y_movement == i)\n",
    "        print(f'   {movement}: {count} samples ({count/len(y_movement)*100:.1f}%)')\n",
    "    \n",
    "    print(f'\\nğŸ‘¥ æ€§åˆ«åˆ†å¸ƒ | Gender distribution:')\n",
    "    for i, gender in enumerate(['M', 'F']):\n",
    "        count = np.sum(y_gender == i)\n",
    "        print(f'   {gender}: {count} samples ({count/len(y_gender)*100:.1f}%)')\n",
    "    \n",
    "    return X, y_movement, y_gender, filenames\n",
    "\n",
    "\n",
    "# åŠ è½½æ•°æ® | Load data\n",
    "if os.path.exists(DATA_DIR):\n",
    "    X, y_movement, y_gender, filenames = load_real_data(DATA_DIR)\n",
    "    \n",
    "    # åˆ†å‰²æ•°æ® | Split data\n",
    "    X_train, X_val, y_movement_train, y_movement_val, y_gender_train, y_gender_val = train_test_split(\n",
    "        X, y_movement, y_gender, \n",
    "        test_size=DATA_TEST_SIZE, \n",
    "        random_state=SEED, \n",
    "        stratify=y_movement  # æŒ‰åŠ¨ä½œè´¨é‡åˆ†å±‚ | Stratify by movement quality\n",
    "    )\n",
    "    \n",
    "    print(f'\\nâœ‚ï¸  æ•°æ®åˆ†å‰² | Data split:')\n",
    "    print(f'   è®­ç»ƒé›† Training: {len(X_train)} samples')\n",
    "    print(f'   éªŒè¯é›† Validation: {len(X_val)} samples')\n",
    "    \n",
    "else:\n",
    "    print(f'\\nâŒ æ•°æ®ç›®å½•æœªæ‰¾åˆ° | Data directory not found: {DATA_DIR}')\n",
    "    print('   è¯·ç¡®ä¿æ•°æ®é›†å·²æ·»åŠ åˆ°æ­¤ç¬”è®°æœ¬ | Please ensure dataset is added to this notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è®­ç»ƒåŠ¨ä½œè´¨é‡åˆ†ç±»å™¨ï¼ˆæ·±åº¦å­¦ä¹ CNNï¼‰| Train Action Quality Classifier (Deep Learning CNN)\n",
    "\n",
    "ä½¿ç”¨æ‰©å±•çš„7å±‚CNNæ¶æ„è®­ç»ƒåŠ¨ä½œè´¨é‡åˆ†ç±»å™¨ã€‚\n",
    "Train action quality classifier using expanded 7-layer CNN architecture.\n",
    "\n",
    "### è®­ç»ƒæ”¹è¿› | Training Improvements:\n",
    "\n",
    "1. **å­¦ä¹ ç‡é¢„çƒ­** - å‰5è½®é€æ­¥å¢åŠ å­¦ä¹ ç‡\n",
    "   **LR Warmup** - Gradually increase LR for first 5 epochs\n",
    "   \n",
    "2. **æ¢¯åº¦è£å‰ª** - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "   **Gradient Clipping** - Prevent gradient explosion\n",
    "   \n",
    "3. **æ ‡ç­¾å¹³æ»‘** - æé«˜æ³›åŒ–èƒ½åŠ›\n",
    "   **Label Smoothing** - Improve generalization\n",
    "   \n",
    "4. **ä½™å¼¦é€€ç«è°ƒåº¦** - æ›´å¥½çš„å­¦ä¹ ç‡è¡°å‡\n",
    "   **Cosine Annealing** - Better LR decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤±\n",
    "    Label smoothing cross entropy loss\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(-1)\n",
    "        log_preds = F.log_softmax(pred, dim=-1)\n",
    "        \n",
    "        # å¹³æ»‘ç›®æ ‡ | Smooth targets\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_preds)\n",
    "            true_dist.fill_(self.smoothing / (n_classes - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        \n",
    "        return torch.mean(torch.sum(-true_dist * log_preds, dim=-1))\n",
    "\n",
    "\n",
    "def get_lr_schedule(optimizer, warmup_epochs, total_epochs, base_lr):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆé¢„çƒ­ + ä½™å¼¦é€€ç«ï¼‰\n",
    "    Create LR scheduler (warmup + cosine annealing)\n",
    "    \"\"\"\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            # çº¿æ€§é¢„çƒ­ | Linear warmup\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            # ä½™å¼¦é€€ç« | Cosine annealing\n",
    "            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "            return 0.5 * (1.0 + np.cos(np.pi * progress))\n",
    "    \n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "def train_action_quality_model(\n",
    "    model, \n",
    "    X_train, y_train, \n",
    "    X_val, y_val,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.0001,\n",
    "    warmup_epochs=5,\n",
    "    grad_clip=1.0,\n",
    "    device='cuda'\n",
    "):\n",
    "    \"\"\"\n",
    "    è®­ç»ƒåŠ¨ä½œè´¨é‡åˆ†ç±»æ¨¡å‹\n",
    "    Train action quality classification model\n",
    "    \"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print('å¼€å§‹è®­ç»ƒåŠ¨ä½œè´¨é‡åˆ†ç±»å™¨ | Starting Action Quality Classifier Training')\n",
    "    print('='*80)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # å‡†å¤‡æ•°æ® | Prepare data\n",
    "    if X_train.ndim == 3:\n",
    "        X_train = X_train[:, np.newaxis, :, :]  # Add channel dim\n",
    "        X_val = X_val[:, np.newaxis, :, :]\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.long)\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, \n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ | Loss and optimizer\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate, \n",
    "        weight_decay=ACTION_WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # å­¦ä¹ ç‡è°ƒåº¦å™¨ | LR scheduler\n",
    "    scheduler = get_lr_schedule(optimizer, warmup_epochs, epochs, learning_rate)\n",
    "    \n",
    "    # ReduceLROnPlateauä½œä¸ºå¤‡ç”¨ | ReduceLROnPlateau as backup\n",
    "    plateau_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=LR_SCHEDULER_FACTOR, \n",
    "        patience=LR_SCHEDULER_PATIENCE, min_lr=LR_SCHEDULER_MIN_LR\n",
    "    )\n",
    "    \n",
    "    # è®­ç»ƒå†å² | Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_path = os.path.join(CHECKPOINT_DIR, 'best_action_quality_model.pt')\n",
    "    \n",
    "    print(f'\\nğŸš€ è®­ç»ƒé…ç½® | Training configuration:')\n",
    "    print(f'   è®¾å¤‡ Device: {device}')\n",
    "    print(f'   è®­ç»ƒæ ·æœ¬ Training samples: {len(X_train)}')\n",
    "    print(f'   éªŒè¯æ ·æœ¬ Validation samples: {len(X_val)}')\n",
    "    print(f'   æ‰¹æ¬¡å¤§å° Batch size: {batch_size}')\n",
    "    print(f'   å­¦ä¹ ç‡ Learning rate: {learning_rate}')\n",
    "    print(f'   é¢„çƒ­è½®æ•° Warmup epochs: {warmup_epochs}')\n",
    "    print(f'   æ¢¯åº¦è£å‰ª Gradient clipping: {grad_clip}')\n",
    "    print(f'   æƒé‡è¡°å‡ Weight decay: {ACTION_WEIGHT_DECAY}')\n",
    "    print()\n",
    "    \n",
    "    # è®­ç»ƒå¾ªç¯ | Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # è®­ç»ƒé˜¶æ®µ | Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for batch_X, batch_y in pbar:\n",
    "            batch_X = batch_X.to(device, non_blocking=True)\n",
    "            batch_y = batch_y.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # æ¢¯åº¦è£å‰ª | Gradient clipping\n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * batch_X.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += batch_y.size(0)\n",
    "            train_correct += predicted.eq(batch_y).sum().item()\n",
    "            \n",
    "            # æ›´æ–°è¿›åº¦æ¡ | Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*train_correct/train_total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # éªŒè¯é˜¶æ®µ | Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device, non_blocking=True)\n",
    "                batch_y = batch_y.to(device, non_blocking=True)\n",
    "                \n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                val_loss += loss.item() * batch_X.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += batch_y.size(0)\n",
    "                val_correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # æ›´æ–°å­¦ä¹ ç‡ | Update learning rate\n",
    "        scheduler.step()\n",
    "        plateau_scheduler.step(val_acc)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # ä¿å­˜å†å² | Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # æ‰“å°è¿›åº¦ | Print progress\n",
    "        print(f'Epoch [{epoch+1:3d}/{epochs}] | '\n",
    "              f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | '\n",
    "              f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | '\n",
    "              f'LR: {current_lr:.6f}')\n",
    "        \n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹ | Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'history': history\n",
    "            }, best_model_path)\n",
    "            print(f'  â­ æ–°æœ€ä½³æ¨¡å‹ï¼| New best model! Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ | Save checkpoint periodically\n",
    "        if (epoch + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, f'action_quality_epoch_{epoch+1}.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'history': history\n",
    "            }, checkpoint_path)\n",
    "            print(f'  ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜ | Checkpoint saved: {checkpoint_path}')\n",
    "    \n",
    "    print('\\n' + '='*80)\n",
    "    print(f'âœ… è®­ç»ƒå®Œæˆï¼| Training complete!')\n",
    "    print(f'   æœ€ä½³éªŒè¯å‡†ç¡®ç‡ Best validation accuracy: {best_val_acc:.4f}')\n",
    "    print(f'   æœ€ä½³æ¨¡å‹å·²ä¿å­˜ Best model saved to: {best_model_path}')\n",
    "    print('='*80)\n",
    "    \n",
    "    return history, best_model_path\n",
    "\n",
    "\n",
    "# åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹ | Create and train model\n",
    "if 'X_train' in locals():\n",
    "    # åˆ›å»ºç¼–ç å™¨ | Create encoder\n",
    "    encoder = ExpandedCNNEncoder(\n",
    "        in_channels=MODEL_IN_CHANNELS,\n",
    "        base_channels=MODEL_BASE_CHANNELS\n",
    "    )\n",
    "    \n",
    "    # åˆ›å»ºåŠ¨ä½œè´¨é‡åˆ†ç±»å™¨ | Create action quality classifier\n",
    "    action_model = ActionQualityCNN(\n",
    "        encoder=encoder,\n",
    "        n_classes=3,  # full, half, invalid\n",
    "        dropout_rate=MODEL_DROPOUT_RATE\n",
    "    )\n",
    "    \n",
    "    # è®­ç»ƒ | Train\n",
    "    action_history, action_best_path = train_action_quality_model(\n",
    "        model=action_model,\n",
    "        X_train=X_train,\n",
    "        y_train=y_movement_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_movement_val,\n",
    "        epochs=ACTION_EPOCHS,\n",
    "        batch_size=ACTION_BATCH_SIZE,\n",
    "        learning_rate=ACTION_LEARNING_RATE,\n",
    "        warmup_epochs=ACTION_WARMUP_EPOCHS,\n",
    "        grad_clip=ACTION_GRAD_CLIP,\n",
    "        device=device\n",
    "    )\n",
    "else:\n",
    "    print('\\nâš ï¸  è¯·å…ˆåŠ è½½æ•°æ® | Please load data first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ | Visualize Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"ç»˜åˆ¶è®­ç»ƒå†å² | Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(epochs, history['train_loss'], 'b-', label='è®­ç»ƒæŸå¤± Train Loss', linewidth=2)\n",
    "    axes[0].plot(epochs, history['val_loss'], 'r-', label='éªŒè¯æŸå¤± Val Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('è½®æ¬¡ Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('æŸå¤± Loss', fontsize=12)\n",
    "    axes[0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤± | Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(epochs, history['train_acc'], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡ Train Acc', linewidth=2)\n",
    "    axes[1].plot(epochs, history['val_acc'], 'r-', label='éªŒè¯å‡†ç¡®ç‡ Val Acc', linewidth=2)\n",
    "    axes[1].set_xlabel('è½®æ¬¡ Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('å‡†ç¡®ç‡ Accuracy', fontsize=12)\n",
    "    axes[1].set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡ | Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning Rate\n",
    "    axes[2].plot(epochs, history['lr'], 'g-', linewidth=2)\n",
    "    axes[2].set_xlabel('è½®æ¬¡ Epoch', fontsize=12)\n",
    "    axes[2].set_ylabel('å­¦ä¹ ç‡ Learning Rate', fontsize=12)\n",
    "    axes[2].set_title('å­¦ä¹ ç‡è°ƒåº¦ | Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_yscale('log')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°ç»Ÿè®¡ | Print statistics\n",
    "    print(f'\\nğŸ“Š è®­ç»ƒç»Ÿè®¡ | Training Statistics:')\n",
    "    print(f'   æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡ Final train acc: {history[\"train_acc\"][-1]:.4f}')\n",
    "    print(f'   æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡ Final val acc: {history[\"val_acc\"][-1]:.4f}')\n",
    "    print(f'   æœ€ä½³éªŒè¯å‡†ç¡®ç‡ Best val acc: {max(history[\"val_acc\"]):.4f}')\n",
    "    print(f'   æœ€ç»ˆè®­ç»ƒæŸå¤± Final train loss: {history[\"train_loss\"][-1]:.4f}')\n",
    "    print(f'   æœ€ç»ˆéªŒè¯æŸå¤± Final val loss: {history[\"val_loss\"][-1]:.4f}')\n",
    "\n",
    "\n",
    "if 'action_history' in locals():\n",
    "    plot_training_history(action_history)\n",
    "else:\n",
    "    print('\\nâš ï¸  è¯·å…ˆè®­ç»ƒæ¨¡å‹ | Please train the model first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. è®­ç»ƒæ€§åˆ«åˆ†ç±»å™¨ï¼ˆSVMï¼‰| Train Gender Classifier (SVM)\n",
    "\n",
    "ä½¿ç”¨è®­ç»ƒå¥½çš„CNNæå–ç‰¹å¾ï¼Œç„¶åè®­ç»ƒSVMè¿›è¡Œæ€§åˆ«åˆ†ç±»ã€‚\n",
    "Use trained CNN to extract features, then train SVM for gender classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderSVMClassifier:\n",
    "    \"\"\"\n",
    "    æ€§åˆ«SVMåˆ†ç±»å™¨\n",
    "    Gender SVM Classifier\n",
    "    \n",
    "    ä½¿ç”¨CNNç‰¹å¾ + SVMåˆ†ç±»å™¨\n",
    "    Uses CNN features + SVM classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_extractor, svm_kernel='rbf', svm_C=10.0, svm_gamma='scale', device='cuda'):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.device = device\n",
    "        self.scaler = StandardScaler()\n",
    "        self.svm = SVC(\n",
    "            kernel=svm_kernel,\n",
    "            C=svm_C,\n",
    "            gamma=svm_gamma,\n",
    "            probability=True,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        print(f'âœ… æ€§åˆ«åˆ†ç±»å™¨å·²åˆ›å»º | Gender classifier created:')\n",
    "        print(f'   ç‰¹å¾æå–å™¨ Feature extractor: CNN')\n",
    "        print(f'   SVMæ ¸å‡½æ•° SVM kernel: {svm_kernel}')\n",
    "        print(f'   Cå‚æ•° C: {svm_C}')\n",
    "        print(f'   Gamma: {svm_gamma}')\n",
    "    \n",
    "    def extract_features(self, X, batch_size=32):\n",
    "        \"\"\"æå–ç‰¹å¾ | Extract features\"\"\"\n",
    "        self.feature_extractor.eval()\n",
    "        \n",
    "        if X.ndim == 3:\n",
    "            X = X[:, np.newaxis, :, :]\n",
    "        \n",
    "        features_list = []\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(X), batch_size), desc='Extracting features'):\n",
    "                batch = torch.tensor(X[i:i+batch_size], dtype=torch.float32).to(self.device)\n",
    "                batch_features = self.feature_extractor.extract_features(batch)\n",
    "                features_list.append(batch_features.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(features_list)\n",
    "    \n",
    "    def fit(self, X, y, batch_size=32):\n",
    "        \"\"\"è®­ç»ƒSVM | Train SVM\"\"\"\n",
    "        print(f'\\nğŸ”§ å¼€å§‹è®­ç»ƒæ€§åˆ«SVMåˆ†ç±»å™¨ | Starting Gender SVM Classifier Training')\n",
    "        \n",
    "        # æå–ç‰¹å¾ | Extract features\n",
    "        features = self.extract_features(X, batch_size)\n",
    "        \n",
    "        # å½’ä¸€åŒ– | Normalize\n",
    "        print('   å½’ä¸€åŒ–ç‰¹å¾ | Normalizing features...')\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # è®­ç»ƒSVM | Train SVM\n",
    "        print('   è®­ç»ƒSVM | Training SVM...')\n",
    "        self.svm.fit(features_scaled, y)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        print('   âœ… è®­ç»ƒå®Œæˆ | Training complete!')\n",
    "    \n",
    "    def predict(self, X, batch_size=32):\n",
    "        \"\"\"é¢„æµ‹ | Predict\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError('å¿…é¡»å…ˆè®­ç»ƒæ¨¡å‹ | Must fit model first')\n",
    "        \n",
    "        features = self.extract_features(X, batch_size)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        return self.svm.predict(features_scaled)\n",
    "    \n",
    "    def predict_proba(self, X, batch_size=32):\n",
    "        \"\"\"é¢„æµ‹æ¦‚ç‡ | Predict probabilities\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError('å¿…é¡»å…ˆè®­ç»ƒæ¨¡å‹ | Must fit model first')\n",
    "        \n",
    "        features = self.extract_features(X, batch_size)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        return self.svm.predict_proba(features_scaled)\n",
    "    \n",
    "    def evaluate(self, X, y, batch_size=32):\n",
    "        \"\"\"è¯„ä¼°æ¨¡å‹ | Evaluate model\"\"\"\n",
    "        y_pred = self.predict(X, batch_size)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': y_pred,\n",
    "            'classification_report': classification_report(y, y_pred, target_names=['M', 'F']),\n",
    "            'confusion_matrix': confusion_matrix(y, y_pred)\n",
    "        }\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"ä¿å­˜æ¨¡å‹ | Save model\"\"\"\n",
    "        with open(f'{path}_scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        with open(f'{path}_svm.pkl', 'wb') as f:\n",
    "            pickle.dump(self.svm, f)\n",
    "        print(f'ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ | Model saved to {path}_*.pkl')\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path, feature_extractor, device='cuda'):\n",
    "        \"\"\"åŠ è½½æ¨¡å‹ | Load model\"\"\"\n",
    "        classifier = cls(feature_extractor, device=device)\n",
    "        with open(f'{path}_scaler.pkl', 'rb') as f:\n",
    "            classifier.scaler = pickle.load(f)\n",
    "        with open(f'{path}_svm.pkl', 'rb') as f:\n",
    "            classifier.svm = pickle.load(f)\n",
    "        classifier.is_fitted = True\n",
    "        print(f'ğŸ“‚ æ¨¡å‹å·²åŠ è½½ | Model loaded from {path}_*.pkl')\n",
    "        return classifier\n",
    "\n",
    "\n",
    "# è®­ç»ƒæ€§åˆ«åˆ†ç±»å™¨ | Train gender classifier\n",
    "if 'action_model' in locals() and 'X_train' in locals():\n",
    "    # åŠ è½½æœ€ä½³åŠ¨ä½œè´¨é‡æ¨¡å‹ç”¨äºç‰¹å¾æå– | Load best action quality model for feature extraction\n",
    "    checkpoint = torch.load(action_best_path, map_location=device)\n",
    "    action_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    action_model.eval()\n",
    "    \n",
    "    # åˆ›å»ºæ€§åˆ«åˆ†ç±»å™¨ | Create gender classifier\n",
    "    gender_classifier = GenderSVMClassifier(\n",
    "        feature_extractor=action_model,\n",
    "        svm_kernel=SVM_KERNEL,\n",
    "        svm_C=SVM_C,\n",
    "        svm_gamma=SVM_GAMMA,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # è®­ç»ƒ | Train\n",
    "    gender_classifier.fit(X_train, y_gender_train, batch_size=32)\n",
    "    \n",
    "    # è¯„ä¼°è®­ç»ƒé›† | Evaluate on training set\n",
    "    print(f'\\nğŸ“Š è®­ç»ƒé›†è¯„ä¼° | Training Set Evaluation:')\n",
    "    train_results = gender_classifier.evaluate(X_train, y_gender_train, batch_size=32)\n",
    "    print(f'   è®­ç»ƒå‡†ç¡®ç‡ Training Accuracy: {train_results[\"accuracy\"]:.4f}')\n",
    "    \n",
    "    # è¯„ä¼°éªŒè¯é›† | Evaluate on validation set\n",
    "    print(f'\\nğŸ“Š éªŒè¯é›†è¯„ä¼° | Validation Set Evaluation:')\n",
    "    val_results = gender_classifier.evaluate(X_val, y_gender_val, batch_size=32)\n",
    "    print(f'   éªŒè¯å‡†ç¡®ç‡ Validation Accuracy: {val_results[\"accuracy\"]:.4f}')\n",
    "    print(f'\\nåˆ†ç±»æŠ¥å‘Š | Classification Report:')\n",
    "    print(val_results['classification_report'])\n",
    "    \n",
    "    # ä¿å­˜æ¨¡å‹ | Save model\n",
    "    gender_model_path = os.path.join(CHECKPOINT_DIR, 'gender_svm_model')\n",
    "    gender_classifier.save(gender_model_path)\n",
    "    \n",
    "else:\n",
    "    print('\\nâš ï¸  è¯·å…ˆè®­ç»ƒåŠ¨ä½œè´¨é‡æ¨¡å‹ | Please train action quality model first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ç»¼åˆè¯„ä¼° | Comprehensive Evaluation\n",
    "\n",
    "è¯„ä¼°ä¸¤ä¸ªåˆ†ç±»å™¨çš„æ€§èƒ½ã€‚\n",
    "Evaluate performance of both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»¼åˆè¯„ä¼° | Comprehensive evaluation\n",
    "if 'action_model' in locals() and 'gender_classifier' in locals():\n",
    "    print('='*80)\n",
    "    print('ç»¼åˆè¯„ä¼°æŠ¥å‘Š | COMPREHENSIVE EVALUATION REPORT')\n",
    "    print('='*80)\n",
    "    \n",
    "    # åŠ¨ä½œè´¨é‡è¯„ä¼° | Action quality evaluation\n",
    "    print(f'\\nğŸ¯ åŠ¨ä½œè´¨é‡åˆ†ç±»å™¨ï¼ˆæ·±åº¦å­¦ä¹ CNNï¼‰| Action Quality Classifier (Deep Learning CNN)')\n",
    "    print('-'*80)\n",
    "    \n",
    "    action_model.eval()\n",
    "    X_val_tensor = torch.tensor(X_val[:, np.newaxis, :, :], dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_movement_val, dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = action_model(X_val_tensor)\n",
    "        _, predictions = outputs.max(1)\n",
    "        action_acc = (predictions == y_val_tensor).float().mean().item()\n",
    "    \n",
    "    y_pred_action = predictions.cpu().numpy()\n",
    "    \n",
    "    print(f'éªŒè¯é›†å‡†ç¡®ç‡ Validation Accuracy: {action_acc:.4f}')\n",
    "    print(f'\\nåˆ†ç±»æŠ¥å‘Š Classification Report:')\n",
    "    print(classification_report(y_movement_val, y_pred_action, target_names=['Full', 'Half', 'Invalid']))\n",
    "    \n",
    "    print(f'\\næ··æ·†çŸ©é˜µ Confusion Matrix:')\n",
    "    cm_action = confusion_matrix(y_movement_val, y_pred_action)\n",
    "    print(cm_action)\n",
    "    \n",
    "    # æ€§åˆ«è¯„ä¼° | Gender evaluation\n",
    "    print(f'\\nğŸ‘¥ æ€§åˆ«åˆ†ç±»å™¨ï¼ˆSVMï¼‰| Gender Classifier (SVM)')\n",
    "    print('-'*80)\n",
    "    \n",
    "    val_results_gender = gender_classifier.evaluate(X_val, y_gender_val, batch_size=32)\n",
    "    print(f'éªŒè¯é›†å‡†ç¡®ç‡ Validation Accuracy: {val_results_gender[\"accuracy\"]:.4f}')\n",
    "    print(f'\\nåˆ†ç±»æŠ¥å‘Š Classification Report:')\n",
    "    print(val_results_gender['classification_report'])\n",
    "    \n",
    "    print(f'\\næ··æ·†çŸ©é˜µ Confusion Matrix:')\n",
    "    print(val_results_gender['confusion_matrix'])\n",
    "    \n",
    "    # å¯è§†åŒ–æ··æ·†çŸ©é˜µ | Visualize confusion matrices\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # åŠ¨ä½œè´¨é‡æ··æ·†çŸ©é˜µ | Action quality confusion matrix\n",
    "    im1 = axes[0].imshow(cm_action, cmap='Blues')\n",
    "    axes[0].set_title('åŠ¨ä½œè´¨é‡æ··æ·†çŸ©é˜µ\\nAction Quality Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('é¢„æµ‹æ ‡ç­¾ Predicted', fontsize=10)\n",
    "    axes[0].set_ylabel('çœŸå®æ ‡ç­¾ True', fontsize=10)\n",
    "    axes[0].set_xticks([0, 1, 2])\n",
    "    axes[0].set_yticks([0, 1, 2])\n",
    "    axes[0].set_xticklabels(['Full', 'Half', 'Invalid'])\n",
    "    axes[0].set_yticklabels(['Full', 'Half', 'Invalid'])\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡æ³¨ | Add value annotations\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            axes[0].text(j, i, str(cm_action[i, j]), \n",
    "                        ha='center', va='center', color='white' if cm_action[i, j] > cm_action.max()/2 else 'black')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # æ€§åˆ«æ··æ·†çŸ©é˜µ | Gender confusion matrix\n",
    "    cm_gender = val_results_gender['confusion_matrix']\n",
    "    im2 = axes[1].imshow(cm_gender, cmap='Greens')\n",
    "    axes[1].set_title('æ€§åˆ«æ··æ·†çŸ©é˜µ\\nGender Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('é¢„æµ‹æ ‡ç­¾ Predicted', fontsize=10)\n",
    "    axes[1].set_ylabel('çœŸå®æ ‡ç­¾ True', fontsize=10)\n",
    "    axes[1].set_xticks([0, 1])\n",
    "    axes[1].set_yticks([0, 1])\n",
    "    axes[1].set_xticklabels(['M', 'F'])\n",
    "    axes[1].set_yticklabels(['M', 'F'])\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡æ³¨ | Add value annotations\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            axes[1].text(j, i, str(cm_gender[i, j]),\n",
    "                        ha='center', va='center', color='white' if cm_gender[i, j] > cm_gender.max()/2 else 'black')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n' + '='*80)\n",
    "    print('âœ… è¯„ä¼°å®Œæˆ | Evaluation Complete')\n",
    "    print('='*80)\n",
    "    \n",
    "else:\n",
    "    print('\\nâš ï¸  è¯·å…ˆè®­ç»ƒä¸¤ä¸ªæ¨¡å‹ | Please train both models first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æ€»ç»“å’Œä¸‹ä¸€æ­¥ | Summary and Next Steps\n",
    "\n",
    "### æ¨¡å‹ä¿å­˜ä½ç½® | Model Save Locations:\n",
    "\n",
    "- **åŠ¨ä½œè´¨é‡åˆ†ç±»å™¨** Action Quality Classifier: `{CHECKPOINT_DIR}/best_action_quality_model.pt`\n",
    "- **æ€§åˆ«SVMåˆ†ç±»å™¨** Gender SVM Classifier: `{CHECKPOINT_DIR}/gender_svm_model_*.pkl`\n",
    "\n",
    "### å…³é”®æ”¹è¿›æ€»ç»“ | Key Improvements Summary:\n",
    "\n",
    "1. âœ… **æ‰©å±•çš„7å±‚CNN** - æ›´å¼ºçš„ç‰¹å¾æå–èƒ½åŠ›\n",
    "   **Expanded 7-layer CNN** - Stronger feature extraction\n",
    "\n",
    "2. âœ… **æ‰¹å½’ä¸€åŒ– + Kaimingåˆå§‹åŒ–** - è§£å†³æ¢¯åº¦é—®é¢˜\n",
    "   **BatchNorm + Kaiming Init** - Solve gradient issues\n",
    "\n",
    "3. âœ… **å­¦ä¹ ç‡é¢„çƒ­å’Œè°ƒåº¦** - æ›´ç¨³å®šçš„è®­ç»ƒ\n",
    "   **LR warmup and scheduling** - More stable training\n",
    "\n",
    "4. âœ… **æ¢¯åº¦è£å‰ª** - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "   **Gradient clipping** - Prevent gradient explosion\n",
    "\n",
    "5. âœ… **æ ‡ç­¾å¹³æ»‘** - æé«˜æ³›åŒ–èƒ½åŠ›\n",
    "   **Label smoothing** - Better generalization\n",
    "\n",
    "6. âœ… **åŒåˆ†ç±»å™¨ç³»ç»Ÿ** - ä¸“é—¨ä¼˜åŒ–æ¯ä¸ªä»»åŠ¡\n",
    "   **Dual classifier system** - Specialized optimization\n",
    "\n",
    "### ä½¿ç”¨å»ºè®® | Usage Recommendations:\n",
    "\n",
    "1. **è°ƒæ•´è¶…å‚æ•°** - æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ç­‰\n",
    "   **Tune hyperparameters** - Adjust LR, batch size based on dataset size\n",
    "\n",
    "2. **æ•°æ®å¢å¼º** - å¦‚æœè®­ç»ƒæ•°æ®è¾ƒå°‘ï¼Œå¯ä»¥æ·»åŠ æ•°æ®å¢å¼º\n",
    "   **Data augmentation** - Add if training data is limited\n",
    "\n",
    "3. **æ¨¡å‹é›†æˆ** - å¯ä»¥è®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶é›†æˆé¢„æµ‹ç»“æœ\n",
    "   **Model ensemble** - Train multiple models and ensemble predictions\n",
    "\n",
    "4. **æŒç»­ç›‘æ§** - è§‚å¯Ÿè®­ç»ƒæ›²çº¿ï¼Œç¡®ä¿lossä¸‹é™ã€accuracyæå‡\n",
    "   **Monitor training** - Watch training curves, ensure loss decreases and accuracy improves\n",
    "\n",
    "### é¢„æœŸæ•ˆæœ | Expected Results:\n",
    "\n",
    "- **æŸå¤±ä¸‹é™** Loss decreases: åº”è¯¥çœ‹åˆ°æ˜æ˜¾çš„lossä¸‹é™æ›²çº¿\n",
    "  You should see clear loss decrease curve\n",
    "  \n",
    "- **å‡†ç¡®ç‡æå‡** Accuracy improves: å‡†ç¡®ç‡åº”è¯¥ç¨³æ­¥æå‡\n",
    "  Accuracy should steadily improve\n",
    "  \n",
    "- **æ”¶æ•›ç¨³å®š** Stable convergence: è®­ç»ƒåº”è¯¥åœ¨50-100è½®å†…æ”¶æ•›\n",
    "  Training should converge within 50-100 epochs\n",
    "\n",
    "å¦‚æœä»ç„¶é‡åˆ°è®­ç»ƒé—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š\n",
    "If you still encounter training issues, check:\n",
    "\n",
    "1. æ•°æ®è´¨é‡å’Œåˆ†å¸ƒ | Data quality and distribution\n",
    "2. å­¦ä¹ ç‡æ˜¯å¦åˆé€‚ | Learning rate appropriateness  \n",
    "3. æ‰¹æ¬¡å¤§å°æ˜¯å¦åˆé€‚ | Batch size appropriateness\n",
    "4. æ˜¯å¦éœ€è¦æ›´å¤šæ•°æ® | Need for more data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}