{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sEMG-HHT CNN Classifier for Movement Quality and Fatigue Classification\n",
    "\n",
    "This notebook implements a Convolutional Neural Network (CNN) encoder with SVM classifier for classifying surface electromyography (sEMG) signals based on movement quality and fatigue levels.\n",
    "\n",
    "## Architecture Overview\n",
    "- **Input**: 256\u00d7256 matrix from HHT (Hilbert-Huang Transform) of sEMG signals\n",
    "- **Encoder**: 3-layer CNN with Conv2D + InstanceNorm + LeakyReLU\n",
    "- **Pooling**: Global Average Pooling\n",
    "- **Classifier**: SVM for multi-class classification\n",
    "\n",
    "## Requirements\n",
    "- PyTorch\n",
    "- scikit-learn\n",
    "- NumPy\n",
    "- Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Integration | Kaggle \u96c6\u6210\n\n",
    "This notebook is configured to work with the **HILBERTMATRIX_NPZ** dataset on Kaggle.\n\n",
    "\u672c\u7b14\u8bb0\u672c\u914d\u7f6e\u4e3a\u4f7f\u7528 Kaggle \u4e0a\u7684 **HILBERTMATRIX_NPZ** \u6570\u636e\u96c6\u3002\n\n",
    "Data path: `/kaggle/input/hilbertmatrix-npz/hht_matrices/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sEMG-HHT CNN Classifier - Kaggle Training\n",
    "# sEMG-HHT CNN \u5206\u7c7b\u5668 - Kaggle \u8bad\u7ec3\n",
    "\n",
    "This notebook trains a CNN-SVM classifier on sEMG Hilbert-Huang Transform data for 6-class classification:\n",
    "\u672c\u7b14\u8bb0\u672c\u8bad\u7ec3\u4e00\u4e2a CNN-SVM \u5206\u7c7b\u5668\uff0c\u7528\u4e8e sEMG \u5e0c\u5c14\u4f2f\u7279-\u9ec4\u53d8\u6362\u6570\u636e\u7684 6 \u7c7b\u5206\u7c7b\uff1a\n",
    "\n",
    "- **Gender (\u6027\u522b)**: Male (\u7537\u6027, M), Female (\u5973\u6027, F)\n",
    "- **Movement Quality (\u52a8\u4f5c\u8d28\u91cf)**: Full (\u5b8c\u6574), Half (\u534a\u7a0b), Invalid (\u65e0\u6548)\n",
    "\n",
    "## Dataset Integration | \u6570\u636e\u96c6\u96c6\u6210\n",
    "\n",
    "This notebook uses the **HILBERTMATRIX_NPZ** dataset from Kaggle.\n",
    "\u672c\u7b14\u8bb0\u672c\u4f7f\u7528 Kaggle \u4e0a\u7684 **HILBERTMATRIX_NPZ** \u6570\u636e\u96c6\u3002\n",
    "\n",
    "Data location: `/kaggle/input/hilbertmatrix-npz/hht_matrices/`\n",
    "\u6570\u636e\u4f4d\u7f6e\uff1a`/kaggle/input/hilbertmatrix-npz/hht_matrices/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data paths for Kaggle\n",
    "# \u914d\u7f6e Kaggle \u6570\u636e\u8def\u5f84\n",
    "import os\n",
    "\n",
    "# Check if running on Kaggle\n",
    "# \u68c0\u67e5\u662f\u5426\u5728 Kaggle \u4e0a\u8fd0\u884c\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    DATA_DIR = '/kaggle/input/hilbertmatrix-npz/hht_matrices'\n",
    "    CHECKPOINT_DIR = '/kaggle/working/checkpoints'\n",
    "    print(f'Running on Kaggle | \u5728 Kaggle \u4e0a\u8fd0\u884c')\n",
    "    print(f'Data directory | \u6570\u636e\u76ee\u5f55: {DATA_DIR}')\n",
    "else:\n",
    "    DATA_DIR = './data'\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    print(f'Running locally | \u672c\u5730\u8fd0\u884c')\n",
    "    print(f'Data directory | \u6570\u636e\u76ee\u5f55: {DATA_DIR}')\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f'Checkpoint directory | \u68c0\u67e5\u70b9\u76ee\u5f55: {CHECKPOINT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if running on Kaggle or fresh environment)\n",
    "# !pip install torch torchvision scikit-learn numpy matplotlib scipy PyEMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from typing import Tuple, Optional\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings that are not critical for this demo\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf9b\ufe0f Hyperparameter Configuration | \u8d85\u53c2\u6570\u914d\u7f6e\n",
    "\n",
    "**Configure all important hyperparameters here** | **\u5728\u6b64\u5904\u914d\u7f6e\u6240\u6709\u91cd\u8981\u7684\u8d85\u53c2\u6570**\n",
    "\n",
    "Modify these values to experiment with different model configurations and training settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HYPERPARAMETER CONFIGURATION | \u8d85\u53c2\u6570\u914d\u7f6e\n",
    "# ============================================================================\n",
    "# Modify these values to experiment with different settings\n",
    "# \u4fee\u6539\u8fd9\u4e9b\u503c\u4ee5\u5c1d\u8bd5\u4e0d\u540c\u7684\u8bbe\u7f6e\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Model Architecture | \u6a21\u578b\u67b6\u6784\n",
    "# ---------------------------------------------------------------------------\n",
    "MODEL_IN_CHANNELS = 1              # Input channels | \u8f93\u5165\u901a\u9053\u6570\n",
    "MODEL_BASE_CHANNELS = 64           # Base number of channels in CNN | CNN\u57fa\u7840\u901a\u9053\u6570\n",
    "MODEL_NUM_ENCODER_LAYERS = 3       # Number of encoder layers | \u7f16\u7801\u5668\u5c42\u6570\n",
    "MODEL_DROPOUT_RATE = 0.5           # Dropout rate for regularization | Dropout\u7387\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Training Configuration | \u8bad\u7ec3\u914d\u7f6e\n",
    "# ---------------------------------------------------------------------------\n",
    "TRAIN_EPOCHS = 50                  # Number of training epochs | \u8bad\u7ec3\u8f6e\u6570\n",
    "TRAIN_BATCH_SIZE = 16              # Batch size for training | \u8bad\u7ec3\u6279\u6b21\u5927\u5c0f\n",
    "TRAIN_LEARNING_RATE = 0.001        # Learning rate | \u5b66\u4e60\u7387\n",
    "TRAIN_CHECKPOINT_INTERVAL = 5      # Save checkpoint every N epochs | \u6bcfN\u8f6e\u4fdd\u5b58\u68c0\u67e5\u70b9\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Learning Rate Scheduler | \u5b66\u4e60\u7387\u8c03\u5ea6\u5668\n",
    "# ---------------------------------------------------------------------------\n",
    "LR_SCHEDULER_FACTOR = 0.5          # Factor to reduce LR | \u5b66\u4e60\u7387\u8870\u51cf\u56e0\u5b50\n",
    "LR_SCHEDULER_PATIENCE = 5          # Epochs to wait before reducing LR | \u7b49\u5f85\u8f6e\u6570\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# SVM Configuration (for CNN+SVM method) | SVM\u914d\u7f6e\uff08\u7528\u4e8eCNN+SVM\u65b9\u6cd5\uff09\n",
    "# ---------------------------------------------------------------------------\n",
    "SVM_KERNEL = 'rbf'                 # SVM kernel type | SVM\u6838\u51fd\u6570\u7c7b\u578b\n",
    "SVM_C = 10.0                       # SVM regularization parameter | SVM\u6b63\u5219\u5316\u53c2\u6570\n",
    "SVM_GAMMA = 'scale'                # SVM kernel coefficient | SVM\u6838\u7cfb\u6570\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Display Configuration | \u663e\u793a\u914d\u7f6e\n",
    "# ---------------------------------------------------------------------------\n",
    "print('='*70)\n",
    "print('HYPERPARAMETER CONFIGURATION | \u8d85\u53c2\u6570\u914d\u7f6e')\n",
    "print('='*70)\n",
    "print(f'\\n\ud83d\udcd0 Model Architecture | \u6a21\u578b\u67b6\u6784:')\n",
    "print(f'  - Input Channels: {MODEL_IN_CHANNELS}')\n",
    "print(f'  - Base Channels: {MODEL_BASE_CHANNELS}')\n",
    "print(f'  - Encoder Layers: {MODEL_NUM_ENCODER_LAYERS}')\n",
    "print(f'  - Dropout Rate: {MODEL_DROPOUT_RATE}')\n",
    "print(f'\\n\ud83c\udfaf Training Configuration | \u8bad\u7ec3\u914d\u7f6e:')\n",
    "print(f'  - Epochs: {TRAIN_EPOCHS}')\n",
    "print(f'  - Batch Size: {TRAIN_BATCH_SIZE}')\n",
    "print(f'  - Learning Rate: {TRAIN_LEARNING_RATE}')\n",
    "print(f'  - Checkpoint Interval: {TRAIN_CHECKPOINT_INTERVAL}')\n",
    "print(f'\\n\ud83d\udcc9 LR Scheduler | \u5b66\u4e60\u7387\u8c03\u5ea6\u5668:')\n",
    "print(f'  - Factor: {LR_SCHEDULER_FACTOR}')\n",
    "print(f'  - Patience: {LR_SCHEDULER_PATIENCE}')\n",
    "print(f'\\n\ud83d\udd27 SVM Configuration | SVM\u914d\u7f6e:')\n",
    "print(f'  - Kernel: {SVM_KERNEL}')\n",
    "print(f'  - C: {SVM_C}')\n",
    "print(f'  - Gamma: {SVM_GAMMA}')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CNN Encoder Architecture\n",
    "\n",
    "The encoder consists of 3 convolutional layers, each with:\n",
    "- Conv2D (kernel=3, stride=2, padding=1)\n",
    "- Instance Normalization\n",
    "- LeakyReLU activation\n",
    "\n",
    "This progressively reduces the spatial dimensions while extracting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block with Conv2D, InstanceNorm, and LeakyReLU.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, \n",
    "                 kernel_size: int = 3, stride: int = 2, padding: int = 1,\n",
    "                 leaky_slope: float = 0.2):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
    "                              kernel_size=kernel_size, \n",
    "                              stride=stride, \n",
    "                              padding=padding)\n",
    "        self.instance_norm = nn.InstanceNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_slope)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.instance_norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class sEMGHHTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN Encoder for sEMG-HHT matrix classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: 1\u00d7256\u00d7256 (single-channel HHT matrix)\n",
    "    - 3 ConvBlocks with increasing channels\n",
    "    - Global Average Pooling\n",
    "    - Output: Feature vector for SVM classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 1, \n",
    "                 base_channels: int = 64,\n",
    "                 num_layers: int = 3,\n",
    "                 leaky_slope: float = 0.2):\n",
    "        super(sEMGHHTEncoder, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Build convolutional layers\n",
    "        layers = []\n",
    "        current_channels = in_channels\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            out_channels = base_channels * (2 ** i)\n",
    "            layers.append(ConvBlock(\n",
    "                in_channels=current_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                leaky_slope=leaky_slope\n",
    "            ))\n",
    "            current_channels = out_channels\n",
    "        \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Calculate output feature dimension\n",
    "        self.feature_dim = base_channels * (2 ** (num_layers - 1))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, channels, height, width)\n",
    "        \n",
    "        Returns:\n",
    "            Feature tensor of shape (batch, feature_dim)\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch, feature_dim)\n",
    "        return x\n",
    "    \n",
    "    def get_feature_dim(self) -> int:\n",
    "        \"\"\"Return the output feature dimension.\"\"\"\n",
    "        return self.feature_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete Classification Pipeline\n",
    "\n",
    "This class combines the CNN encoder with an SVM classifier for end-to-end classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sEMGHHTClassifier:\n",
    "    \"\"\"\n",
    "    Complete classification pipeline combining CNN encoder and SVM classifier.\n",
    "    \n",
    "    The pipeline:\n",
    "    1. Extracts features using CNN encoder\n",
    "    2. Normalizes features using StandardScaler\n",
    "    3. Classifies using SVM (supports multi-class)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 encoder: Optional[sEMGHHTEncoder] = None,\n",
    "                 svm_kernel: str = 'rbf',\n",
    "                 svm_C: float = 1.0,\n",
    "                 svm_gamma: str = 'scale',\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Args:\n",
    "            encoder: Pre-trained or new CNN encoder (creates default if None)\n",
    "            svm_kernel: SVM kernel type ('rbf', 'linear', 'poly')\n",
    "            svm_C: SVM regularization parameter\n",
    "            svm_gamma: SVM gamma parameter\n",
    "            device: Device to run the encoder on\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize encoder\n",
    "        if encoder is None:\n",
    "            self.encoder = sEMGHHTEncoder(\n",
    "                in_channels=1, \n",
    "                base_channels=64, \n",
    "                num_layers=3\n",
    "            )\n",
    "        else:\n",
    "            self.encoder = encoder\n",
    "        \n",
    "        self.encoder.to(self.device)\n",
    "        \n",
    "        # Initialize scaler and SVM\n",
    "        self.scaler = StandardScaler()\n",
    "        self.svm = SVC(\n",
    "            kernel=svm_kernel,\n",
    "            C=svm_C,\n",
    "            gamma=svm_gamma,\n",
    "            decision_function_shape='ovr',  # One-vs-Rest for multi-class\n",
    "            probability=True  # Enable probability estimates\n",
    "        )\n",
    "        \n",
    "        self._is_fitted = False\n",
    "    \n",
    "    def extract_features(self, X: np.ndarray, batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract features from HHT matrices using the CNN encoder.\n",
    "        \n",
    "        Args:\n",
    "            X: Input array of shape (n_samples, height, width) or (n_samples, 1, height, width)\n",
    "            batch_size: Batch size for processing\n",
    "        \n",
    "        Returns:\n",
    "            Feature array of shape (n_samples, feature_dim)\n",
    "        \"\"\"\n",
    "        self.encoder.eval()\n",
    "        \n",
    "        # Ensure correct shape\n",
    "        if X.ndim == 3:\n",
    "            X = X[:, np.newaxis, :, :]  # Add channel dimension\n",
    "        \n",
    "        features_list = []\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch = torch.tensor(X[i:i+batch_size], dtype=torch.float32).to(self.device)\n",
    "                batch_features = self.encoder(batch)\n",
    "                features_list.append(batch_features.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(features_list)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Fit the classifier (extract features and train SVM).\n",
    "        \n",
    "        Args:\n",
    "            X: Training HHT matrices of shape (n_samples, height, width)\n",
    "            y: Training labels of shape (n_samples,)\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \"\"\"\n",
    "        print(\"Extracting features from training data...\")\n",
    "        features = self.extract_features(X, batch_size)\n",
    "        \n",
    "        print(\"Normalizing features...\")\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        \n",
    "        print(\"Training SVM classifier...\")\n",
    "        self.svm.fit(features_scaled, y)\n",
    "        \n",
    "        self._is_fitted = True\n",
    "        print(\"Training complete!\")\n",
    "    \n",
    "    def predict(self, X: np.ndarray, batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class labels for samples.\n",
    "        \n",
    "        Args:\n",
    "            X: Input HHT matrices of shape (n_samples, height, width)\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \n",
    "        Returns:\n",
    "            Predicted labels of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        if not self._is_fitted:\n",
    "            raise RuntimeError(\"Classifier must be fitted before predicting\")\n",
    "        \n",
    "        features = self.extract_features(X, batch_size)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        return self.svm.predict(features_scaled)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray, batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class probabilities for samples.\n",
    "        \n",
    "        Args:\n",
    "            X: Input HHT matrices of shape (n_samples, height, width)\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \n",
    "        Returns:\n",
    "            Probability array of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        if not self._is_fitted:\n",
    "            raise RuntimeError(\"Classifier must be fitted before predicting\")\n",
    "        \n",
    "        features = self.extract_features(X, batch_size)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        return self.svm.predict_proba(features_scaled)\n",
    "    \n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray, \n",
    "                 batch_size: int = 32) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate the classifier on test data.\n",
    "        \n",
    "        Args:\n",
    "            X: Test HHT matrices\n",
    "            y: True labels\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing accuracy, predictions, and classification report\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X, batch_size)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': y_pred,\n",
    "            'classification_report': classification_report(y, y_pred),\n",
    "            'confusion_matrix': confusion_matrix(y, y_pred)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "# Training Methods Overview | \u8bad\u7ec3\u65b9\u6cd5\u6982\u8ff0\n\n",
    "This notebook supports **TWO** training approaches. Choose the one that fits your needs:\n\n",
    "\u672c\u7b14\u8bb0\u672c\u652f\u6301**\u4e24\u79cd**\u8bad\u7ec3\u65b9\u6cd5\u3002\u6839\u636e\u9700\u6c42\u9009\u62e9\uff1a\n\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "# Training Methods Comparison | \u8bad\u7ec3\u65b9\u6cd5\u5bf9\u6bd4\n\n",
    "This notebook supports **TWO** training methods. Choose based on your needs:\n\n",
    "\u672c\u7b14\u8bb0\u672c\u652f\u6301**\u4e24\u79cd**\u8bad\u7ec3\u65b9\u6cd5\u3002\u6839\u636e\u9700\u6c42\u9009\u62e9\uff1a\n\n",
    "---\n\n",
    "## Method 1: CNN+SVM (Traditional) | \u65b9\u6cd5\u4e00\uff1aCNN+SVM\uff08\u4f20\u7edf\uff09\n\n",
    "**English**:\n",
    "- CNN encoder extracts features (weights **frozen**)\n",
    "- SVM classifier trains on extracted features\n",
    "- **Fast training**, good for small datasets\n",
    "- **No epochs** needed - one-shot training\n\n",
    "**\u4e2d\u6587**\uff1a\n",
    "- CNN \u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff08\u6743\u91cd**\u51bb\u7ed3**\uff09\n",
    "- SVM \u5206\u7c7b\u5668\u5728\u63d0\u53d6\u7684\u7279\u5f81\u4e0a\u8bad\u7ec3\n",
    "- **\u8bad\u7ec3\u5feb\u901f**\uff0c\u9002\u5408\u5c0f\u6570\u636e\u96c6\n",
    "- **\u65e0\u9700\u591a\u8f6e\u8bad\u7ec3** - \u4e00\u6b21\u6027\u8bad\u7ec3\n\n",
    "**Use When | \u4f7f\u7528\u573a\u666f**:\n",
    "- Limited data (< 1000 samples) | \u6570\u636e\u6709\u9650\uff08< 1000 \u6837\u672c\uff09\n",
    "- Quick prototyping | \u5feb\u901f\u539f\u578b\n",
    "- Stable baseline needed | \u9700\u8981\u7a33\u5b9a\u57fa\u7ebf\n\n",
    "---\n\n",
    "## Method 2: End-to-End with Encoder Fine-tuning | \u65b9\u6cd5\u4e8c\uff1a\u7aef\u5230\u7aef\u7f16\u7801\u5668\u5fae\u8c03\n\n",
    "**English**:\n",
    "- **Entire network** trains together (encoder + classifier)\n",
    "- Encoder **adapts** to your specific data\n",
    "- **Multi-epoch** training with checkpointing\n",
    "- Can resume from interruption\n",
    "- Real-time progress monitoring (accuracy/loss)\n\n",
    "**\u4e2d\u6587**\uff1a\n",
    "- **\u6574\u4e2a\u7f51\u7edc**\u4e00\u8d77\u8bad\u7ec3\uff08\u7f16\u7801\u5668 + \u5206\u7c7b\u5668\uff09\n",
    "- \u7f16\u7801\u5668**\u9002\u5e94**\u4f60\u7684\u7279\u5b9a\u6570\u636e\n",
    "- **\u591a\u8f6e**\u8bad\u7ec3\u4e0e\u68c0\u67e5\u70b9\u4fdd\u5b58\n",
    "- \u53ef\u4ece\u4e2d\u65ad\u5904\u6062\u590d\n",
    "- \u5b9e\u65f6\u8fdb\u5ea6\u76d1\u63a7\uff08\u51c6\u786e\u7387/\u635f\u5931\uff09\n\n",
    "**Use When | \u4f7f\u7528\u573a\u666f**:\n",
    "- Large dataset (> 1000 samples) | \u5927\u6570\u636e\u96c6\uff08> 1000 \u6837\u672c\uff09\n",
    "- Maximum accuracy needed | \u9700\u8981\u6700\u5927\u51c6\u786e\u7387\n",
    "- Domain-specific data | \u9886\u57df\u7279\u5b9a\u6570\u636e\n\n",
    "---\n\n",
    "**\ud83d\udccd Scroll down to find sections for each method:**\n\n",
    "**\ud83d\udccd \u5411\u4e0b\u6eda\u52a8\u627e\u5230\u6bcf\u79cd\u65b9\u6cd5\u7684\u5bf9\u5e94\u7ae0\u8282\uff1a**\n\n",
    "- **Section 5**: CNN+SVM Training | \u7b2c 5 \u8282\uff1aCNN+SVM \u8bad\u7ec3\n",
    "- **Section 6**: End-to-End Training | \u7b2c 6 \u8282\uff1a\u7aef\u5230\u7aef\u8bad\u7ec3\n\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_end_to_end_with_checkpointing(\n",
    "    model: nn.Module,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    epochs: int = 50,\n",
    "    batch_size: int = 16,\n",
    "    learning_rate: float = 0.001,\n",
    "    checkpoint_interval: int = 5,\n",
    "    checkpoint_dir: str = None,\n",
    "    resume_from: str = None,\n",
    "    device: torch.device = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train end-to-end model with comprehensive checkpointing support.\n",
    "    \u7aef\u5230\u7aef\u8bad\u7ec3\u6a21\u578b\uff0c\u652f\u6301\u5b8c\u6574\u7684\u68c0\u67e5\u70b9\u529f\u80fd\u3002\n",
    "    \n",
    "    Features | \u529f\u80fd:\n",
    "    - Multi-epoch training | \u591a\u8f6e\u8bad\u7ec3\n",
    "    - Checkpoint saving every N epochs | \u6bcf N \u8f6e\u4fdd\u5b58\u68c0\u67e5\u70b9\n",
    "    - Resume from interruption | \u4e2d\u65ad\u540e\u6062\u590d\n",
    "    - Best model auto-save | \u81ea\u52a8\u4fdd\u5b58\u6700\u4f73\u6a21\u578b\n",
    "    - Real-time progress display | \u5b9e\u65f6\u8fdb\u5ea6\u663e\u793a\n",
    "    \n",
    "    Args:\n",
    "        model: Model to train | \u8981\u8bad\u7ec3\u7684\u6a21\u578b\n",
    "        X_train, y_train: Training data | \u8bad\u7ec3\u6570\u636e\n",
    "        X_val, y_val: Validation data | \u9a8c\u8bc1\u6570\u636e\n",
    "        epochs: Number of epochs | \u8bad\u7ec3\u8f6e\u6570\n",
    "        batch_size: Batch size | \u6279\u6b21\u5927\u5c0f\n",
    "        learning_rate: Learning rate | \u5b66\u4e60\u7387\n",
    "        checkpoint_interval: Save checkpoint every N epochs | \u6bcf N \u8f6e\u4fdd\u5b58\u68c0\u67e5\u70b9\n",
    "        checkpoint_dir: Where to save checkpoints | \u68c0\u67e5\u70b9\u4fdd\u5b58\u76ee\u5f55\n",
    "        resume_from: Path to checkpoint to resume from | \u8981\u6062\u590d\u7684\u68c0\u67e5\u70b9\u8def\u5f84\n",
    "        device: Device to use | \u4f7f\u7528\u7684\u8bbe\u5907\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training history | \u5305\u542b\u8bad\u7ec3\u5386\u53f2\u7684\u5b57\u5178\n",
    "    \"\"\"\n",
    "    # Input validation | \u8f93\u5165\u9a8c\u8bc1\n",
    "    if epochs <= 0:\n",
    "        raise ValueError(f\"epochs must be > 0, got {epochs}\")\n",
    "    if batch_size <= 0:\n",
    "        raise ValueError(f\"batch_size must be > 0, got {batch_size}\")\n",
    "    if learning_rate <= 0:\n",
    "        raise ValueError(f\"learning_rate must be > 0, got {learning_rate}\")\n",
    "    if checkpoint_interval <= 0:\n",
    "        raise ValueError(f\"checkpoint_interval must be > 0, got {checkpoint_interval}\")\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = CHECKPOINT_DIR\n",
    "    \n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Ensure correct shape\n",
    "    if X_train.ndim == 3:\n",
    "        X_train = X_train[:, np.newaxis, :, :]\n",
    "        X_val = X_val[:, np.newaxis, :, :]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.long)\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=LR_SCHEDULER_FACTOR, patience=LR_SCHEDULER_PATIENCE\n",
    "    )\n",
    "    \n",
    "    # Initialize history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Resume from checkpoint if specified\n",
    "    if resume_from and os.path.exists(resume_from):\n",
    "        print(f'\\n\ud83d\udcc2 Resuming from checkpoint: {resume_from}')\n",
    "        checkpoint = torch.load(resume_from, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_acc = checkpoint.get('best_val_acc', 0.0)\n",
    "        history = checkpoint.get('history', history)\n",
    "        print(f'\u2705 Resumed from epoch {start_epoch}, best val acc: {best_val_acc:.4f}')\n",
    "    \n",
    "    print(f'\\n\ud83d\ude80 Training from epoch {start_epoch} to {epochs}...')\n",
    "    print(f'Device: {device}')\n",
    "    print(f'Train samples: {len(X_train)}, Val samples: {len(X_val)}')\n",
    "    print(f'Checkpoint interval: every {checkpoint_interval} epochs')\n",
    "    print('='*70)\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * batch_X.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += batch_y.size(0)\n",
    "            train_correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                val_loss += loss.item() * batch_X.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += batch_y.size(0)\n",
    "                val_correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_acc)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch+1:3d}/{epochs}] | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n",
    "              f\"LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'history': history\n",
    "            }, best_path)\n",
    "            print(f\"  \u2b50 New best model! Val Acc: {val_acc:.4f} (saved to {best_path})\")\n",
    "        \n",
    "        # Save checkpoint at interval\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'history': history\n",
    "            }, checkpoint_path)\n",
    "            print(f\"  \ud83d\udcbe Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    final_path = os.path.join(checkpoint_dir, 'final_model.pt')\n",
    "    torch.save({\n",
    "        'epoch': epochs - 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'history': history\n",
    "    }, final_path)\n",
    "    \n",
    "    print('='*70)\n",
    "    print(f'\\n\u2705 Training Complete!')\n",
    "    print(f'Best Validation Accuracy: {best_val_acc:.4f}')\n",
    "    print(f'Final model saved to: {final_path}')\n",
    "    print(f'Best model saved to: {best_path}')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history: dict):\n",
    "    \"\"\"\n",
    "    Plot training history (loss and accuracy).\n",
    "    \u7ed8\u5236\u8bad\u7ec3\u5386\u53f2\uff08\u635f\u5931\u548c\u51c6\u786e\u7387\uff09\u3002\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss | \u8bad\u7ec3\u635f\u5931', linewidth=2)\n",
    "    axes[0].plot(epochs, history['val_loss'], 'r-', label='Val Loss | \u9a8c\u8bc1\u635f\u5931', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch | \u8f6e\u6b21', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss | \u635f\u5931', fontsize=12)\n",
    "    axes[0].set_title('Training and Validation Loss | \u8bad\u7ec3\u548c\u9a8c\u8bc1\u635f\u5931', fontsize=14)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[1].plot(epochs, history['train_acc'], 'b-', label='Train Acc | \u8bad\u7ec3\u51c6\u786e\u7387', linewidth=2)\n",
    "    axes[1].plot(epochs, history['val_acc'], 'r-', label='Val Acc | \u9a8c\u8bc1\u51c6\u786e\u7387', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch | \u8f6e\u6b21', fontsize=12)\n",
    "    axes[1].set_ylabel('Accuracy | \u51c6\u786e\u7387', fontsize=12)\n",
    "    axes[1].set_title('Training and Validation Accuracy | \u8bad\u7ec3\u548c\u9a8c\u8bc1\u51c6\u786e\u7387', fontsize=14)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot learning rate\n",
    "    if 'learning_rate' in history:\n",
    "        axes[2].plot(epochs, history['learning_rate'], 'g-', linewidth=2)\n",
    "        axes[2].set_xlabel('Epoch | \u8f6e\u6b21', fontsize=12)\n",
    "        axes[2].set_ylabel('Learning Rate | \u5b66\u4e60\u7387', fontsize=12)\n",
    "        axes[2].set_title('Learning Rate Schedule | \u5b66\u4e60\u7387\u8c03\u5ea6', fontsize=14)\n",
    "        axes[2].set_yscale('log')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(f'\\n\ud83d\udcca Training Statistics | \u8bad\u7ec3\u7edf\u8ba1:')\n",
    "    print(f'   Final Train Acc | \u6700\u7ec8\u8bad\u7ec3\u51c6\u786e\u7387: {history[\"train_acc\"][-1]:.4f}')\n",
    "    print(f'   Final Val Acc | \u6700\u7ec8\u9a8c\u8bc1\u51c6\u786e\u7387: {history[\"val_acc\"][-1]:.4f}')\n",
    "    print(f'   Best Val Acc | \u6700\u4f73\u9a8c\u8bc1\u51c6\u786e\u7387: {max(history[\"val_acc\"]):.4f}')\n",
    "    print(f'   Final Train Loss | \u6700\u7ec8\u8bad\u7ec3\u635f\u5931: {history[\"train_loss\"][-1]:.4f}')\n",
    "    print(f'   Final Val Loss | \u6700\u7ec8\u9a8c\u8bc1\u635f\u5931: {history[\"val_loss\"][-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class sEMGHHTEndToEndClassifier(nn.Module):\n    \"\"\"\n    End-to-end trainable classifier with CNN encoder and linear classification head.\n    \n    This version allows the encoder to be fine-tuned during training, enabling\n    the model to adapt to your specific data distribution.\n    \n    Architecture:\n        Input (1\u00d7256\u00d7256) \n        \u2192 CNN Encoder (3 ConvBlocks) \n        \u2192 Features (256-dim)\n        \u2192 Dropout \u2192 FC(256\u2192128) \u2192 ReLU \u2192 Dropout \u2192 FC(128\u2192n_classes)\n        \u2192 Logits (n_classes)\n    \n    Args:\n        n_classes (int): Number of output classes (default: 4)\n            For 6-class problem (M_full, M_half, M_invalid, F_full, F_half, F_invalid), use n_classes=6\n        in_channels (int): Number of input channels (default: 1 for grayscale HHT matrix)\n        base_channels (int): Base number of channels in first conv layer (default: 64)\n            Channel progression: 64 \u2192 128 \u2192 256\n        num_encoder_layers (int): Number of convolutional blocks in encoder (default: 3)\n            Must match the encoder architecture you want to use\n        dropout_rate (float): Dropout probability for regularization (default: 0.5)\n            Range: 0.0 (no dropout) to 0.9 (high dropout)\n            Typical values: 0.3-0.6\n    \n    Example:\n        >>> model = sEMGHHTEndToEndClassifier(\n        ...     n_classes=6,\n        ...     base_channels=64,\n        ...     num_encoder_layers=3,\n        ...     dropout_rate=0.5\n        ... )\n        >>> x = torch.randn(16, 1, 256, 256)  # Batch of 16 HHT matrices\n        >>> logits = model(x)  # Output shape: (16, 6)\n    \"\"\"\n    \n    def __init__(self, \n                 n_classes: int = 4,\n                 in_channels: int = 1,\n                 base_channels: int = 64,\n                 num_encoder_layers: int = 3,\n                 dropout_rate: float = 0.5):\n        super(sEMGHHTEndToEndClassifier, self).__init__()\n        \n        self.encoder = sEMGHHTEncoder(\n            in_channels=in_channels,\n            base_channels=base_channels,\n            num_layers=num_encoder_layers\n        )\n        \n        feature_dim = self.encoder.get_feature_dim()\n        \n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout_rate),\n            nn.Linear(feature_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(128, n_classes)\n        )\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        features = self.encoder(x)\n        logits = self.classifier(features)\n        return logits\n    \n    def get_features(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Extract features without classification.\"\"\"\n        return self.encoder(x)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_end_to_end(model: nn.Module,\n",
    "                     X_train: np.ndarray,\n",
    "                     y_train: np.ndarray,\n",
    "                     X_val: np.ndarray,\n",
    "                     y_val: np.ndarray,\n",
    "                     epochs: int = 50,\n",
    "                     batch_size: int = 16,\n",
    "                     learning_rate: float = 0.001,\n",
    "                     device: torch.device = torch.device('cpu')) -> dict:\n",
    "    \"\"\"\n",
    "    Train the end-to-end model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        X_train, y_train: Training data and labels\n",
    "        X_val, y_val: Validation data and labels\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        learning_rate: Learning rate\n",
    "        device: Device to train on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing training history\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Ensure correct shape\n",
    "    if X_train.ndim == 3:\n",
    "        X_train = X_train[:, np.newaxis, :, :]\n",
    "        X_val = X_val[:, np.newaxis, :, :]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.long)\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=LR_SCHEDULER_FACTOR, patience=LR_SCHEDULER_PATIENCE\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * batch_X.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += batch_y.size(0)\n",
    "            train_correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                val_loss += loss.item() * batch_X.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += batch_y.size(0)\n",
    "                val_correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_svm_classifier(classifier: sEMGHHTClassifier, path: str):\n",
    "    \"\"\"Save the SVM-based classifier to disk.\"\"\"\n",
    "    # Save encoder\n",
    "    torch.save(classifier.encoder.state_dict(), f\"{path}_encoder.pt\")\n",
    "    \n",
    "    # Save scaler and SVM\n",
    "    with open(f\"{path}_scaler.pkl\", 'wb') as f:\n",
    "        pickle.dump(classifier.scaler, f)\n",
    "    \n",
    "    with open(f\"{path}_svm.pkl\", 'wb') as f:\n",
    "        pickle.dump(classifier.svm, f)\n",
    "    \n",
    "    print(f\"Classifier saved to {path}_*.pt/pkl\")\n",
    "\n",
    "def load_svm_classifier(path: str, device: torch.device = torch.device('cpu')) -> sEMGHHTClassifier:\n",
    "    \"\"\"Load a saved SVM-based classifier.\"\"\"\n",
    "    classifier = sEMGHHTClassifier(device=device)\n",
    "    \n",
    "    # Load encoder\n",
    "    classifier.encoder.load_state_dict(torch.load(f\"{path}_encoder.pt\", map_location=device))\n",
    "    \n",
    "    # Load scaler and SVM\n",
    "    with open(f\"{path}_scaler.pkl\", 'rb') as f:\n",
    "        classifier.scaler = pickle.load(f)\n",
    "    \n",
    "    with open(f\"{path}_svm.pkl\", 'rb') as f:\n",
    "        classifier.svm = pickle.load(f)\n",
    "    \n",
    "    classifier._is_fitted = True\n",
    "    print(f\"Classifier loaded from {path}_*.pt/pkl\")\n",
    "    return classifier\n",
    "\n",
    "def save_e2e_model(model: nn.Module, path: str):\n",
    "    \"\"\"Save the end-to-end model.\"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_e2e_model(path: str, n_classes: int = 4, \n",
    "                   device: torch.device = torch.device('cpu')) -> sEMGHHTEndToEndClassifier:\n",
    "    \"\"\"Load a saved end-to-end model.\"\"\"\n",
    "    model = sEMGHHTEndToEndClassifier(n_classes=n_classes)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data Training | \u771f\u5b9e\u6570\u636e\u8bad\u7ec3\n",
    "\n",
    "Train the classifier on real sEMG HHT data from the Kaggle dataset.\n",
    "\u5728 Kaggle \u6570\u636e\u96c6\u7684\u771f\u5b9e sEMG HHT \u6570\u636e\u4e0a\u8bad\u7ec3\u5206\u7c7b\u5668\u3002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "# \u5bfc\u5165\u6240\u9700\u6a21\u5757\n",
    "import glob\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse filename to extract labels.\n",
    "    \u89e3\u6790\u6587\u4ef6\u540d\u4ee5\u63d0\u53d6\u6807\u7b7e\u3002\n",
    "    \n",
    "    Returns None if filename starts with 'Test' (unlabeled test data)\n",
    "    \u5982\u679c\u6587\u4ef6\u540d\u4ee5 'Test' \u5f00\u5934\u5219\u8fd4\u56de None\uff08\u672a\u6807\u8bb0\u7684\u6d4b\u8bd5\u6570\u636e\uff09\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    # Skip files that start with 'Test'\n",
    "    # \u8df3\u8fc7\u4ee5 'Test' \u5f00\u5934\u7684\u6587\u4ef6\n",
    "    if basename.lower().startswith('test'):\n",
    "        return None\n",
    "    \n",
    "    # Extract gender (M or F)\n",
    "    # \u63d0\u53d6\u6027\u522b\uff08M \u6216 F\uff09\n",
    "    gender_match = re.search(r'[_-]([MF])[_-]', basename)\n",
    "    if not gender_match:\n",
    "        return None\n",
    "    gender = gender_match.group(1)\n",
    "    \n",
    "    # Extract movement quality\n",
    "    # \u63d0\u53d6\u52a8\u4f5c\u8d28\u91cf\n",
    "    basename_lower = basename.lower()\n",
    "    if 'fatiguetest' in basename_lower or 'full' in basename_lower:\n",
    "        movement = 'full'\n",
    "    elif 'half' in basename_lower:\n",
    "        movement = 'half'\n",
    "    elif 'invalid' in basename_lower or 'wrong' in basename_lower:\n",
    "        movement = 'invalid'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return {'gender': gender, 'movement': movement}\n",
    "\n",
    "def load_data_from_directory(data_dir):\n",
    "    \"\"\"\n",
    "    Load HHT matrices from npz files.\n",
    "    \u4ece npz \u6587\u4ef6\u52a0\u8f7d HHT \u77e9\u9635\u3002\n",
    "    \"\"\"\n",
    "    npz_files = glob.glob(os.path.join(data_dir, '*.npz'))\n",
    "    \n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    filenames = []\n",
    "    test_files = []\n",
    "    \n",
    "    # Create label encoder\n",
    "    # \u521b\u5efa\u6807\u7b7e\u7f16\u7801\u5668\n",
    "    all_classes = ['M_full', 'M_half', 'M_invalid', 'F_full', 'F_half', 'F_invalid']\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_classes)\n",
    "    \n",
    "    for npz_file in npz_files:\n",
    "        labels = parse_filename(npz_file)\n",
    "        \n",
    "        if labels is None:\n",
    "            test_files.append(npz_file)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            data = np.load(npz_file)\n",
    "            if 'hht' in data:\n",
    "                hht_matrix = data['hht']\n",
    "            else:\n",
    "                hht_matrix = data[list(data.keys())[0]]\n",
    "            \n",
    "            if hht_matrix.shape != (256, 256):\n",
    "                continue\n",
    "            \n",
    "            # Create combined label\n",
    "            # \u521b\u5efa\u7ec4\u5408\u6807\u7b7e\n",
    "            combined = f\"{labels['gender']}_{labels['movement']}\"\n",
    "            label = label_encoder.transform([combined])[0]\n",
    "            \n",
    "            X_list.append(hht_matrix)\n",
    "            y_list.append(label)\n",
    "            filenames.append(npz_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error loading {npz_file}: {e}')\n",
    "            continue\n",
    "    \n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    print(f'\\nLoaded {len(X)} training samples | \u52a0\u8f7d\u4e86 {len(X)} \u4e2a\u8bad\u7ec3\u6837\u672c')\n",
    "    print(f'Found {len(test_files)} test files | \u627e\u5230 {len(test_files)} \u4e2a\u6d4b\u8bd5\u6587\u4ef6')\n",
    "    print(f'\\nClass distribution | \u7c7b\u522b\u5206\u5e03:')\n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        count = np.sum(y == i)\n",
    "        print(f'  {class_name}: {count} samples')\n",
    "    \n",
    "    return X, y, filenames, test_files, label_encoder\n",
    "\n",
    "# Load data from Kaggle dataset\n",
    "# \u4ece Kaggle \u6570\u636e\u96c6\u52a0\u8f7d\u6570\u636e\n",
    "if os.path.exists(DATA_DIR):\n",
    "    X, y, filenames, test_files, label_encoder = load_data_from_directory(DATA_DIR)\n",
    "else:\n",
    "    print(f'Data directory not found | \u6570\u636e\u76ee\u5f55\u672a\u627e\u5230: {DATA_DIR}')\n",
    "    print('Please ensure the HILBERTMATRIX_NPZ dataset is added to this notebook.')\n",
    "    print('\u8bf7\u786e\u4fdd HILBERTMATRIX_NPZ \u6570\u636e\u96c6\u5df2\u6dfb\u52a0\u5230\u6b64\u7b14\u8bb0\u672c\u3002')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real HHT Transformation (For Reference)\n",
    "\n",
    "When you have real sEMG data, you can use the following functions to perform HHT transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier | \u8bad\u7ec3\u5206\u7c7b\u5668\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if 'X' in locals() and len(X) > 0:\n",
    "    # Split data | \u5206\u5272\u6570\u636e\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f'Training set | \u8bad\u7ec3\u96c6: {X_train.shape[0]} samples')\n",
    "    print(f'Validation set | \u9a8c\u8bc1\u96c6: {X_val.shape[0]} samples')\n",
    "    \n",
    "    # Initialize classifier | \u521d\u59cb\u5316\u5206\u7c7b\u5668\n",
    "    classifier = sEMGHHTClassifier(\n",
    "        encoder=None,\n",
    "        svm_kernel=SVM_KERNEL,\n",
    "        svm_C=SVM_C,\n",
    "        svm_gamma=SVM_GAMMA,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Train | \u8bad\u7ec3\n",
    "    print('\\n' + '='*60)\n",
    "    print('Training SVM Classifier | \u8bad\u7ec3 SVM \u5206\u7c7b\u5668')\n",
    "    print('='*60)\n",
    "    classifier.fit(X_train, y_train, batch_size=32)\n",
    "    \n",
    "    # Evaluate on training set | \u5728\u8bad\u7ec3\u96c6\u4e0a\u8bc4\u4f30\n",
    "    train_results = classifier.evaluate(X_train, y_train, batch_size=32)\n",
    "    print(f'\\nTraining Accuracy | \u8bad\u7ec3\u51c6\u786e\u7387: {train_results[\"accuracy\"]:.4f}')\n",
    "    \n",
    "    # Evaluate on validation set | \u5728\u9a8c\u8bc1\u96c6\u4e0a\u8bc4\u4f30\n",
    "    val_results = classifier.evaluate(X_val, y_val, batch_size=32)\n",
    "    print(f'Validation Accuracy | \u9a8c\u8bc1\u51c6\u786e\u7387: {val_results[\"accuracy\"]:.4f}')\n",
    "    print('\\nClassification Report | \u5206\u7c7b\u62a5\u544a:')\n",
    "    print(val_results['classification_report'])\n",
    "else:\n",
    "    print('No data loaded. Please check the data directory.')\n",
    "    print('\u672a\u52a0\u8f7d\u6570\u636e\u3002\u8bf7\u68c0\u67e5\u6570\u636e\u76ee\u5f55\u3002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hht_matrix(signal: np.ndarray, \n",
    "                       fs: float, \n",
    "                       matrix_size: int = 256,\n",
    "                       max_imf: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute HHT (Hilbert-Huang Transform) matrix from a signal.\n",
    "    \n",
    "    This is a reference implementation. For real usage, you may need to install:\n",
    "    pip install PyEMD scipy\n",
    "    \n",
    "    Args:\n",
    "        signal: 1D input signal\n",
    "        fs: Sampling frequency\n",
    "        matrix_size: Output matrix size (matrix_size \u00d7 matrix_size)\n",
    "        max_imf: Maximum number of IMFs to extract\n",
    "    \n",
    "    Returns:\n",
    "        HHT matrix of shape (matrix_size, matrix_size)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from PyEMD import EMD\n",
    "        from scipy.signal import hilbert\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install PyEMD and scipy: pip install PyEMD scipy\")\n",
    "    \n",
    "    # Perform EMD\n",
    "    emd = EMD()\n",
    "    imfs = emd(signal, max_imf=max_imf)\n",
    "    \n",
    "    # Compute Hilbert transform for each IMF\n",
    "    n_samples = len(signal)\n",
    "    t = np.arange(n_samples) / fs\n",
    "    \n",
    "    # Initialize time-frequency matrix\n",
    "    freq_bins = np.linspace(0, fs/2, matrix_size)\n",
    "    time_bins = np.linspace(0, t[-1], matrix_size)\n",
    "    hht_matrix = np.zeros((matrix_size, matrix_size))\n",
    "    \n",
    "    for imf in imfs:\n",
    "        # Compute analytic signal\n",
    "        analytic = hilbert(imf)\n",
    "        amplitude = np.abs(analytic)\n",
    "        phase = np.unwrap(np.angle(analytic))\n",
    "        \n",
    "        # Compute instantaneous frequency\n",
    "        inst_freq = np.diff(phase) / (2 * np.pi) * fs\n",
    "        inst_freq = np.concatenate([inst_freq, [inst_freq[-1]]])\n",
    "        inst_freq = np.clip(inst_freq, 0, fs/2)\n",
    "        \n",
    "        # Map to time-frequency matrix\n",
    "        for i, (ti, fi, ai) in enumerate(zip(t, inst_freq, amplitude)):\n",
    "            t_idx = int(ti / t[-1] * (matrix_size - 1))\n",
    "            f_idx = int(fi / (fs/2) * (matrix_size - 1))\n",
    "            \n",
    "            t_idx = np.clip(t_idx, 0, matrix_size - 1)\n",
    "            f_idx = np.clip(f_idx, 0, matrix_size - 1)\n",
    "            \n",
    "            hht_matrix[f_idx, t_idx] += ai\n",
    "    \n",
    "    # Normalize\n",
    "    if hht_matrix.max() > 0:\n",
    "        hht_matrix = hht_matrix / hht_matrix.max()\n",
    "    \n",
    "    return hht_matrix.astype(np.float32)\n",
    "\n",
    "\n",
    "# Example usage (uncomment when you have real data):\n",
    "# signal = np.random.randn(1000)  # Replace with real sEMG signal\n",
    "# fs = 1000  # Sampling frequency in Hz\n",
    "# hht_matrix = compute_hht_matrix(signal, fs, matrix_size=256)\n",
    "# plt.imshow(hht_matrix, aspect='auto', cmap='hot', origin='lower')\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "# 5. Method 1: CNN+SVM Training | \u65b9\u6cd5\u4e00\uff1aCNN+SVM \u8bad\u7ec3\n\n",
    "## Usage Instructions | \u4f7f\u7528\u8bf4\u660e\n\n",
    "**English**: This section shows how to use the traditional CNN+SVM approach. The CNN encoder is used for feature extraction only (weights frozen), and an SVM is trained on those features.\n\n",
    "**\u4e2d\u6587**\uff1a\u672c\u8282\u5c55\u793a\u5982\u4f55\u4f7f\u7528\u4f20\u7edf\u7684 CNN+SVM \u65b9\u6cd5\u3002CNN \u7f16\u7801\u5668\u4ec5\u7528\u4e8e\u7279\u5f81\u63d0\u53d6\uff08\u6743\u91cd\u51bb\u7ed3\uff09\uff0cSVM \u5728\u8fd9\u4e9b\u7279\u5f81\u4e0a\u8bad\u7ec3\u3002\n\n",
    "### Steps | \u6b65\u9aa4:\n",
    "1. Load and split data | \u52a0\u8f7d\u548c\u5206\u5272\u6570\u636e\n",
    "2. Initialize CNN+SVM classifier | \u521d\u59cb\u5316 CNN+SVM \u5206\u7c7b\u5668\n",
    "3. Train (one-shot, no epochs) | \u8bad\u7ec3\uff08\u4e00\u6b21\u6027\uff0c\u65e0\u9700\u591a\u8f6e\uff09\n",
    "4. Evaluate and save | \u8bc4\u4f30\u548c\u4fdd\u5b58\n\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN+SVM classifier | \u8bad\u7ec3 CNN+SVM \u5206\u7c7b\u5668\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if 'X' in locals() and len(X) > 0:\n",
    "    print('='*70)\n",
    "    print('METHOD 1: CNN+SVM Training | \u65b9\u6cd5\u4e00\uff1aCNN+SVM \u8bad\u7ec3')\n",
    "    print('='*70)\n",
    "    \n",
    "    # Split data | \u5206\u5272\u6570\u636e\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f'\\nTraining set | \u8bad\u7ec3\u96c6: {X_train.shape[0]} samples')\n",
    "    print(f'Validation set | \u9a8c\u8bc1\u96c6: {X_val.shape[0]} samples')\n",
    "    \n",
    "    # Initialize classifier | \u521d\u59cb\u5316\u5206\u7c7b\u5668\n",
    "    print('\\n\ud83d\udce6 Initializing CNN+SVM classifier...')\n",
    "    svm_classifier = sEMGHHTClassifier(\n",
    "        encoder=None,  # Will create default encoder\n",
    "        svm_kernel=SVM_KERNEL,\n",
    "        svm_C=SVM_C,\n",
    "        svm_gamma=SVM_GAMMA,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Train | \u8bad\u7ec3\n",
    "    print('\\n\ud83d\ude80 Training SVM classifier (this is a one-shot process)...')\n",
    "    print('\u8bad\u7ec3 SVM \u5206\u7c7b\u5668\uff08\u8fd9\u662f\u4e00\u6b21\u6027\u8fc7\u7a0b\uff09...')\n",
    "    svm_classifier.fit(X_train, y_train, batch_size=32)\n",
    "    \n",
    "    # Evaluate on training set | \u5728\u8bad\u7ec3\u96c6\u4e0a\u8bc4\u4f30\n",
    "    print('\\n\ud83d\udcca Evaluating on training set | \u5728\u8bad\u7ec3\u96c6\u4e0a\u8bc4\u4f30...')\n",
    "    train_results = svm_classifier.evaluate(X_train, y_train, batch_size=32)\n",
    "    print(f'Training Accuracy | \u8bad\u7ec3\u51c6\u786e\u7387: {train_results[\"accuracy\"]:.4f}')\n",
    "    \n",
    "    # Evaluate on validation set | \u5728\u9a8c\u8bc1\u96c6\u4e0a\u8bc4\u4f30\n",
    "    print('\\n\ud83d\udcca Evaluating on validation set | \u5728\u9a8c\u8bc1\u96c6\u4e0a\u8bc4\u4f30...')\n",
    "    val_results = svm_classifier.evaluate(X_val, y_val, batch_size=32)\n",
    "    print(f'Validation Accuracy | \u9a8c\u8bc1\u51c6\u786e\u7387: {val_results[\"accuracy\"]:.4f}')\n",
    "    \n",
    "    print('\\n\ud83d\udccb Classification Report | \u5206\u7c7b\u62a5\u544a:')\n",
    "    print(val_results['classification_report'])\n",
    "    \n",
    "    # Save model | \u4fdd\u5b58\u6a21\u578b\n",
    "    svm_model_path = os.path.join(CHECKPOINT_DIR, 'cnn_svm_model')\n",
    "    save_svm_classifier(svm_classifier, svm_model_path)\n",
    "    \n",
    "    print(f'\\n\u2705 CNN+SVM training complete!')\n",
    "    print(f'Model saved to: {svm_model_path}_*')\n",
    "    \n",
    "else:\n",
    "    print('\u26a0\ufe0f  No data loaded. Please run the data loading cell first.')\n",
    "    print('\u672a\u52a0\u8f7d\u6570\u636e\u3002\u8bf7\u5148\u8fd0\u884c\u6570\u636e\u52a0\u8f7d\u5355\u5143\u683c\u3002')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "# 6. Method 2: End-to-End Training with Encoder Fine-tuning | \u65b9\u6cd5\u4e8c\uff1a\u7aef\u5230\u7aef\u7f16\u7801\u5668\u5fae\u8c03\u8bad\u7ec3\n\n",
    "## Usage Instructions | \u4f7f\u7528\u8bf4\u660e\n\n",
    "**English**: This section shows how to train the entire network end-to-end. Both the CNN encoder and the classification head are trained together, allowing the encoder to adapt to your specific data.\n\n",
    "**\u4e2d\u6587**\uff1a\u672c\u8282\u5c55\u793a\u5982\u4f55\u7aef\u5230\u7aef\u8bad\u7ec3\u6574\u4e2a\u7f51\u7edc\u3002CNN \u7f16\u7801\u5668\u548c\u5206\u7c7b\u5934\u4e00\u8d77\u8bad\u7ec3\uff0c\u5141\u8bb8\u7f16\u7801\u5668\u9002\u5e94\u4f60\u7684\u7279\u5b9a\u6570\u636e\u3002\n\n",
    "### Key Features | \u5173\u952e\u7279\u6027:\n",
    "- \u2705 **Multi-epoch training** | \u591a\u8f6e\u8bad\u7ec3\n",
    "- \u2705 **Automatic checkpointing** | \u81ea\u52a8\u68c0\u67e5\u70b9\u4fdd\u5b58\n",
    "- \u2705 **Resume from interruption** | \u4e2d\u65ad\u540e\u6062\u590d\n",
    "- \u2705 **Best model auto-save** | \u6700\u4f73\u6a21\u578b\u81ea\u52a8\u4fdd\u5b58\n",
    "- \u2705 **Real-time progress** | \u5b9e\u65f6\u8fdb\u5ea6\u663e\u793a\n",
    "- \u2705 **Learning rate scheduling** | \u5b66\u4e60\u7387\u8c03\u5ea6\n\n",
    "### Training Process | \u8bad\u7ec3\u6d41\u7a0b:\n",
    "1. Initialize model | \u521d\u59cb\u5316\u6a21\u578b\n",
    "2. Start training (or resume) | \u5f00\u59cb\u8bad\u7ec3\uff08\u6216\u6062\u590d\uff09\n",
    "3. Monitor progress | \u76d1\u63a7\u8fdb\u5ea6\n",
    "4. Visualize results | \u53ef\u89c6\u5316\u7ed3\u679c\n\n",
    "### Checkpoints | \u68c0\u67e5\u70b9\u8bf4\u660e:\n",
    "- `best_model.pt`: Best performing model (highest val accuracy) | \u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\uff08\u6700\u9ad8\u9a8c\u8bc1\u51c6\u786e\u7387\uff09\n",
    "- `checkpoint_epoch_N.pt`: Saved every N epochs | \u6bcf N \u8f6e\u4fdd\u5b58\n",
    "- `final_model.pt`: Final model after all epochs | \u6240\u6709\u8f6e\u6b21\u540e\u7684\u6700\u7ec8\u6a21\u578b\n\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END-TO-END TRAINING - Initial Training | \u7aef\u5230\u7aef\u8bad\u7ec3 - \u521d\u59cb\u8bad\u7ec3\n",
    "# Run this cell to start training from scratch\n",
    "# \u8fd0\u884c\u6b64\u5355\u5143\u683c\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\n",
    "\n",
    "if 'X' in locals() and len(X) > 0:\n",
    "    print('='*70)\n",
    "    print('METHOD 2: End-to-End Training | \u65b9\u6cd5\u4e8c\uff1a\u7aef\u5230\u7aef\u8bad\u7ec3')\n",
    "    print('='*70)\n",
    "    \n",
    "    # Determine number of classes\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f'\\nNumber of classes | \u7c7b\u522b\u6570: {n_classes}')\n",
    "    \n",
    "    # Initialize end-to-end model | \u521d\u59cb\u5316\u7aef\u5230\u7aef\u6a21\u578b\n",
    "    print('\\n\ud83d\udce6 Initializing end-to-end model...')\n",
    "    e2e_model = sEMGHHTEndToEndClassifier(\n",
    "        n_classes=n_classes,\n",
    "        in_channels=1,\n",
    "        base_channels=MODEL_BASE_CHANNELS,\n",
    "        num_encoder_layers=MODEL_NUM_ENCODER_LAYERS,\n",
    "        dropout_rate=MODEL_DROPOUT_RATE\n",
    "    )\n",
    "    \n",
    "    print(f'Model architecture | \u6a21\u578b\u67b6\u6784:')\n",
    "    print(f'  - Encoder: 3-layer CNN | \u7f16\u7801\u5668\uff1a3 \u5c42 CNN')\n",
    "    print(f'  - Feature dim: {e2e_model.encoder.get_feature_dim()}')\n",
    "    print(f'  - Classifier: 2-layer FC | \u5206\u7c7b\u5668\uff1a2 \u5c42\u5168\u8fde\u63a5')\n",
    "    print(f'  - Output classes: {n_classes}')\n",
    "    \n",
    "    # Training configuration | \u8bad\u7ec3\u914d\u7f6e\n",
    "    EPOCHS = TRAIN_EPOCHS  # Configured above | \u5728\u4e0a\u9762\u914d\u7f6e\n",
    "    BATCH_SIZE = TRAIN_BATCH_SIZE\n",
    "    LEARNING_RATE = TRAIN_LEARNING_RATE\n",
    "    CHECKPOINT_INTERVAL = TRAIN_CHECKPOINT_INTERVAL  # Configured above | \u5728\u4e0a\u9762\u914d\u7f6e\n",
    "    \n",
    "    print(f'\\n\u2699\ufe0f  Training Configuration | \u8bad\u7ec3\u914d\u7f6e:')\n",
    "    print(f'  - Epochs | \u8bad\u7ec3\u8f6e\u6570: {EPOCHS}')\n",
    "    print(f'  - Batch size | \u6279\u6b21\u5927\u5c0f: {BATCH_SIZE}')\n",
    "    print(f'  - Learning rate | \u5b66\u4e60\u7387: {LEARNING_RATE}')\n",
    "    print(f'  - Checkpoint interval | \u68c0\u67e5\u70b9\u95f4\u9694: {CHECKPOINT_INTERVAL} epochs')\n",
    "    print(f'  - Device | \u8bbe\u5907: {device}')\n",
    "    \n",
    "    # Start training | \u5f00\u59cb\u8bad\u7ec3\n",
    "    print('\\n\ud83d\ude80 Starting training... | \u5f00\u59cb\u8bad\u7ec3...')\n",
    "    print('\ud83d\udca1 Tip: Training can be interrupted (Ctrl+C) and resumed later')\n",
    "    print('\u63d0\u793a\uff1a\u8bad\u7ec3\u53ef\u4ee5\u4e2d\u65ad\uff08Ctrl+C\uff09\u5e76\u7a0d\u540e\u6062\u590d\\n')\n",
    "    \n",
    "    e2e_history = train_end_to_end_with_checkpointing(\n",
    "        model=e2e_model,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        checkpoint_interval=CHECKPOINT_INTERVAL,\n",
    "        checkpoint_dir=CHECKPOINT_DIR,\n",
    "        resume_from=None,  # Set to None for initial training | \u521d\u59cb\u8bad\u7ec3\u8bbe\u4e3a None\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print('\\n\u2705 Training completed successfully! | \u8bad\u7ec3\u6210\u529f\u5b8c\u6210\uff01')\n",
    "    \n",
    "else:\n",
    "    print('\u26a0\ufe0f  No data loaded. Please run the data loading cell first.')\n",
    "    print('\u672a\u52a0\u8f7d\u6570\u636e\u3002\u8bf7\u5148\u8fd0\u884c\u6570\u636e\u52a0\u8f7d\u5355\u5143\u683c\u3002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END-TO-END TRAINING - Resume from Checkpoint | \u7aef\u5230\u7aef\u8bad\u7ec3 - \u4ece\u68c0\u67e5\u70b9\u6062\u590d\n",
    "# Run this cell ONLY if you want to continue training from a checkpoint\n",
    "# \u4ec5\u5728\u60f3\u8981\u4ece\u68c0\u67e5\u70b9\u7ee7\u7eed\u8bad\u7ec3\u65f6\u8fd0\u884c\u6b64\u5355\u5143\u683c\n",
    "\n",
    "# Specify which checkpoint to resume from | \u6307\u5b9a\u8981\u6062\u590d\u7684\u68c0\u67e5\u70b9\n",
    "# Options | \u9009\u9879:\n",
    "#   - 'best_model.pt': Resume from best model | \u4ece\u6700\u4f73\u6a21\u578b\u6062\u590d\n",
    "#   - 'checkpoint_epoch_10.pt': Resume from epoch 10 | \u4ece\u7b2c 10 \u8f6e\u6062\u590d\n",
    "#   - 'final_model.pt': Resume from final | \u4ece\u6700\u7ec8\u6a21\u578b\u6062\u590d\n",
    "\n",
    "RESUME_CHECKPOINT = os.path.join(CHECKPOINT_DIR, 'best_model.pt')  # Change this | \u66f4\u6539\u6b64\u5904\n",
    "ADDITIONAL_EPOCHS = 20  # How many more epochs to train | \u518d\u8bad\u7ec3\u591a\u5c11\u8f6e\n",
    "\n",
    "if os.path.exists(RESUME_CHECKPOINT):\n",
    "    print('='*70)\n",
    "    print('RESUMING End-to-End Training | \u6062\u590d\u7aef\u5230\u7aef\u8bad\u7ec3')\n",
    "    print('='*70)\n",
    "    \n",
    "    # Load checkpoint to check current epoch\n",
    "    checkpoint = torch.load(RESUME_CHECKPOINT, map_location=device)\n",
    "    current_epoch = checkpoint['epoch']\n",
    "    best_val_acc = checkpoint.get('best_val_acc', 0.0)\n",
    "    \n",
    "    print(f'\\n\ud83d\udcc2 Resuming from: {RESUME_CHECKPOINT}')\n",
    "    print(f'   Last completed epoch: {current_epoch}')\n",
    "    print(f'   Best val accuracy so far: {best_val_acc:.4f}')\n",
    "    print(f'   Will train for {ADDITIONAL_EPOCHS} more epochs')\n",
    "    \n",
    "    # Reinitialize model (architecture must match!)\n",
    "    n_classes = len(np.unique(y))\n",
    "    e2e_model_resumed = sEMGHHTEndToEndClassifier(\n",
    "        n_classes=n_classes,\n",
    "        in_channels=1,\n",
    "        base_channels=64,\n",
    "        num_encoder_layers=3,\n",
    "        dropout_rate=0.5\n",
    "    )\n",
    "    \n",
    "    # Resume training\n",
    "    print('\\n\ud83d\ude80 Resuming training... | \u6062\u590d\u8bad\u7ec3...\\n')\n",
    "    \n",
    "    e2e_history_resumed = train_end_to_end_with_checkpointing(\n",
    "        model=e2e_model_resumed,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        epochs=current_epoch + 1 + ADDITIONAL_EPOCHS,  # Total epochs\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        checkpoint_interval=CHECKPOINT_INTERVAL,\n",
    "        checkpoint_dir=CHECKPOINT_DIR,\n",
    "        resume_from=RESUME_CHECKPOINT,  # Resume from checkpoint\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print('\\n\u2705 Resumed training completed! | \u6062\u590d\u8bad\u7ec3\u5b8c\u6210\uff01')\n",
    "    e2e_history = e2e_history_resumed  # Update history variable\n",
    "    \n",
    "else:\n",
    "    print(f'\u26a0\ufe0f  Checkpoint not found: {RESUME_CHECKPOINT}')\n",
    "    print('Please train the model first or check the checkpoint path.')\n",
    "    print('\u8bf7\u5148\u8bad\u7ec3\u6a21\u578b\u6216\u68c0\u67e5\u68c0\u67e5\u70b9\u8def\u5f84\u3002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training History | \u53ef\u89c6\u5316\u8bad\u7ec3\u5386\u53f2\n",
    "\n",
    "if 'e2e_history' in locals():\n",
    "    print('\ud83d\udcca Plotting training history... | \u7ed8\u5236\u8bad\u7ec3\u5386\u53f2...\\n')\n",
    "    plot_training_history(e2e_history)\n",
    "else:\n",
    "    print('\u26a0\ufe0f  No training history found. Train the model first.')\n",
    "    print('\u672a\u627e\u5230\u8bad\u7ec3\u5386\u53f2\u3002\u8bf7\u5148\u8bad\u7ec3\u6a21\u578b\u3002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load and Evaluate Best Model | \u52a0\u8f7d\u548c\u8bc4\u4f30\u6700\u4f73\u6a21\u578b\n\nbest_model_path = os.path.join(CHECKPOINT_DIR, 'best_model.pt')\n\nif os.path.exists(best_model_path) and 'X_val' in locals() and 'y_val' in locals():\n    print('='*70)\n    print('Loading and Evaluating Best Model | \u52a0\u8f7d\u548c\u8bc4\u4f30\u6700\u4f73\u6a21\u578b')\n    print('='*70)\n    \n    # Load best model\n    n_classes = len(np.unique(y))\n    best_model = sEMGHHTEndToEndClassifier(\n        n_classes=n_classes,\n        in_channels=1,\n        base_channels=64,\n        num_encoder_layers=3,\n        dropout_rate=0.5\n    )\n    \n    checkpoint = torch.load(best_model_path, map_location=device)\n    best_model.load_state_dict(checkpoint['model_state_dict'])\n    best_model.to(device)\n    best_model.eval()\n    \n    print(f'\\n\u2705 Loaded best model from epoch {checkpoint[\"epoch\"] + 1}')\n    print(f'   Best validation accuracy: {checkpoint[\"best_val_acc\"]:.4f}')\n    \n    # Evaluate on validation set\n    X_val_tensor = torch.tensor(X_val[:, np.newaxis, :, :], dtype=torch.float32).to(device)\n    y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n    \n    with torch.no_grad():\n        outputs = best_model(X_val_tensor)\n        _, predictions = outputs.max(1)\n        accuracy = (predictions == y_val_tensor).float().mean().item()\n    \n    print(f'\\n\ud83d\udcca Validation Set Performance | \u9a8c\u8bc1\u96c6\u6027\u80fd:')\n    print(f'   Accuracy | \u51c6\u786e\u7387: {accuracy:.4f}')\n    \n    # Classification report\n    from sklearn.metrics import classification_report\n    y_pred = predictions.cpu().numpy()\n    print(f'\\n\ud83d\udccb Classification Report | \u5206\u7c7b\u62a5\u544a:')\n    if 'label_encoder' in locals():\n        target_names = label_encoder.classes_\n        print(classification_report(y_val, y_pred, target_names=target_names))\n    else:\n        print(classification_report(y_val, y_pred))\n    \nelse:\n    if not os.path.exists(best_model_path):\n        print(f'\u26a0\ufe0f  Best model not found at: {best_model_path}')\n        print('Please train the model first.')\n    else:\n        print('\u26a0\ufe0f  Validation data not found. Please load data first.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test files | \u5bf9\u6d4b\u8bd5\u6587\u4ef6\u8fdb\u884c\u63a8\u7406\n",
    "if 'classifier' in locals() and len(test_files) > 0:\n",
    "    print('\\n' + '='*60)\n",
    "    print('Running Inference on Test Files | \u5bf9\u6d4b\u8bd5\u6587\u4ef6\u8fdb\u884c\u63a8\u7406')\n",
    "    print('='*60)\n",
    "    \n",
    "    X_test_list = []\n",
    "    valid_test_files = []\n",
    "    \n",
    "    for test_file in test_files[:10]:  # Limit to first 10 for demo\n",
    "        try:\n",
    "            data = np.load(test_file)\n",
    "            if 'hht' in data:\n",
    "                hht_matrix = data['hht']\n",
    "            else:\n",
    "                hht_matrix = data[list(data.keys())[0]]\n",
    "            \n",
    "            if hht_matrix.shape == (256, 256):\n",
    "                X_test_list.append(hht_matrix)\n",
    "                valid_test_files.append(test_file)\n",
    "        except Exception as e:\n",
    "            print(f'Error loading {test_file}: {e}')\n",
    "    \n",
    "    if len(X_test_list) > 0:\n",
    "        X_test = np.array(X_test_list, dtype=np.float32)\n",
    "        y_test_pred = classifier.predict(X_test, batch_size=32)\n",
    "        y_test_proba = classifier.predict_proba(X_test, batch_size=32)\n",
    "        \n",
    "        svm_classes = classifier.svm.classes_\n",
    "        \n",
    "        print(f'\\nPredictions for {len(valid_test_files)} test files:')\n",
    "        print(f'\u524d {len(valid_test_files)} \u4e2a\u6d4b\u8bd5\u6587\u4ef6\u7684\u9884\u6d4b\u7ed3\u679c\uff1a\\n')\n",
    "        for i, (filename, pred, proba) in enumerate(zip(valid_test_files, y_test_pred, y_test_proba)):\n",
    "            pred_idx = np.where(svm_classes == pred)[0][0]\n",
    "            class_name = label_encoder.classes_[pred]\n",
    "            confidence = proba[pred_idx]\n",
    "            print(f'{os.path.basename(filename)}: {class_name} (confidence: {confidence:.4f})')\n",
    "else:\n",
    "    print('No test files to process or classifier not trained.')\n",
    "    print('\u6ca1\u6709\u6d4b\u8bd5\u6587\u4ef6\u6216\u5206\u7c7b\u5668\u672a\u8bad\u7ec3\u3002')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "1. **CNN Encoder Architecture**: 3-layer convolutional network with Instance Normalization and LeakyReLU\n",
    "2. **SVM Classifier**: Multi-class classification using extracted CNN features\n",
    "3. **End-to-End Model**: Optional fully trainable model with neural network classifier\n",
    "4. **Real Data Training**: Load and train on actual HHT matrices from Kaggle dataset\n",
    "5. **HHT Computation**: Reference implementation for real sEMG signals\n",
    "\n",
    "### Hyperparameter Tuning:\n",
    "- `base_channels`: Number of channels in first conv layer (default: 64)\n",
    "- `svm_C`: SVM regularization parameter\n",
    "- `svm_kernel`: SVM kernel type ('rbf', 'linear', 'poly')\n",
    "- `learning_rate`: Learning rate for end-to-end training\n",
    "- `dropout_rate`: Dropout rate in end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"sEMG-HHT CNN Classifier - Ready for Use\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDevice: {device}\")\n",
    "print(f\"Encoder feature dimension: {encoder.get_feature_dim()}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"\\nClass names:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}