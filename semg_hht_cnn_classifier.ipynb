{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sEMG-HHT CNN Classifier for Movement Quality and Fatigue Classification\n",
    "\n",
    "This notebook implements a Convolutional Neural Network (CNN) encoder with SVM classifier for classifying surface electromyography (sEMG) signals based on movement quality and fatigue levels.\n",
    "\n",
    "## Architecture Overview\n",
    "- **Input**: 256Ã—256 matrix from HHT (Hilbert-Huang Transform) of sEMG signals\n",
    "- **Encoder**: 3-layer CNN with Conv2D + InstanceNorm + LeakyReLU\n",
    "- **Pooling**: Global Average Pooling\n",
    "- **Classifier**: SVM for multi-class classification\n",
    "\n",
    "## Requirements\n",
    "- PyTorch\n",
    "- scikit-learn\n",
    "- NumPy\n",
    "- Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Integration | Kaggle é›†æˆ\n\n",
    "This notebook is configured to work with the **HILBERTMATRIX_NPZ** dataset on Kaggle.\n\n",
    "æœ¬ç¬”è®°æœ¬é…ç½®ä¸ºä½¿ç”¨ Kaggle ä¸Šçš„ **HILBERTMATRIX_NPZ** æ•°æ®é›†ã€‚\n\n",
    "Data path: `/kaggle/input/hilbertmatrix-npz/hht_matrices/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sEMG-HHT CNN Classifier - Kaggle Training\n",
    "# sEMG-HHT CNN åˆ†ç±»å™¨ - Kaggle è®­ç»ƒ\n",
    "\n",
    "This notebook trains a CNN-SVM classifier on sEMG Hilbert-Huang Transform data for 6-class classification:\n",
    "æœ¬ç¬”è®°æœ¬è®­ç»ƒä¸€ä¸ª CNN-SVM åˆ†ç±»å™¨ï¼Œç”¨äº sEMG å¸Œå°”ä¼¯ç‰¹-é»„å˜æ¢æ•°æ®çš„ 6 ç±»åˆ†ç±»ï¼š\n",
    "\n",
    "- **Gender (æ€§åˆ«)**: Male (ç”·æ€§, M), Female (å¥³æ€§, F)\n",
    "- **Movement Quality (åŠ¨ä½œè´¨é‡)**: Full (å®Œæ•´), Half (åŠç¨‹), Invalid (æ— æ•ˆ)\n",
    "\n",
    "## Dataset Integration | æ•°æ®é›†é›†æˆ\n",
    "\n",
    "This notebook uses the **HILBERTMATRIX_NPZ** dataset from Kaggle.\n",
    "æœ¬ç¬”è®°æœ¬ä½¿ç”¨ Kaggle ä¸Šçš„ **HILBERTMATRIX_NPZ** æ•°æ®é›†ã€‚\n",
    "\n",
    "Data location: `/kaggle/input/hilbertmatrix-npz/hht_matrices/`\n",
    "æ•°æ®ä½ç½®ï¼š`/kaggle/input/hilbertmatrix-npz/hht_matrices/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data paths for Kaggle\n",
    "# é…ç½® Kaggle æ•°æ®è·¯å¾„\n",
    "import os\n",
    "\n",
    "# Check if running on Kaggle\n",
    "# æ£€æŸ¥æ˜¯å¦åœ¨ Kaggle ä¸Šè¿è¡Œ\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    DATA_DIR = '/kaggle/input/hilbertmatrix-npz/hht_matrices'\n",
    "    CHECKPOINT_DIR = '/kaggle/working/checkpoints'\n",
    "    print(f'Running on Kaggle | åœ¨ Kaggle ä¸Šè¿è¡Œ')\n",
    "    print(f'Data directory | æ•°æ®ç›®å½•: {DATA_DIR}')\n",
    "else:\n",
    "    DATA_DIR = './data'\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    print(f'Running locally | æœ¬åœ°è¿è¡Œ')\n",
    "    print(f'Data directory | æ•°æ®ç›®å½•: {DATA_DIR}')\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f'Checkpoint directory | æ£€æŸ¥ç‚¹ç›®å½•: {CHECKPOINT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if running on Kaggle or fresh environment)\n",
    "# !pip install torch torchvision scikit-learn numpy matplotlib scipy PyEMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from typing import Tuple, Optional\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings that are not critical for this demo\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CNN Encoder Architecture\n",
    "\n",
    "The encoder consists of 3 convolutional layers, each with:\n",
    "- Conv2D (kernel=3, stride=2, padding=1)\n",
    "- Instance Normalization\n",
    "- LeakyReLU activation\n",
    "\n",
    "This progressively reduces the spatial dimensions while extracting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block with Conv2D, InstanceNorm, and LeakyReLU.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, \n",
    "                 kernel_size: int = 3, stride: int = 2, padding: int = 1,\n",
    "                 leaky_slope: float = 0.2):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
    "                              kernel_size=kernel_size, \n",
    "                              stride=stride, \n",
    "                              padding=padding)\n",
    "        self.instance_norm = nn.InstanceNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_slope)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.instance_norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class sEMGHHTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN Encoder for sEMG-HHT matrix classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: 1Ã—256Ã—256 (single-channel HHT matrix)\n",
    "    - 3 ConvBlocks with increasing channels\n",
    "    - Global Average Pooling\n",
    "    - Output: Feature vector for SVM classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 1, \n",
    "                 base_channels: int = 64,\n",
    "                 num_layers: int = 3,\n",
    "                 leaky_slope: float = 0.2):\n",
    "        super(sEMGHHTEncoder, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Build convolutional layers\n",
    "        layers = []\n",
    "        current_channels = in_channels\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            out_channels = base_channels * (2 ** i)\n",
    "            layers.append(ConvBlock(\n",
    "                in_channels=current_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                leaky_slope=leaky_slope\n",
    "            ))\n",
    "            current_channels = out_channels\n",
    "        \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Calculate output feature dimension\n",
    "        self.feature_dim = base_channels * (2 ** (num_layers - 1))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, channels, height, width)\n",
    "        \n",
    "        Returns:\n",
    "            Feature tensor of shape (batch, feature_dim)\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch, feature_dim)\n",
    "        return x\n",
    "    \n",
    "    def get_feature_dim(self) -> int:\n",
    "        \"\"\"Return the output feature dimension.\"\"\"\n",
    "        return self.feature_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete Classification Pipeline\n",
    "\n",
    "This class combines the CNN encoder with an SVM classifier for end-to-end classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sEMGHHTClassifier:\n",
    "    \"\"\"\n",
    "    Complete classification pipeline combining CNN encoder and SVM classifier.\n",
    "    \n",
    "    The pipeline:\n",
    "    1. Extracts features using CNN encoder\n",
    "    2. Normalizes features using StandardScaler\n",
    "    3. Classifies using SVM (supports multi-class)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 encoder: Optional[sEMGHHTEncoder] = None,\n",
    "                 svm_kernel: str = 'rbf',\n",
    "                 svm_C: float = 1.0,\n",
    "                 svm_gamma: str = 'scale',\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Args:\n",
    "            encoder: Pre-trained or new CNN encoder (creates default if None)\n",
    "            svm_kernel: SVM kernel type ('rbf', 'linear', 'poly')\n",
    "            svm_C: SVM regularization parameter\n",
    "            svm_gamma: SVM gamma parameter\n",
    "            device: Device to run the encoder on\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize encoder\n",
    "        if encoder is None:\n",
    "            self.encoder = sEMGHHTEncoder(\n",
    "                in_channels=1, \n",
    "                base_channels=64, \n",
    "                num_layers=3\n",
    "            )\n",
    "        else:\n",
    "            self.encoder = encoder\n",
    "        \n",
    "        self.encoder.to(self.device)\n",
    "        \n",
    "        # Initialize scaler and SVM\n",
    "        self.scaler = StandardScaler()\n",
    "        self.svm = SVC(\n",
    "            kernel=svm_kernel,\n",
    "            C=svm_C,\n",
    "            gamma=svm_gamma,\n",
    "            decision_function_shape='ovr',  # One-vs-Rest for multi-class\n",
    "            probability=True  # Enable probability estimates\n",
    "        )\n",
    "        \n",
    "        self._is_fitted = False\n",
    "    \n",
    "    def extract_features(self, X: np.ndarray, batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract features from HHT matrices using the CNN encoder.\n",
    "        \n",
    "        Args:\n",
    "            X: Input array of shape (n_samples, height, width) or (n_samples, 1, height, width)\n",
    "            batch_size: Batch size for processing\n",
    "        \n",
    "        Returns:\n",
    "            Feature array of shape (n_samples, feature_dim)\n",
    "        \"\"\"\n",
    "        self.encoder.eval()\n",
    "        \n",
    "        # Ensure correct shape\n",
    "        if X.ndim == 3:\n",
    "            X = X[:, np.newaxis, :, :]  # Add channel dimension\n",
    "        \n",
    "        features_list = []\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch = torch.tensor(X[i:i+batch_size], dtype=torch.float32).to(self.device)\n",
    "                batch_features = self.encoder(batch)\n",
    "                features_list.append(batch_features.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(features_list)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Fit the classifier (extract features and train SVM).\n",
    "        \n",
    "        Args:\n",
    "            X: Training HHT matrices of shape (n_samples, height, width)\n",
    "            y: Training labels of shape (n_samples,)\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \"\"\"\n",
    "        print(\"Extracting features from training data...\")\n",
    "        features = self.extract_features(X, batch_size)\n",
    "        \n",
    "        print(\"Normalizing features...\")\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        \n",
    "        print(\"Training SVM classifier...\")\n",
    "        self.svm.fit(features_scaled, y)\n",
    "        \n",
    "        self._is_fitted = True\n",
    "        print(\"Training complete!\")\n",
    "    \n",
    "    def predict(self, X: np.ndarray, batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class labels for samples.\n",
    "        \n",
    "        Args:\n",
    "            X: Input HHT matrices of shape (n_samples, height, width)\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \n",
    "        Returns:\n",
    "            Predicted labels of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        if not self._is_fitted:\n",
    "            raise RuntimeError(\"Classifier must be fitted before predicting\")\n",
    "        \n",
    "        features = self.extract_features(X, batch_size)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        return self.svm.predict(features_scaled)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray, batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class probabilities for samples.\n",
    "        \n",
    "        Args:\n",
    "            X: Input HHT matrices of shape (n_samples, height, width)\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \n",
    "        Returns:\n",
    "            Probability array of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        if not self._is_fitted:\n",
    "            raise RuntimeError(\"Classifier must be fitted before predicting\")\n",
    "        \n",
    "        features = self.extract_features(X, batch_size)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        return self.svm.predict_proba(features_scaled)\n",
    "    \n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray, \n",
    "                 batch_size: int = 32) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate the classifier on test data.\n",
    "        \n",
    "        Args:\n",
    "            X: Test HHT matrices\n",
    "            y: True labels\n",
    "            batch_size: Batch size for feature extraction\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing accuracy, predictions, and classification report\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X, batch_size)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': y_pred,\n",
    "            'classification_report': classification_report(y, y_pred),\n",
    "            'confusion_matrix': confusion_matrix(y, y_pred)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "# Training Methods Overview | è®­ç»ƒæ–¹æ³•æ¦‚è¿°\n\n",
    "This notebook supports **TWO** training approaches. Choose the one that fits your needs:\n\n",
    "æœ¬ç¬”è®°æœ¬æ”¯æŒ**ä¸¤ç§**è®­ç»ƒæ–¹æ³•ã€‚æ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š\n\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "# Training Methods Comparison | è®­ç»ƒæ–¹æ³•å¯¹æ¯”\n\n",
    "This notebook supports **TWO** training methods. Choose based on your needs:\n\n",
    "æœ¬ç¬”è®°æœ¬æ”¯æŒ**ä¸¤ç§**è®­ç»ƒæ–¹æ³•ã€‚æ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š\n\n",
    "---\n\n",
    "## Method 1: CNN+SVM (Traditional) | æ–¹æ³•ä¸€ï¼šCNN+SVMï¼ˆä¼ ç»Ÿï¼‰\n\n",
    "**English**:\n",
    "- CNN encoder extracts features (weights **frozen**)\n",
    "- SVM classifier trains on extracted features\n",
    "- **Fast training**, good for small datasets\n",
    "- **No epochs** needed - one-shot training\n\n",
    "**ä¸­æ–‡**ï¼š\n",
    "- CNN ç¼–ç å™¨æå–ç‰¹å¾ï¼ˆæƒé‡**å†»ç»“**ï¼‰\n",
    "- SVM åˆ†ç±»å™¨åœ¨æå–çš„ç‰¹å¾ä¸Šè®­ç»ƒ\n",
    "- **è®­ç»ƒå¿«é€Ÿ**ï¼Œé€‚åˆå°æ•°æ®é›†\n",
    "- **æ— éœ€å¤šè½®è®­ç»ƒ** - ä¸€æ¬¡æ€§è®­ç»ƒ\n\n",
    "**Use When | ä½¿ç”¨åœºæ™¯**:\n",
    "- Limited data (< 1000 samples) | æ•°æ®æœ‰é™ï¼ˆ< 1000 æ ·æœ¬ï¼‰\n",
    "- Quick prototyping | å¿«é€ŸåŸå‹\n",
    "- Stable baseline needed | éœ€è¦ç¨³å®šåŸºçº¿\n\n",
    "---\n\n",
    "## Method 2: End-to-End with Encoder Fine-tuning | æ–¹æ³•äºŒï¼šç«¯åˆ°ç«¯ç¼–ç å™¨å¾®è°ƒ\n\n",
    "**English**:\n",
    "- **Entire network** trains together (encoder + classifier)\n",
    "- Encoder **adapts** to your specific data\n",
    "- **Multi-epoch** training with checkpointing\n",
    "- Can resume from interruption\n",
    "- Real-time progress monitoring (accuracy/loss)\n\n",
    "**ä¸­æ–‡**ï¼š\n",
    "- **æ•´ä¸ªç½‘ç»œ**ä¸€èµ·è®­ç»ƒï¼ˆç¼–ç å™¨ + åˆ†ç±»å™¨ï¼‰\n",
    "- ç¼–ç å™¨**é€‚åº”**ä½ çš„ç‰¹å®šæ•°æ®\n",
    "- **å¤šè½®**è®­ç»ƒä¸æ£€æŸ¥ç‚¹ä¿å­˜\n",
    "- å¯ä»ä¸­æ–­å¤„æ¢å¤\n",
    "- å®æ—¶è¿›åº¦ç›‘æ§ï¼ˆå‡†ç¡®ç‡/æŸå¤±ï¼‰\n\n",
    "**Use When | ä½¿ç”¨åœºæ™¯**:\n",
    "- Large dataset (> 1000 samples) | å¤§æ•°æ®é›†ï¼ˆ> 1000 æ ·æœ¬ï¼‰\n",
    "- Maximum accuracy needed | éœ€è¦æœ€å¤§å‡†ç¡®ç‡\n",
    "- Domain-specific data | é¢†åŸŸç‰¹å®šæ•°æ®\n\n",
    "---\n\n",
    "**ğŸ“ Scroll down to find sections for each method:**\n\n",
    "**ğŸ“ å‘ä¸‹æ»šåŠ¨æ‰¾åˆ°æ¯ç§æ–¹æ³•çš„å¯¹åº”ç« èŠ‚ï¼š**\n\n",
    "- **Section 5**: CNN+SVM Training | ç¬¬ 5 èŠ‚ï¼šCNN+SVM è®­ç»ƒ\n",
    "- **Section 6**: End-to-End Training | ç¬¬ 6 èŠ‚ï¼šç«¯åˆ°ç«¯è®­ç»ƒ\n\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_end_to_end_with_checkpointing(\n    model: nn.Module,\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_val: np.ndarray,\n    y_val: np.ndarray,\n    epochs: int = 50,\n    batch_size: int = 16,\n    learning_rate: float = 0.001,\n    checkpoint_interval: int = 5,\n    checkpoint_dir: str = None,\n    resume_from: str = None,\n    device: torch.device = None\n) -> dict:\n    \"\"\"\n    Train end-to-end model with comprehensive checkpointing support.\n    ç«¯åˆ°ç«¯è®­ç»ƒæ¨¡å‹ï¼Œæ”¯æŒå®Œæ•´çš„æ£€æŸ¥ç‚¹åŠŸèƒ½ã€‚\n    \n    Features | åŠŸèƒ½:\n    - Multi-epoch training | å¤šè½®è®­ç»ƒ\n    - Checkpoint saving every N epochs | æ¯ N è½®ä¿å­˜æ£€æŸ¥ç‚¹\n    - Resume from interruption | ä¸­æ–­åæ¢å¤\n    - Best model auto-save | è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹\n    - Real-time progress display | å®æ—¶è¿›åº¦æ˜¾ç¤º\n    \n    Args:\n        model: Model to train | è¦è®­ç»ƒçš„æ¨¡å‹\n        X_train, y_train: Training data | è®­ç»ƒæ•°æ®\n        X_val, y_val: Validation data | éªŒè¯æ•°æ®\n        epochs: Number of epochs | è®­ç»ƒè½®æ•°\n        batch_size: Batch size | æ‰¹æ¬¡å¤§å°\n        learning_rate: Learning rate | å­¦ä¹ ç‡\n        checkpoint_interval: Save checkpoint every N epochs | æ¯ N è½®ä¿å­˜æ£€æŸ¥ç‚¹\n        checkpoint_dir: Where to save checkpoints | æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•\n        resume_from: Path to checkpoint to resume from | è¦æ¢å¤çš„æ£€æŸ¥ç‚¹è·¯å¾„\n        device: Device to use | ä½¿ç”¨çš„è®¾å¤‡\n    \n    Returns:\n        Dictionary with training history | åŒ…å«è®­ç»ƒå†å²çš„å­—å…¸\n    \"\"\"\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    if checkpoint_dir is None:\n        checkpoint_dir = CHECKPOINT_DIR\n    \n    os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    model = model.to(device)\n    \n    # Ensure correct shape\n    if X_train.ndim == 3:\n        X_train = X_train[:, np.newaxis, :, :]\n        X_val = X_val[:, np.newaxis, :, :]\n    \n    # Create data loaders\n    train_dataset = torch.utils.data.TensorDataset(\n        torch.tensor(X_train, dtype=torch.float32),\n        torch.tensor(y_train, dtype=torch.long)\n    )\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True\n    )\n    \n    val_dataset = torch.utils.data.TensorDataset(\n        torch.tensor(X_val, dtype=torch.float32),\n        torch.tensor(y_val, dtype=torch.long)\n    )\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False\n    )\n    \n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=5, verbose=True\n    )\n    \n    # Initialize history\n    history = {\n        'train_loss': [],\n        'train_acc': [],\n        'val_loss': [],\n        'val_acc': [],\n        'learning_rate': []\n    }\n    \n    start_epoch = 0\n    best_path = os.path.join(checkpoint_dir, 'best_model.pt')\n    best_val_acc = 0.0\n    \n    # Resume from checkpoint if specified\n    if resume_from and os.path.exists(resume_from):\n        print(f'\\nğŸ“‚ Resuming from checkpoint: {resume_from}')\n        checkpoint = torch.load(resume_from, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        best_val_acc = checkpoint.get('best_val_acc', 0.0)\n        history = checkpoint.get('history', history)\n        print(f'âœ… Resumed from epoch {start_epoch}, best val acc: {best_val_acc:.4f}')\n    \n    print(f'\\nğŸš€ Training from epoch {start_epoch} to {epochs}...')\n    print(f'Device: {device}')\n    print(f'Train samples: {len(X_train)}, Val samples: {len(X_val)}')\n    print(f'Checkpoint interval: every {checkpoint_interval} epochs')\n    print('='*70)\n    \n    for epoch in range(start_epoch, epochs):\n        # Training phase\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * batch_X.size(0)\n            _, predicted = outputs.max(1)\n            train_total += batch_y.size(0)\n            train_correct += predicted.eq(batch_y).sum().item()\n        \n        train_loss /= train_total\n        train_acc = train_correct / train_total\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n                \n                outputs = model(batch_X)\n                loss = criterion(outputs, batch_y)\n                \n                val_loss += loss.item() * batch_X.size(0)\n                _, predicted = outputs.max(1)\n                val_total += batch_y.size(0)\n                val_correct += predicted.eq(batch_y).sum().item()\n        \n        val_loss /= val_total\n        val_acc = val_correct / val_total\n        \n        # Update learning rate\n        scheduler.step(val_acc)\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # Save history\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        history['learning_rate'].append(current_lr)\n        \n        # Print progress\n        print(f\"Epoch [{epoch+1:3d}/{epochs}] | \"\n              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n              f\"LR: {current_lr:.6f}\")\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'best_val_acc': best_val_acc,\n                'history': history\n            }, best_path)\n            print(f\"  â­ New best model! Val Acc: {val_acc:.4f} (saved to {best_path})\")\n        \n        # Save checkpoint at interval\n        if (epoch + 1) % checkpoint_interval == 0:\n            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt')\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'best_val_acc': best_val_acc,\n                'history': history\n            }, checkpoint_path)\n            print(f\"  ğŸ’¾ Checkpoint saved: {checkpoint_path}\")\n    \n    # Save final checkpoint\n    final_path = os.path.join(checkpoint_dir, 'final_model.pt')\n    torch.save({\n        'epoch': epochs - 1,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'best_val_acc': best_val_acc,\n        'history': history\n    }, final_path)\n    \n    print('='*70)\n    print(f'\\nâœ… Training Complete!')\n    print(f'Best Validation Accuracy: {best_val_acc:.4f}')\n    print(f'Final model saved to: {final_path}')\n    print(f'Best model saved to: {best_path}')\n    \n    return history"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history: dict):\n",
    "    \"\"\"\n",
    "    Plot training history (loss and accuracy).\n",
    "    ç»˜åˆ¶è®­ç»ƒå†å²ï¼ˆæŸå¤±å’Œå‡†ç¡®ç‡ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss | è®­ç»ƒæŸå¤±', linewidth=2)\n",
    "    axes[0].plot(epochs, history['val_loss'], 'r-', label='Val Loss | éªŒè¯æŸå¤±', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch | è½®æ¬¡', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss | æŸå¤±', fontsize=12)\n",
    "    axes[0].set_title('Training and Validation Loss | è®­ç»ƒå’ŒéªŒè¯æŸå¤±', fontsize=14)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[1].plot(epochs, history['train_acc'], 'b-', label='Train Acc | è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)\n",
    "    axes[1].plot(epochs, history['val_acc'], 'r-', label='Val Acc | éªŒè¯å‡†ç¡®ç‡', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch | è½®æ¬¡', fontsize=12)\n",
    "    axes[1].set_ylabel('Accuracy | å‡†ç¡®ç‡', fontsize=12)\n",
    "    axes[1].set_title('Training and Validation Accuracy | è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡', fontsize=14)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot learning rate\n",
    "    if 'learning_rate' in history:\n",
    "        axes[2].plot(epochs, history['learning_rate'], 'g-', linewidth=2)\n",
    "        axes[2].set_xlabel('Epoch | è½®æ¬¡', fontsize=12)\n",
    "        axes[2].set_ylabel('Learning Rate | å­¦ä¹ ç‡', fontsize=12)\n",
    "        axes[2].set_title('Learning Rate Schedule | å­¦ä¹ ç‡è°ƒåº¦', fontsize=14)\n",
    "        axes[2].set_yscale('log')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(f'\\nğŸ“Š Training Statistics | è®­ç»ƒç»Ÿè®¡:')\n",
    "    print(f'   Final Train Acc | æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {history[\"train_acc\"][-1]:.4f}')\n",
    "    print(f'   Final Val Acc | æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {history[\"val_acc\"][-1]:.4f}')\n",
    "    print(f'   Best Val Acc | æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(history[\"val_acc\"]):.4f}')\n",
    "    print(f'   Final Train Loss | æœ€ç»ˆè®­ç»ƒæŸå¤±: {history[\"train_loss\"][-1]:.4f}')\n",
    "    print(f'   Final Val Loss | æœ€ç»ˆéªŒè¯æŸå¤±: {history[\"val_loss\"][-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class sEMGHHTEndToEndClassifier(nn.Module):\n    \"\"\"\n    End-to-end trainable classifier with CNN encoder and linear classification head.\n    \n    This version allows the encoder to be fine-tuned during training, enabling\n    the model to adapt to your specific data distribution.\n    \n    Architecture:\n        Input (1Ã—256Ã—256) \n        â†’ CNN Encoder (3 ConvBlocks) \n        â†’ Features (256-dim)\n        â†’ Dropout â†’ FC(256â†’128) â†’ ReLU â†’ Dropout â†’ FC(128â†’n_classes)\n        â†’ Logits (n_classes)\n    \n    Args:\n        n_classes (int): Number of output classes (default: 4)\n            For 6-class problem (M_full, M_half, M_invalid, F_full, F_half, F_invalid), use n_classes=6\n        in_channels (int): Number of input channels (default: 1 for grayscale HHT matrix)\n        base_channels (int): Base number of channels in first conv layer (default: 64)\n            Channel progression: 64 â†’ 128 â†’ 256\n        num_encoder_layers (int): Number of convolutional blocks in encoder (default: 3)\n            Must match the encoder architecture you want to use\n        dropout_rate (float): Dropout probability for regularization (default: 0.5)\n            Range: 0.0 (no dropout) to 0.9 (high dropout)\n            Typical values: 0.3-0.6\n    \n    Example:\n        >>> model = sEMGHHTEndToEndClassifier(\n        ...     n_classes=6,\n        ...     base_channels=64,\n        ...     num_encoder_layers=3,\n        ...     dropout_rate=0.5\n        ... )\n        >>> x = torch.randn(16, 1, 256, 256)  # Batch of 16 HHT matrices\n        >>> logits = model(x)  # Output shape: (16, 6)\n    \"\"\"\n    \n    def __init__(self, \n                 n_classes: int = 4,\n                 in_channels: int = 1,\n                 base_channels: int = 64,\n                 num_encoder_layers: int = 3,\n                 dropout_rate: float = 0.5):\n        super(sEMGHHTEndToEndClassifier, self).__init__()\n        \n        self.encoder = sEMGHHTEncoder(\n            in_channels=in_channels,\n            base_channels=base_channels,\n            num_layers=num_encoder_layers\n        )\n        \n        feature_dim = self.encoder.get_feature_dim()\n        \n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout_rate),\n            nn.Linear(feature_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(128, n_classes)\n        )\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        features = self.encoder(x)\n        logits = self.classifier(features)\n        return logits\n    \n    def get_features(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Extract features without classification.\"\"\"\n        return self.encoder(x)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_end_to_end(model: nn.Module,\n",
    "                     X_train: np.ndarray,\n",
    "                     y_train: np.ndarray,\n",
    "                     X_val: np.ndarray,\n",
    "                     y_val: np.ndarray,\n",
    "                     epochs: int = 50,\n",
    "                     batch_size: int = 16,\n",
    "                     learning_rate: float = 0.001,\n",
    "                     device: torch.device = torch.device('cpu')) -> dict:\n",
    "    \"\"\"\n",
    "    Train the end-to-end model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        X_train, y_train: Training data and labels\n",
    "        X_val, y_val: Validation data and labels\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        learning_rate: Learning rate\n",
    "        device: Device to train on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing training history\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Ensure correct shape\n",
    "    if X_train.ndim == 3:\n",
    "        X_train = X_train[:, np.newaxis, :, :]\n",
    "        X_val = X_val[:, np.newaxis, :, :]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.long)\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * batch_X.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += batch_y.size(0)\n",
    "            train_correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                val_loss += loss.item() * batch_X.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += batch_y.size(0)\n",
    "                val_correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_svm_classifier(classifier: sEMGHHTClassifier, path: str):\n",
    "    \"\"\"Save the SVM-based classifier to disk.\"\"\"\n",
    "    # Save encoder\n",
    "    torch.save(classifier.encoder.state_dict(), f\"{path}_encoder.pt\")\n",
    "    \n",
    "    # Save scaler and SVM\n",
    "    with open(f\"{path}_scaler.pkl\", 'wb') as f:\n",
    "        pickle.dump(classifier.scaler, f)\n",
    "    \n",
    "    with open(f\"{path}_svm.pkl\", 'wb') as f:\n",
    "        pickle.dump(classifier.svm, f)\n",
    "    \n",
    "    print(f\"Classifier saved to {path}_*.pt/pkl\")\n",
    "\n",
    "def load_svm_classifier(path: str, device: torch.device = torch.device('cpu')) -> sEMGHHTClassifier:\n",
    "    \"\"\"Load a saved SVM-based classifier.\"\"\"\n",
    "    classifier = sEMGHHTClassifier(device=device)\n",
    "    \n",
    "    # Load encoder\n",
    "    classifier.encoder.load_state_dict(torch.load(f\"{path}_encoder.pt\", map_location=device))\n",
    "    \n",
    "    # Load scaler and SVM\n",
    "    with open(f\"{path}_scaler.pkl\", 'rb') as f:\n",
    "        classifier.scaler = pickle.load(f)\n",
    "    \n",
    "    with open(f\"{path}_svm.pkl\", 'rb') as f:\n",
    "        classifier.svm = pickle.load(f)\n",
    "    \n",
    "    classifier._is_fitted = True\n",
    "    print(f\"Classifier loaded from {path}_*.pt/pkl\")\n",
    "    return classifier\n",
    "\n",
    "def save_e2e_model(model: nn.Module, path: str):\n",
    "    \"\"\"Save the end-to-end model.\"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_e2e_model(path: str, n_classes: int = 4, \n",
    "                   device: torch.device = torch.device('cpu')) -> sEMGHHTEndToEndClassifier:\n",
    "    \"\"\"Load a saved end-to-end model.\"\"\"\n",
    "    model = sEMGHHTEndToEndClassifier(n_classes=n_classes)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data Training | çœŸå®æ•°æ®è®­ç»ƒ\n",
    "\n",
    "Train the classifier on real sEMG HHT data from the Kaggle dataset.\n",
    "åœ¨ Kaggle æ•°æ®é›†çš„çœŸå® sEMG HHT æ•°æ®ä¸Šè®­ç»ƒåˆ†ç±»å™¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "# å¯¼å…¥æ‰€éœ€æ¨¡å—\n",
    "import glob\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse filename to extract labels.\n",
    "    è§£ææ–‡ä»¶åä»¥æå–æ ‡ç­¾ã€‚\n",
    "    \n",
    "    Returns None if filename starts with 'Test' (unlabeled test data)\n",
    "    å¦‚æœæ–‡ä»¶åä»¥ 'Test' å¼€å¤´åˆ™è¿”å› Noneï¼ˆæœªæ ‡è®°çš„æµ‹è¯•æ•°æ®ï¼‰\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    # Skip files that start with 'Test'\n",
    "    # è·³è¿‡ä»¥ 'Test' å¼€å¤´çš„æ–‡ä»¶\n",
    "    if basename.lower().startswith('test'):\n",
    "        return None\n",
    "    \n",
    "    # Extract gender (M or F)\n",
    "    # æå–æ€§åˆ«ï¼ˆM æˆ– Fï¼‰\n",
    "    gender_match = re.search(r'[_-]([MF])[_-]', basename)\n",
    "    if not gender_match:\n",
    "        return None\n",
    "    gender = gender_match.group(1)\n",
    "    \n",
    "    # Extract movement quality\n",
    "    # æå–åŠ¨ä½œè´¨é‡\n",
    "    basename_lower = basename.lower()\n",
    "    if 'fatiguetest' in basename_lower or 'full' in basename_lower:\n",
    "        movement = 'full'\n",
    "    elif 'half' in basename_lower:\n",
    "        movement = 'half'\n",
    "    elif 'invalid' in basename_lower or 'wrong' in basename_lower:\n",
    "        movement = 'invalid'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return {'gender': gender, 'movement': movement}\n",
    "\n",
    "def load_data_from_directory(data_dir):\n",
    "    \"\"\"\n",
    "    Load HHT matrices from npz files.\n",
    "    ä» npz æ–‡ä»¶åŠ è½½ HHT çŸ©é˜µã€‚\n",
    "    \"\"\"\n",
    "    npz_files = glob.glob(os.path.join(data_dir, '*.npz'))\n",
    "    \n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    filenames = []\n",
    "    test_files = []\n",
    "    \n",
    "    # Create label encoder\n",
    "    # åˆ›å»ºæ ‡ç­¾ç¼–ç å™¨\n",
    "    all_classes = ['M_full', 'M_half', 'M_invalid', 'F_full', 'F_half', 'F_invalid']\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_classes)\n",
    "    \n",
    "    for npz_file in npz_files:\n",
    "        labels = parse_filename(npz_file)\n",
    "        \n",
    "        if labels is None:\n",
    "            test_files.append(npz_file)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            data = np.load(npz_file)\n",
    "            if 'hht' in data:\n",
    "                hht_matrix = data['hht']\n",
    "            else:\n",
    "                hht_matrix = data[list(data.keys())[0]]\n",
    "            \n",
    "            if hht_matrix.shape != (256, 256):\n",
    "                continue\n",
    "            \n",
    "            # Create combined label\n",
    "            # åˆ›å»ºç»„åˆæ ‡ç­¾\n",
    "            combined = f\"{labels['gender']}_{labels['movement']}\"\n",
    "            label = label_encoder.transform([combined])[0]\n",
    "            \n",
    "            X_list.append(hht_matrix)\n",
    "            y_list.append(label)\n",
    "            filenames.append(npz_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error loading {npz_file}: {e}')\n",
    "            continue\n",
    "    \n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    print(f'\\nLoaded {len(X)} training samples | åŠ è½½äº† {len(X)} ä¸ªè®­ç»ƒæ ·æœ¬')\n",
    "    print(f'Found {len(test_files)} test files | æ‰¾åˆ° {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶')\n",
    "    print(f'\\nClass distribution | ç±»åˆ«åˆ†å¸ƒ:')\n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        count = np.sum(y == i)\n",
    "        print(f'  {class_name}: {count} samples')\n",
    "    \n",
    "    return X, y, filenames, test_files, label_encoder\n",
    "\n",
    "# Load data from Kaggle dataset\n",
    "# ä» Kaggle æ•°æ®é›†åŠ è½½æ•°æ®\n",
    "if os.path.exists(DATA_DIR):\n",
    "    X, y, filenames, test_files, label_encoder = load_data_from_directory(DATA_DIR)\n",
    "else:\n",
    "    print(f'Data directory not found | æ•°æ®ç›®å½•æœªæ‰¾åˆ°: {DATA_DIR}')\n",
    "    print('Please ensure the HILBERTMATRIX_NPZ dataset is added to this notebook.')\n",
    "    print('è¯·ç¡®ä¿ HILBERTMATRIX_NPZ æ•°æ®é›†å·²æ·»åŠ åˆ°æ­¤ç¬”è®°æœ¬ã€‚')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real HHT Transformation (For Reference)\n",
    "\n",
    "When you have real sEMG data, you can use the following functions to perform HHT transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier | è®­ç»ƒåˆ†ç±»å™¨\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if 'X' in locals() and len(X) > 0:\n",
    "    # Split data | åˆ†å‰²æ•°æ®\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f'Training set | è®­ç»ƒé›†: {X_train.shape[0]} samples')\n",
    "    print(f'Validation set | éªŒè¯é›†: {X_val.shape[0]} samples')\n",
    "    \n",
    "    # Initialize classifier | åˆå§‹åŒ–åˆ†ç±»å™¨\n",
    "    classifier = sEMGHHTClassifier(\n",
    "        encoder=None,\n",
    "        svm_kernel='rbf',\n",
    "        svm_C=10.0,\n",
    "        svm_gamma='scale',\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Train | è®­ç»ƒ\n",
    "    print('\\n' + '='*60)\n",
    "    print('Training SVM Classifier | è®­ç»ƒ SVM åˆ†ç±»å™¨')\n",
    "    print('='*60)\n",
    "    classifier.fit(X_train, y_train, batch_size=32)\n",
    "    \n",
    "    # Evaluate on training set | åœ¨è®­ç»ƒé›†ä¸Šè¯„ä¼°\n",
    "    train_results = classifier.evaluate(X_train, y_train, batch_size=32)\n",
    "    print(f'\\nTraining Accuracy | è®­ç»ƒå‡†ç¡®ç‡: {train_results[\"accuracy\"]:.4f}')\n",
    "    \n",
    "    # Evaluate on validation set | åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°\n",
    "    val_results = classifier.evaluate(X_val, y_val, batch_size=32)\n",
    "    print(f'Validation Accuracy | éªŒè¯å‡†ç¡®ç‡: {val_results[\"accuracy\"]:.4f}')\n",
    "    print('\\nClassification Report | åˆ†ç±»æŠ¥å‘Š:')\n",
    "    print(val_results['classification_report'])\n",
    "else:\n",
    "    print('No data loaded. Please check the data directory.')\n",
    "    print('æœªåŠ è½½æ•°æ®ã€‚è¯·æ£€æŸ¥æ•°æ®ç›®å½•ã€‚')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hht_matrix(signal: np.ndarray, \n",
    "                       fs: float, \n",
    "                       matrix_size: int = 256,\n",
    "                       max_imf: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute HHT (Hilbert-Huang Transform) matrix from a signal.\n",
    "    \n",
    "    This is a reference implementation. For real usage, you may need to install:\n",
    "    pip install PyEMD scipy\n",
    "    \n",
    "    Args:\n",
    "        signal: 1D input signal\n",
    "        fs: Sampling frequency\n",
    "        matrix_size: Output matrix size (matrix_size Ã— matrix_size)\n",
    "        max_imf: Maximum number of IMFs to extract\n",
    "    \n",
    "    Returns:\n",
    "        HHT matrix of shape (matrix_size, matrix_size)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from PyEMD import EMD\n",
    "        from scipy.signal import hilbert\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install PyEMD and scipy: pip install PyEMD scipy\")\n",
    "    \n",
    "    # Perform EMD\n",
    "    emd = EMD()\n",
    "    imfs = emd(signal, max_imf=max_imf)\n",
    "    \n",
    "    # Compute Hilbert transform for each IMF\n",
    "    n_samples = len(signal)\n",
    "    t = np.arange(n_samples) / fs\n",
    "    \n",
    "    # Initialize time-frequency matrix\n",
    "    freq_bins = np.linspace(0, fs/2, matrix_size)\n",
    "    time_bins = np.linspace(0, t[-1], matrix_size)\n",
    "    hht_matrix = np.zeros((matrix_size, matrix_size))\n",
    "    \n",
    "    for imf in imfs:\n",
    "        # Compute analytic signal\n",
    "        analytic = hilbert(imf)\n",
    "        amplitude = np.abs(analytic)\n",
    "        phase = np.unwrap(np.angle(analytic))\n",
    "        \n",
    "        # Compute instantaneous frequency\n",
    "        inst_freq = np.diff(phase) / (2 * np.pi) * fs\n",
    "        inst_freq = np.concatenate([inst_freq, [inst_freq[-1]]])\n",
    "        inst_freq = np.clip(inst_freq, 0, fs/2)\n",
    "        \n",
    "        # Map to time-frequency matrix\n",
    "        for i, (ti, fi, ai) in enumerate(zip(t, inst_freq, amplitude)):\n",
    "            t_idx = int(ti / t[-1] * (matrix_size - 1))\n",
    "            f_idx = int(fi / (fs/2) * (matrix_size - 1))\n",
    "            \n",
    "            t_idx = np.clip(t_idx, 0, matrix_size - 1)\n",
    "            f_idx = np.clip(f_idx, 0, matrix_size - 1)\n",
    "            \n",
    "            hht_matrix[f_idx, t_idx] += ai\n",
    "    \n",
    "    # Normalize\n",
    "    if hht_matrix.max() > 0:\n",
    "        hht_matrix = hht_matrix / hht_matrix.max()\n",
    "    \n",
    "    return hht_matrix.astype(np.float32)\n",
    "\n",
    "\n",
    "# Example usage (uncomment when you have real data):\n",
    "# signal = np.random.randn(1000)  # Replace with real sEMG signal\n",
    "# fs = 1000  # Sampling frequency in Hz\n",
    "# hht_matrix = compute_hht_matrix(signal, fs, matrix_size=256)\n",
    "# plt.imshow(hht_matrix, aspect='auto', cmap='hot', origin='lower')\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "# 5. Method 1: CNN+SVM Training | æ–¹æ³•ä¸€ï¼šCNN+SVM è®­ç»ƒ\n\n",
    "## Usage Instructions | ä½¿ç”¨è¯´æ˜\n\n",
    "**English**: This section shows how to use the traditional CNN+SVM approach. The CNN encoder is used for feature extraction only (weights frozen), and an SVM is trained on those features.\n\n",
    "**ä¸­æ–‡**ï¼šæœ¬èŠ‚å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä¼ ç»Ÿçš„ CNN+SVM æ–¹æ³•ã€‚CNN ç¼–ç å™¨ä»…ç”¨äºç‰¹å¾æå–ï¼ˆæƒé‡å†»ç»“ï¼‰ï¼ŒSVM åœ¨è¿™äº›ç‰¹å¾ä¸Šè®­ç»ƒã€‚\n\n",
    "### Steps | æ­¥éª¤:\n",
    "1. Load and split data | åŠ è½½å’Œåˆ†å‰²æ•°æ®\n",
    "2. Initialize CNN+SVM classifier | åˆå§‹åŒ– CNN+SVM åˆ†ç±»å™¨\n",
    "3. Train (one-shot, no epochs) | è®­ç»ƒï¼ˆä¸€æ¬¡æ€§ï¼Œæ— éœ€å¤šè½®ï¼‰\n",
    "4. Evaluate and save | è¯„ä¼°å’Œä¿å­˜\n\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN+SVM classifier | è®­ç»ƒ CNN+SVM åˆ†ç±»å™¨\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if 'X' in locals() and len(X) > 0:\n",
    "    print('='*70)\n",
    "    print('METHOD 1: CNN+SVM Training | æ–¹æ³•ä¸€ï¼šCNN+SVM è®­ç»ƒ')\n",
    "    print('='*70)\n",
    "    \n",
    "    # Split data | åˆ†å‰²æ•°æ®\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f'\\nTraining set | è®­ç»ƒé›†: {X_train.shape[0]} samples')\n",
    "    print(f'Validation set | éªŒè¯é›†: {X_val.shape[0]} samples')\n",
    "    \n",
    "    # Initialize classifier | åˆå§‹åŒ–åˆ†ç±»å™¨\n",
    "    print('\\nğŸ“¦ Initializing CNN+SVM classifier...')\n",
    "    svm_classifier = sEMGHHTClassifier(\n",
    "        encoder=None,  # Will create default encoder\n",
    "        svm_kernel='rbf',\n",
    "        svm_C=10.0,\n",
    "        svm_gamma='scale',\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Train | è®­ç»ƒ\n",
    "    print('\\nğŸš€ Training SVM classifier (this is a one-shot process)...')\n",
    "    print('è®­ç»ƒ SVM åˆ†ç±»å™¨ï¼ˆè¿™æ˜¯ä¸€æ¬¡æ€§è¿‡ç¨‹ï¼‰...')\n",
    "    svm_classifier.fit(X_train, y_train, batch_size=32)\n",
    "    \n",
    "    # Evaluate on training set | åœ¨è®­ç»ƒé›†ä¸Šè¯„ä¼°\n",
    "    print('\\nğŸ“Š Evaluating on training set | åœ¨è®­ç»ƒé›†ä¸Šè¯„ä¼°...')\n",
    "    train_results = svm_classifier.evaluate(X_train, y_train, batch_size=32)\n",
    "    print(f'Training Accuracy | è®­ç»ƒå‡†ç¡®ç‡: {train_results[\"accuracy\"]:.4f}')\n",
    "    \n",
    "    # Evaluate on validation set | åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°\n",
    "    print('\\nğŸ“Š Evaluating on validation set | åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°...')\n",
    "    val_results = svm_classifier.evaluate(X_val, y_val, batch_size=32)\n",
    "    print(f'Validation Accuracy | éªŒè¯å‡†ç¡®ç‡: {val_results[\"accuracy\"]:.4f}')\n",
    "    \n",
    "    print('\\nğŸ“‹ Classification Report | åˆ†ç±»æŠ¥å‘Š:')\n",
    "    print(val_results['classification_report'])\n",
    "    \n",
    "    # Save model | ä¿å­˜æ¨¡å‹\n",
    "    svm_model_path = os.path.join(CHECKPOINT_DIR, 'cnn_svm_model')\n",
    "    save_svm_classifier(svm_classifier, svm_model_path)\n",
    "    \n",
    "    print(f'\\nâœ… CNN+SVM training complete!')\n",
    "    print(f'Model saved to: {svm_model_path}_*')\n",
    "    \n",
    "else:\n",
    "    print('âš ï¸  No data loaded. Please run the data loading cell first.')\n",
    "    print('æœªåŠ è½½æ•°æ®ã€‚è¯·å…ˆè¿è¡Œæ•°æ®åŠ è½½å•å…ƒæ ¼ã€‚')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "# 6. Method 2: End-to-End Training with Encoder Fine-tuning | æ–¹æ³•äºŒï¼šç«¯åˆ°ç«¯ç¼–ç å™¨å¾®è°ƒè®­ç»ƒ\n\n",
    "## Usage Instructions | ä½¿ç”¨è¯´æ˜\n\n",
    "**English**: This section shows how to train the entire network end-to-end. Both the CNN encoder and the classification head are trained together, allowing the encoder to adapt to your specific data.\n\n",
    "**ä¸­æ–‡**ï¼šæœ¬èŠ‚å±•ç¤ºå¦‚ä½•ç«¯åˆ°ç«¯è®­ç»ƒæ•´ä¸ªç½‘ç»œã€‚CNN ç¼–ç å™¨å’Œåˆ†ç±»å¤´ä¸€èµ·è®­ç»ƒï¼Œå…è®¸ç¼–ç å™¨é€‚åº”ä½ çš„ç‰¹å®šæ•°æ®ã€‚\n\n",
    "### Key Features | å…³é”®ç‰¹æ€§:\n",
    "- âœ… **Multi-epoch training** | å¤šè½®è®­ç»ƒ\n",
    "- âœ… **Automatic checkpointing** | è‡ªåŠ¨æ£€æŸ¥ç‚¹ä¿å­˜\n",
    "- âœ… **Resume from interruption** | ä¸­æ–­åæ¢å¤\n",
    "- âœ… **Best model auto-save** | æœ€ä½³æ¨¡å‹è‡ªåŠ¨ä¿å­˜\n",
    "- âœ… **Real-time progress** | å®æ—¶è¿›åº¦æ˜¾ç¤º\n",
    "- âœ… **Learning rate scheduling** | å­¦ä¹ ç‡è°ƒåº¦\n\n",
    "### Training Process | è®­ç»ƒæµç¨‹:\n",
    "1. Initialize model | åˆå§‹åŒ–æ¨¡å‹\n",
    "2. Start training (or resume) | å¼€å§‹è®­ç»ƒï¼ˆæˆ–æ¢å¤ï¼‰\n",
    "3. Monitor progress | ç›‘æ§è¿›åº¦\n",
    "4. Visualize results | å¯è§†åŒ–ç»“æœ\n\n",
    "### Checkpoints | æ£€æŸ¥ç‚¹è¯´æ˜:\n",
    "- `best_model.pt`: Best performing model (highest val accuracy) | è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼ˆæœ€é«˜éªŒè¯å‡†ç¡®ç‡ï¼‰\n",
    "- `checkpoint_epoch_N.pt`: Saved every N epochs | æ¯ N è½®ä¿å­˜\n",
    "- `final_model.pt`: Final model after all epochs | æ‰€æœ‰è½®æ¬¡åçš„æœ€ç»ˆæ¨¡å‹\n\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END-TO-END TRAINING - Initial Training | ç«¯åˆ°ç«¯è®­ç»ƒ - åˆå§‹è®­ç»ƒ\n",
    "# Run this cell to start training from scratch\n",
    "# è¿è¡Œæ­¤å•å…ƒæ ¼ä»å¤´å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "if 'X' in locals() and len(X) > 0:\n",
    "    print('='*70)\n",
    "    print('METHOD 2: End-to-End Training | æ–¹æ³•äºŒï¼šç«¯åˆ°ç«¯è®­ç»ƒ')\n",
    "    print('='*70)\n",
    "    \n",
    "    # Determine number of classes\n",
    "    n_classes = len(np.unique(y))\n",
    "    print(f'\\nNumber of classes | ç±»åˆ«æ•°: {n_classes}')\n",
    "    \n",
    "    # Initialize end-to-end model | åˆå§‹åŒ–ç«¯åˆ°ç«¯æ¨¡å‹\n",
    "    print('\\nğŸ“¦ Initializing end-to-end model...')\n",
    "    e2e_model = sEMGHHTEndToEndClassifier(\n",
    "        n_classes=n_classes,\n",
    "        in_channels=1,\n",
    "        base_channels=64,\n",
    "        num_encoder_layers=3,\n",
    "        dropout_rate=0.5\n",
    "    )\n",
    "    \n",
    "    print(f'Model architecture | æ¨¡å‹æ¶æ„:')\n",
    "    print(f'  - Encoder: 3-layer CNN | ç¼–ç å™¨ï¼š3 å±‚ CNN')\n",
    "    print(f'  - Feature dim: {e2e_model.encoder.get_feature_dim()}')\n",
    "    print(f'  - Classifier: 2-layer FC | åˆ†ç±»å™¨ï¼š2 å±‚å…¨è¿æ¥')\n",
    "    print(f'  - Output classes: {n_classes}')\n",
    "    \n",
    "    # Training configuration | è®­ç»ƒé…ç½®\n",
    "    EPOCHS = 50  # You can adjust this | å¯ä»¥è°ƒæ•´\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 0.001\n",
    "    CHECKPOINT_INTERVAL = 5  # Save every 5 epochs | æ¯ 5 è½®ä¿å­˜ä¸€æ¬¡\n",
    "    \n",
    "    print(f'\\nâš™ï¸  Training Configuration | è®­ç»ƒé…ç½®:')\n",
    "    print(f'  - Epochs | è®­ç»ƒè½®æ•°: {EPOCHS}')\n",
    "    print(f'  - Batch size | æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}')\n",
    "    print(f'  - Learning rate | å­¦ä¹ ç‡: {LEARNING_RATE}')\n",
    "    print(f'  - Checkpoint interval | æ£€æŸ¥ç‚¹é—´éš”: {CHECKPOINT_INTERVAL} epochs')\n",
    "    print(f'  - Device | è®¾å¤‡: {device}')\n",
    "    \n",
    "    # Start training | å¼€å§‹è®­ç»ƒ\n",
    "    print('\\nğŸš€ Starting training... | å¼€å§‹è®­ç»ƒ...')\n",
    "    print('ğŸ’¡ Tip: Training can be interrupted (Ctrl+C) and resumed later')\n",
    "    print('æç¤ºï¼šè®­ç»ƒå¯ä»¥ä¸­æ–­ï¼ˆCtrl+Cï¼‰å¹¶ç¨åæ¢å¤\\n')\n",
    "    \n",
    "    e2e_history = train_end_to_end_with_checkpointing(\n",
    "        model=e2e_model,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        checkpoint_interval=CHECKPOINT_INTERVAL,\n",
    "        checkpoint_dir=CHECKPOINT_DIR,\n",
    "        resume_from=None,  # Set to None for initial training | åˆå§‹è®­ç»ƒè®¾ä¸º None\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print('\\nâœ… Training completed successfully! | è®­ç»ƒæˆåŠŸå®Œæˆï¼')\n",
    "    \n",
    "else:\n",
    "    print('âš ï¸  No data loaded. Please run the data loading cell first.')\n",
    "    print('æœªåŠ è½½æ•°æ®ã€‚è¯·å…ˆè¿è¡Œæ•°æ®åŠ è½½å•å…ƒæ ¼ã€‚')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END-TO-END TRAINING - Resume from Checkpoint | ç«¯åˆ°ç«¯è®­ç»ƒ - ä»æ£€æŸ¥ç‚¹æ¢å¤\n",
    "# Run this cell ONLY if you want to continue training from a checkpoint\n",
    "# ä»…åœ¨æƒ³è¦ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒæ—¶è¿è¡Œæ­¤å•å…ƒæ ¼\n",
    "\n",
    "# Specify which checkpoint to resume from | æŒ‡å®šè¦æ¢å¤çš„æ£€æŸ¥ç‚¹\n",
    "# Options | é€‰é¡¹:\n",
    "#   - 'best_model.pt': Resume from best model | ä»æœ€ä½³æ¨¡å‹æ¢å¤\n",
    "#   - 'checkpoint_epoch_10.pt': Resume from epoch 10 | ä»ç¬¬ 10 è½®æ¢å¤\n",
    "#   - 'final_model.pt': Resume from final | ä»æœ€ç»ˆæ¨¡å‹æ¢å¤\n",
    "\n",
    "RESUME_CHECKPOINT = os.path.join(CHECKPOINT_DIR, 'best_model.pt')  # Change this | æ›´æ”¹æ­¤å¤„\n",
    "ADDITIONAL_EPOCHS = 20  # How many more epochs to train | å†è®­ç»ƒå¤šå°‘è½®\n",
    "\n",
    "if os.path.exists(RESUME_CHECKPOINT):\n",
    "    print('='*70)\n",
    "    print('RESUMING End-to-End Training | æ¢å¤ç«¯åˆ°ç«¯è®­ç»ƒ')\n",
    "    print('='*70)\n",
    "    \n",
    "    # Load checkpoint to check current epoch\n",
    "    checkpoint = torch.load(RESUME_CHECKPOINT, map_location=device)\n",
    "    current_epoch = checkpoint['epoch']\n",
    "    best_val_acc = checkpoint.get('best_val_acc', 0.0)\n",
    "    \n",
    "    print(f'\\nğŸ“‚ Resuming from: {RESUME_CHECKPOINT}')\n",
    "    print(f'   Last completed epoch: {current_epoch}')\n",
    "    print(f'   Best val accuracy so far: {best_val_acc:.4f}')\n",
    "    print(f'   Will train for {ADDITIONAL_EPOCHS} more epochs')\n",
    "    \n",
    "    # Reinitialize model (architecture must match!)\n",
    "    n_classes = len(np.unique(y))\n",
    "    e2e_model_resumed = sEMGHHTEndToEndClassifier(\n",
    "        n_classes=n_classes,\n",
    "        in_channels=1,\n",
    "        base_channels=64,\n",
    "        num_encoder_layers=3,\n",
    "        dropout_rate=0.5\n",
    "    )\n",
    "    \n",
    "    # Resume training\n",
    "    print('\\nğŸš€ Resuming training... | æ¢å¤è®­ç»ƒ...\\n')\n",
    "    \n",
    "    e2e_history_resumed = train_end_to_end_with_checkpointing(\n",
    "        model=e2e_model_resumed,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        epochs=current_epoch + 1 + ADDITIONAL_EPOCHS,  # Total epochs\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        checkpoint_interval=CHECKPOINT_INTERVAL,\n",
    "        checkpoint_dir=CHECKPOINT_DIR,\n",
    "        resume_from=RESUME_CHECKPOINT,  # Resume from checkpoint\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print('\\nâœ… Resumed training completed! | æ¢å¤è®­ç»ƒå®Œæˆï¼')\n",
    "    e2e_history = e2e_history_resumed  # Update history variable\n",
    "    \n",
    "else:\n",
    "    print(f'âš ï¸  Checkpoint not found: {RESUME_CHECKPOINT}')\n",
    "    print('Please train the model first or check the checkpoint path.')\n",
    "    print('è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æ£€æŸ¥æ£€æŸ¥ç‚¹è·¯å¾„ã€‚')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training History | å¯è§†åŒ–è®­ç»ƒå†å²\n",
    "\n",
    "if 'e2e_history' in locals():\n",
    "    print('ğŸ“Š Plotting training history... | ç»˜åˆ¶è®­ç»ƒå†å²...\\n')\n",
    "    plot_training_history(e2e_history)\n",
    "else:\n",
    "    print('âš ï¸  No training history found. Train the model first.')\n",
    "    print('æœªæ‰¾åˆ°è®­ç»ƒå†å²ã€‚è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Evaluate Best Model | åŠ è½½å’Œè¯„ä¼°æœ€ä½³æ¨¡å‹\n",
    "\n",
    "best_model_path = os.path.join(CHECKPOINT_DIR, 'best_model.pt')\n",
    "\n",
    "if os.path.exists(best_model_path) and 'X_val' in locals():\n",
    "    print('='*70)\n",
    "    print('Loading and Evaluating Best Model | åŠ è½½å’Œè¯„ä¼°æœ€ä½³æ¨¡å‹')\n",
    "    print('='*70)\n",
    "    \n",
    "    # Load best model\n",
    "    n_classes = len(np.unique(y))\n",
    "    best_model = sEMGHHTEndToEndClassifier(\n",
    "        n_classes=n_classes,\n",
    "        in_channels=1,\n",
    "        base_channels=64,\n",
    "        num_encoder_layers=3,\n",
    "        dropout_rate=0.5\n",
    "    )\n",
    "    \n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "    \n",
    "    print(f'\\nâœ… Loaded best model from epoch {checkpoint[\"epoch\"] + 1}')\n",
    "    print(f'   Best validation accuracy: {checkpoint[\"best_val_acc\"]:.4f}')\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    X_val_tensor = torch.tensor(X_val[:, np.newaxis, :, :], dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = best_model(X_val_tensor)\n",
    "        _, predictions = outputs.max(1)\n",
    "        accuracy = (predictions == y_val_tensor).float().mean().item()\n",
    "    \n",
    "    print(f'\\nğŸ“Š Validation Set Performance | éªŒè¯é›†æ€§èƒ½:')\n",
    "    print(f'   Accuracy | å‡†ç¡®ç‡: {accuracy:.4f}')\n",
    "    \n",
    "    # Classification report\n",
    "    from sklearn.metrics import classification_report\n",
    "    y_pred = predictions.cpu().numpy()\n",
    "    print(f'\\nğŸ“‹ Classification Report | åˆ†ç±»æŠ¥å‘Š:')\n",
    "    if 'label_encoder' in locals():\n",
    "        target_names = label_encoder.classes_\n",
    "        print(classification_report(y_val, y_pred, target_names=target_names))\n",
    "    else:\n",
    "        print(classification_report(y_val, y_pred))\n",
    "    \n",
    "else:\n",
    "    if not os.path.exists(best_model_path):\n",
    "        print(f'âš ï¸  Best model not found at: {best_model_path}')\n",
    "        print('Please train the model first.')\n",
    "    else:\n",
    "        print('âš ï¸  Validation data not found. Please load data first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test files | å¯¹æµ‹è¯•æ–‡ä»¶è¿›è¡Œæ¨ç†\n",
    "if 'classifier' in locals() and len(test_files) > 0:\n",
    "    print('\\n' + '='*60)\n",
    "    print('Running Inference on Test Files | å¯¹æµ‹è¯•æ–‡ä»¶è¿›è¡Œæ¨ç†')\n",
    "    print('='*60)\n",
    "    \n",
    "    X_test_list = []\n",
    "    valid_test_files = []\n",
    "    \n",
    "    for test_file in test_files[:10]:  # Limit to first 10 for demo\n",
    "        try:\n",
    "            data = np.load(test_file)\n",
    "            if 'hht' in data:\n",
    "                hht_matrix = data['hht']\n",
    "            else:\n",
    "                hht_matrix = data[list(data.keys())[0]]\n",
    "            \n",
    "            if hht_matrix.shape == (256, 256):\n",
    "                X_test_list.append(hht_matrix)\n",
    "                valid_test_files.append(test_file)\n",
    "        except Exception as e:\n",
    "            print(f'Error loading {test_file}: {e}')\n",
    "    \n",
    "    if len(X_test_list) > 0:\n",
    "        X_test = np.array(X_test_list, dtype=np.float32)\n",
    "        y_test_pred = classifier.predict(X_test, batch_size=32)\n",
    "        y_test_proba = classifier.predict_proba(X_test, batch_size=32)\n",
    "        \n",
    "        svm_classes = classifier.svm.classes_\n",
    "        \n",
    "        print(f'\\nPredictions for {len(valid_test_files)} test files:')\n",
    "        print(f'å‰ {len(valid_test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶çš„é¢„æµ‹ç»“æœï¼š\\n')\n",
    "        for i, (filename, pred, proba) in enumerate(zip(valid_test_files, y_test_pred, y_test_proba)):\n",
    "            pred_idx = np.where(svm_classes == pred)[0][0]\n",
    "            class_name = label_encoder.classes_[pred]\n",
    "            confidence = proba[pred_idx]\n",
    "            print(f'{os.path.basename(filename)}: {class_name} (confidence: {confidence:.4f})')\n",
    "else:\n",
    "    print('No test files to process or classifier not trained.')\n",
    "    print('æ²¡æœ‰æµ‹è¯•æ–‡ä»¶æˆ–åˆ†ç±»å™¨æœªè®­ç»ƒã€‚')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "1. **CNN Encoder Architecture**: 3-layer convolutional network with Instance Normalization and LeakyReLU\n",
    "2. **SVM Classifier**: Multi-class classification using extracted CNN features\n",
    "3. **End-to-End Model**: Optional fully trainable model with neural network classifier\n",
    "4. **Real Data Training**: Load and train on actual HHT matrices from Kaggle dataset\n",
    "5. **HHT Computation**: Reference implementation for real sEMG signals\n",
    "\n",
    "### Hyperparameter Tuning:\n",
    "- `base_channels`: Number of channels in first conv layer (default: 64)\n",
    "- `svm_C`: SVM regularization parameter\n",
    "- `svm_kernel`: SVM kernel type ('rbf', 'linear', 'poly')\n",
    "- `learning_rate`: Learning rate for end-to-end training\n",
    "- `dropout_rate`: Dropout rate in end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"sEMG-HHT CNN Classifier - Ready for Use\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDevice: {device}\")\n",
    "print(f\"Encoder feature dimension: {encoder.get_feature_dim()}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"\\nClass names:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}