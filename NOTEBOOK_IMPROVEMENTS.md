# Jupyter Notebook Training Improvements

## æ”¹è¿›è¯´æ˜ | Improvement Summary

æœ¬æ¬¡æ”¹è¿›è§£å†³äº†Jupyter Notebookè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¤šä¸ªé—®é¢˜ï¼Œå¹¶å¢åŠ äº†æ–°åŠŸèƒ½ã€‚

This improvement addresses several issues in the Jupyter Notebook training process and adds new features.

## è§£å†³çš„é—®é¢˜ | Issues Resolved

### 1. å­¦ä¹ ç‡è¿‡åº¦è¡°å‡ | Excessive Learning Rate Decay

**é—®é¢˜ | Problem:**
- å­¦ä¹ ç‡æœ€å°å€¼è®¾ç½®ä¸º `1e-7`ï¼Œå¯¼è‡´è®­ç»ƒåæœŸå­¦ä¹ ç‡è¿‡ä½ï¼Œå½±å“æ¨¡å‹æ”¶æ•›
- Minimum learning rate was set to `1e-7`, causing extremely low learning rates in later training stages

**è§£å†³æ–¹æ¡ˆ | Solution:**
- å°† `LR_SCHEDULER_MIN_LR` ä» `1e-7` æ”¹ä¸º `1e-6`
- Changed `LR_SCHEDULER_MIN_LR` from `1e-7` to `1e-6`

**ä½ç½® | Location:** Cell 6 - Hyperparameter Configuration

### 2. å¤šè½®è®­ç»ƒæ”¯æŒ | Multiple Training Rounds Support

**æ–°åŠŸèƒ½ | New Feature:**
- æ·»åŠ äº†è®­ç»ƒè½®æ•°é…ç½®ï¼Œæ”¯æŒå¤šè½®è®­ç»ƒ
- Added training rounds configuration to support multiple training rounds

**æ–°å¢å¸¸é‡ | New Constants:**
```python
NUM_TRAINING_ROUNDS = 3      # æ€»è®­ç»ƒè½®æ•° | Total training rounds
EPOCHS_PER_ROUND = 100       # æ¯è½®è®­ç»ƒçš„epochæ•° | Epochs per training round
```

**åŠŸèƒ½è¯´æ˜ | Features:**
- æ¯è½®è®­ç»ƒ100ä¸ªepochï¼Œæ€»å…±è®­ç»ƒ3è½®ï¼ˆå¯é…ç½®ï¼‰
- Each round trains for 100 epochs, total 3 rounds (configurable)
- è®­ç»ƒè¿›åº¦æ˜¾ç¤ºå½“å‰è½®æ¬¡å’Œæ€»è½®æ¬¡
- Training progress shows current round and total rounds
- æ£€æŸ¥ç‚¹æ–‡ä»¶ååŒ…å«è½®æ¬¡ä¿¡æ¯
- Checkpoint filenames include round information

**ä½ç½® | Location:** Cell 6 - Hyperparameter Configuration, Cell 12 - Training Function

### 3. ç»§ç»­è®­ç»ƒåŠŸèƒ½ | Resume Training Support

**æ–°åŠŸèƒ½ | New Feature:**
- æ”¯æŒä»ä¸Šæ¬¡è®­ç»ƒçš„æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ
- Support resuming training from the last checkpoint

**å®ç°ç»†èŠ‚ | Implementation Details:**
- è‡ªåŠ¨æ£€æµ‹æ˜¯å¦å­˜åœ¨ä¹‹å‰çš„æ£€æŸ¥ç‚¹ `best_action_quality_model.pt`
- Automatically detects if previous checkpoint `best_action_quality_model.pt` exists
- å¦‚æœå­˜åœ¨ï¼ŒåŠ è½½æ¨¡å‹çŠ¶æ€ã€ä¼˜åŒ–å™¨çŠ¶æ€ã€è®­ç»ƒå†å²å’Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡
- If exists, loads model state, optimizer state, training history, and best validation accuracy
- ä»ä¸Šæ¬¡åœæ­¢çš„epochç»§ç»­è®­ç»ƒ
- Continues training from the last stopped epoch

**æ–°å¢å‚æ•° | New Parameters:**
```python
def train_action_quality_model(
    ...
    resume_from=None,           # æ£€æŸ¥ç‚¹è·¯å¾„ | Checkpoint path
    num_rounds=1,               # è®­ç»ƒè½®æ•° | Number of rounds
    epochs_per_round=100        # æ¯è½®epochæ•° | Epochs per round
):
```

**ä½ç½® | Location:** Cell 12 - Training Function

### 4. DataLoaderå¤šè¿›ç¨‹é”™è¯¯ | DataLoader Multiprocessing Error

**é—®é¢˜ | Problem:**
```
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__>
IOStream.flush timed out
RuntimeError: cannot join current thread
AssertionError: can only test a child process
```

**åŸå›  | Cause:**
- Jupyter Notebookç¯å¢ƒä¸­ä½¿ç”¨å¤šè¿›ç¨‹ä¼šå¯¼è‡´å„ç§é—®é¢˜
- Using multiprocessing in Jupyter Notebook environment causes various issues
- ç‰¹åˆ«æ˜¯åœ¨notebookçš„ä¸»çº¿ç¨‹ä¸­å¯åŠ¨DataLoader workers
- Especially when starting DataLoader workers in the notebook's main thread

**è§£å†³æ–¹æ¡ˆ | Solution:**
- å°† `num_workers` ä» `2` æ”¹ä¸º `0`
- Changed `num_workers` from `2` to `0`
- è¿™å°†ä½¿DataLoaderåœ¨ä¸»è¿›ç¨‹ä¸­åŠ è½½æ•°æ®ï¼Œé¿å…å¤šè¿›ç¨‹é—®é¢˜
- This makes DataLoader load data in the main process, avoiding multiprocessing issues

**ä½ç½® | Location:** Cell 12 - Training Function

### 5. torch.loadçš„UnpicklingError | torch.load UnpicklingError

**é—®é¢˜ | Problem:**
```
UnpicklingError: Weights only load failed...
WeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default
```

**åŸå›  | Cause:**
- PyTorch 2.6å¼€å§‹ï¼Œ`torch.load`çš„`weights_only`å‚æ•°é»˜è®¤å€¼ä»`False`æ”¹ä¸º`True`
- Starting from PyTorch 2.6, the default value of `weights_only` parameter in `torch.load` changed from `False` to `True`
- è¿™å¯¼è‡´åŠ è½½åŒ…å«numpyå¯¹è±¡çš„æ£€æŸ¥ç‚¹æ—¶å‡ºé”™
- This causes errors when loading checkpoints containing numpy objects

**è§£å†³æ–¹æ¡ˆ | Solution:**
- åœ¨æ‰€æœ‰ `torch.load` è°ƒç”¨ä¸­æ˜¾å¼æ·»åŠ  `weights_only=False` å‚æ•°
- Explicitly add `weights_only=False` parameter to all `torch.load` calls

**ä¿®æ”¹ä½ç½® | Modified Locations:**
- Cell 12: è®­ç»ƒå‡½æ•°ä¸­åŠ è½½æ£€æŸ¥ç‚¹ | Loading checkpoint in training function
- Cell 16: SVMè®­ç»ƒä¸­åŠ è½½æ¨¡å‹ | Loading model in SVM training

## ä½¿ç”¨è¯´æ˜ | Usage Instructions

### é¦–æ¬¡è®­ç»ƒ | First Training

1. è¿è¡Œæ‰€æœ‰å•å…ƒæ ¼ç›´åˆ°è®­ç»ƒå•å…ƒæ ¼ï¼ˆCell 12ï¼‰
2. Run all cells up to the training cell (Cell 12)
3. è®­ç»ƒå°†è‡ªåŠ¨å¼€å§‹ï¼Œæ€»å…±è®­ç»ƒ300ä¸ªepochï¼ˆ3è½® Ã— 100 epoch/è½®ï¼‰
4. Training will automatically start, total 300 epochs (3 rounds Ã— 100 epochs/round)

### ç»§ç»­è®­ç»ƒ | Resume Training

1. å¦‚æœä¹‹å‰å·²ç»è®­ç»ƒè¿‡ï¼Œå†æ¬¡è¿è¡Œè®­ç»ƒå•å…ƒæ ¼ï¼ˆCell 12ï¼‰
2. If previously trained, run the training cell (Cell 12) again
3. ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½ä¹‹å‰çš„æ£€æŸ¥ç‚¹
4. The system will automatically detect and load the previous checkpoint
5. è®­ç»ƒå°†ä»ä¸Šæ¬¡åœæ­¢çš„åœ°æ–¹ç»§ç»­
6. Training will continue from where it stopped

### ä¿®æ”¹è®­ç»ƒè½®æ•° | Modify Training Rounds

åœ¨ Cell 6 ä¸­ä¿®æ”¹ä»¥ä¸‹å¸¸é‡ï¼š
Modify the following constants in Cell 6:

```python
NUM_TRAINING_ROUNDS = 5      # æ”¹ä¸º5è½® | Change to 5 rounds
EPOCHS_PER_ROUND = 50        # æ¯è½®50ä¸ªepoch | 50 epochs per round
```

## è¾“å‡ºç¤ºä¾‹ | Output Examples

### æ–°è®­ç»ƒ | New Training
```
ğŸ†• å¼€å§‹æ–°çš„è®­ç»ƒ | Starting new training

================================================================================
å¼€å§‹è®­ç»ƒåŠ¨ä½œè´¨é‡åˆ†ç±»å™¨ | Starting Action Quality Classifier Training
================================================================================

ğŸš€ è®­ç»ƒé…ç½® | Training configuration:
   è®¾å¤‡ Device: cuda
   è®­ç»ƒæ ·æœ¬ Training samples: 800
   éªŒè¯æ ·æœ¬ Validation samples: 200
   è®­ç»ƒè½®æ•° Training rounds: 3
   æ¯è½®epochæ•° Epochs per round: 100
   æ€»epochæ•° Total epochs: 300
   èµ·å§‹epoch Starting epoch: 0
   ...

Round [1/3] Epoch [  1/300] | Train Loss: 0.8234 | Train Acc: 0.6500 | ...
Round [1/3] Epoch [  2/300] | Train Loss: 0.7123 | Train Acc: 0.7100 | ...
...
Round [2/3] Epoch [101/300] | Train Loss: 0.3456 | Train Acc: 0.8900 | ...
...
Round [3/3] Epoch [300/300] | Train Loss: 0.2123 | Train Acc: 0.9300 | ...
```

### ç»§ç»­è®­ç»ƒ | Resume Training
```
â™»ï¸  å‘ç°æ£€æŸ¥ç‚¹ï¼Œå°†ç»§ç»­è®­ç»ƒ | Found checkpoint, will resume training

ğŸ“‚ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ | Resuming training from checkpoint: ./checkpoints/best_action_quality_model.pt
   âœ… å·²æ¢å¤åˆ°epoch 150, æœ€ä½³éªŒè¯å‡†ç¡®ç‡: 0.9123
   âœ… Resumed to epoch 150, best val acc: 0.9123

ğŸš€ è®­ç»ƒé…ç½® | Training configuration:
   ...
   èµ·å§‹epoch Starting epoch: 150
   ...

Round [2/3] Epoch [150/300] | Train Loss: 0.3012 | Train Acc: 0.9000 | ...
```

## æŠ€æœ¯ç»†èŠ‚ | Technical Details

### æ£€æŸ¥ç‚¹æ ¼å¼ | Checkpoint Format

æ£€æŸ¥ç‚¹ç°åœ¨åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š
Checkpoints now contain the following information:

```python
{
    'epoch': current_epoch,           # å½“å‰epoch | Current epoch
    'round': current_round,           # å½“å‰è½®æ¬¡ | Current round
    'model_state_dict': ...,          # æ¨¡å‹å‚æ•° | Model parameters
    'optimizer_state_dict': ...,      # ä¼˜åŒ–å™¨çŠ¶æ€ | Optimizer state
    'best_val_acc': ...,              # æœ€ä½³éªŒè¯å‡†ç¡®ç‡ | Best validation accuracy
    'history': {                      # è®­ç»ƒå†å² | Training history
        'train_loss': [...],
        'train_acc': [...],
        'val_loss': [...],
        'val_acc': [...],
        'lr': [...]
    }
}
```

### æ–‡ä»¶å‘½å | File Naming

- æœ€ä½³æ¨¡å‹: `best_action_quality_model.pt`
- Best model: `best_action_quality_model.pt`
- å®šæœŸæ£€æŸ¥ç‚¹: `action_quality_round_{round}_epoch_{epoch}.pt`
- Periodic checkpoints: `action_quality_round_{round}_epoch_{epoch}.pt`

## æ³¨æ„äº‹é¡¹ | Notes

1. **å­¦ä¹ ç‡è°ƒåº¦å™¨ | Learning Rate Scheduler:**
   - ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼Œå­¦ä¹ ç‡ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸é™ä½
   - Uses cosine annealing scheduler, learning rate gradually decreases during training
   - æœ€å°å­¦ä¹ ç‡ç°åœ¨è®¾ç½®ä¸º `1e-6`ï¼Œé¿å…è¿‡åº¦è¡°å‡
   - Minimum learning rate now set to `1e-6` to avoid excessive decay

2. **ç»§ç»­è®­ç»ƒæ—¶çš„å­¦ä¹ ç‡ | Learning Rate When Resuming:**
   - ç»§ç»­è®­ç»ƒæ—¶ä¼šæ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡
   - Optimizer state including learning rate is restored when resuming
   - å­¦ä¹ ç‡å°†ä»ä¿å­˜æ—¶çš„å€¼ç»§ç»­
   - Learning rate will continue from the saved value

3. **æ€§èƒ½è€ƒè™‘ | Performance Considerations:**
   - `num_workers=0` å¯èƒ½ä¼šç¨å¾®é™ä½æ•°æ®åŠ è½½é€Ÿåº¦
   - `num_workers=0` may slightly slow down data loading
   - ä½†åœ¨Jupyterç¯å¢ƒä¸­è¿™æ˜¯å¿…è¦çš„ï¼Œä»¥é¿å…å¤šè¿›ç¨‹é—®é¢˜
   - But this is necessary in Jupyter environment to avoid multiprocessing issues

4. **ç£ç›˜ç©ºé—´ | Disk Space:**
   - å®šæœŸæ£€æŸ¥ç‚¹æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡
   - Periodic checkpoints are saved every 10 epochs
   - é•¿æ—¶é—´è®­ç»ƒå¯èƒ½ä¼šå ç”¨è¾ƒå¤šç£ç›˜ç©ºé—´
   - Long training sessions may consume significant disk space
   - å¯ä»¥é€‚å½“å¢åŠ  `CHECKPOINT_INTERVAL` å€¼æ¥å‡å°‘æ£€æŸ¥ç‚¹æ•°é‡
   - Can increase `CHECKPOINT_INTERVAL` value to reduce number of checkpoints
