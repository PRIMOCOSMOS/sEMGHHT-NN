# sEMG-HHT CNN Classifier | sEMG-HHT CNN åˆ†ç±»å™¨

[English](#english) | [ä¸­æ–‡](#chinese)

---

## <a name="english"></a>English Version

A dual classification system for surface electromyography (sEMG) signals using Hilbert-Huang Transform (HHT) representation. This project separates gender and movement quality classification into specialized models for better accuracy.

### ğŸ¯ Overview

This project implements a dual deep learning pipeline:
1. **Deep Learning CNN** for Action Quality (Full, Half, Invalid) - 3 classes
2. **SVM Classifier** for Gender Classification (M, F) - 2 classes

**Key Features:**
- âœ… **Expanded CNN architecture** (7 layers, 2048 channels) - **NEW!**
- âœ… **BatchNormalization** + Kaiming initialization for training stability
- âœ… **Learning rate warmup** + cosine annealing scheduling
- âœ… **Gradient clipping** to prevent explosion
- âœ… **Label smoothing** for better generalization
- âœ… **Separate optimized models** for each task

**ğŸ“– LATEST: [Refactoring Summary](REFACTORING_SUMMARY.md)** - **NEW!** Complete details on the expanded architecture and training optimizations.

**ğŸ“– [Dual Classifier System Guide](DUAL_CLASSIFIER_GUIDE.md)** - Complete documentation on the dual classifier system.

### ğŸ—ï¸ Model Architecture

**Configurable CNN Encoder Structure (1-8 Layers):** - **CONFIGURABLE!**
- **Flexible depth** from 1 to 8 convolutional layers (default: 5)
- Each convolutional layer contains:
  - Conv2D (kernel=3, stride=2, padding=1, bias=False)
  - **Batch Normalization** (training stability)
  - LeakyReLU activation (slope=0.2)
  - **Kaiming initialization** (proper gradient flow)
- **Residual connections** in deeper layers (6+)
- **Global Average Pooling** at the end
- **Channel progression**: 64 â†’ 128 â†’ 256 â†’ 512 â†’ 1024 â†’ 2048 â†’ ...
- Output: **Feature vector** (size depends on num_layers)
  - 3 layers: 256-dim
  - 5 layers: 1024-dim
  - 7 layers: 2048-dim
  - 8 layers: 2048-dim

**Adaptive Classifier Head:**
- Automatically scales intermediate layer sizes based on encoder depth
- Ensures smooth gradient flow regardless of network configuration

**Classifier Options:**
1. **Action Quality CNN**: Configurable CNN â†’ Dropout â†’ Adaptive FC layers â†’ 3 classes
2. **Gender SVM**: CNN features â†’ StandardScaler â†’ RBF SVM â†’ 2 classes

### ğŸ“Š Classification Task

**Dual Classification System:**
- **Action Quality**: 3 classes (Full, Half, Invalid) - Deep Learning CNN
- **Gender**: 2 classes (Male, Female) - SVM

**Why Dual Classifiers?**
- Better accuracy through task-specific optimization
- Faster convergence for each simpler task
- More stable training dynamics
- Easier to debug and improve

**Class Mapping:**
| Task | Classes | Model Type |
|------|---------|------------|
| Action Quality | Full, Half, Invalid | Deep CNN (configurable 1-8 layers) **[CONFIGURABLE]** |
| Gender | M, F | SVM (RBF kernel) |

### ğŸ†• Recent Improvements

**2025-12-22 (Latest Update):**
1. **Configurable Network Depth** - Number of layers now configurable as hyperparameter (1-8 layers)
2. **Adaptive Architecture** - Classifier head automatically scales with encoder depth
3. **Parameter Validation** - Input validation ensures proper dimensions throughout training
4. **Safe Checkpoint Saving** - Atomic writes with disk space checks prevent corruption
5. **Command-line Control** - New `--num_encoder_layers` and `--base_channels` arguments

**Previous Improvements (2025-12-22):**

**Problem Solved:** Previous notebook had issues with loss barely decreasing and accuracy not improving.

**Key Solutions:**
1. **Expanded Network** - From 3-5 layers to **configurable depth** (default 7 layers)
2. **Better Initialization** - Kaiming initialization prevents vanishing/exploding gradients
3. **Batch Normalization** - Replaced InstanceNorm for faster, more stable training
4. **Learning Rate Strategy** - Lowered LR (0.0001) + warmup (5 epochs) + cosine annealing
5. **Gradient Clipping** - Prevents gradient explosion in deep network
6. **Label Smoothing** - Improves generalization and prevents overconfidence
7. **Residual Connections** - Better gradient flow in deeper layers
8. **AdamW Optimizer** - With weight decay for better regularization

**See [REFACTORING_SUMMARY.md](REFACTORING_SUMMARY.md) for complete details.**

### ğŸš€ Quick Start

#### Option 1: Kaggle Notebook (Easiest)

1. Upload `semg_hht_cnn_classifier.ipynb` to Kaggle
2. Add the **HILBERTMATRIX_NPZ** dataset to your notebook
3. Enable GPU accelerator (Settings â†’ Accelerator â†’ GPU)
4. Run all cells

The notebook automatically detects Kaggle environment and loads data from `/kaggle/input/hilbertmatrix-npz/hht_matrices/`

#### Option 2: Command-Line Training (For Local/Server)

```bash
# Install dependencies
pip install -r requirements.txt

# Train with your data (new dual classifier system)
python train.py --data_dir ./data --checkpoint_dir ./checkpoints --epochs 100

# Advanced training with custom parameters and configurable network depth
python train.py \
    --data_dir ./data \
    --checkpoint_dir ./checkpoints \
    --epochs 100 \
    --batch_size 16 \
    --learning_rate 0.001 \
    --num_encoder_layers 5 \
    --base_channels 64 \
    --test_size 0.2

# Train with deeper network (7 layers, more features)
python train.py \
    --data_dir ./data \
    --num_encoder_layers 7 \
    --base_channels 64 \
    --epochs 100

# Train with shallower network (3 layers, faster training)
python train.py \
    --data_dir ./data \
    --num_encoder_layers 3 \
    --base_channels 64 \
    --epochs 100

# Resume from checkpoint
python train.py --data_dir ./data --checkpoint_dir ./checkpoints --resume

# Run inference
python inference.py --checkpoint ./checkpoints/final --input ./new_data/
```

**New Parameters:**
- `--num_encoder_layers`: Number of CNN layers (1-8, default: 5)
  - More layers = deeper network, better feature extraction, slower training
  - Fewer layers = faster training, less memory usage
- `--base_channels`: Base number of channels (default: 64)
  - Higher values = more parameters, better capacity

See [DUAL_CLASSIFIER_GUIDE.md](DUAL_CLASSIFIER_GUIDE.md) for detailed instructions on the new architecture.

### ğŸ“ Data Format

**File Naming Convention:**
```
MUSCLENAME_movement_GENDER_###.npz
```

Examples:
- `BICEPS_fatiguetest_M_006.npz` â†’ Male, Full movement
- `TRICEPS_half_F_012.npz` â†’ Female, Half movement
- `FOREARM_invalid_M_003.npz` â†’ Male, Invalid movement
- `Test1_1_015.npz` â†’ Unlabeled test file (starts with "Test")

**File Content:**
Each `.npz` file contains a 256Ã—256 HHT matrix stored with key `'hht'`.

### ğŸ”§ Module Functions

**1. `train.py` - Production Training Script**
- Loads .npz files from directory
- Parses filenames to extract gender and movement labels
- Trains CNN encoder to extract features
- Trains SVM classifier on extracted features
- Saves checkpoints (encoder, scaler, SVM, metadata)
- Evaluates on validation set
- Runs inference on test files (files starting with "Test")

**2. `inference.py` - Inference Script**
- Loads trained model from checkpoint
- Processes single file or batch of files
- Outputs predictions with confidence scores
- Saves results to JSON

**3. `generate_sample_data.py` - Data Generator**
- Creates synthetic 256Ã—256 HHT matrices for testing
- Generates proper filename formats
- Useful for testing the pipeline

**4. Jupyter Notebook `semg_hht_cnn_classifier.ipynb`**
- Interactive exploration and visualization
- Integrated with Kaggle datasets
- Step-by-step training workflow
- Suitable for experimentation

### ğŸ“ˆ Training Process

1. **Data Loading**: Loads all .npz files, filters test files (starting with "Test")
2. **Feature Extraction**: CNN encoder processes 256Ã—256 matrices â†’ 256-dim vectors
3. **Normalization**: StandardScaler normalizes features
4. **SVM Training**: RBF kernel SVM trains on normalized features
5. **Validation**: Computes accuracy, precision, recall, F1-score
6. **Test Inference**: Predicts labels for test files
7. **Checkpoint Saving**: Saves complete model state

---

## <a name="chinese"></a>ä¸­æ–‡ç‰ˆæœ¬

åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„è¡¨é¢è‚Œç”µä¿¡å·ï¼ˆsEMGï¼‰åŒåˆ†ç±»å™¨ç³»ç»Ÿï¼Œä½¿ç”¨å¸Œå°”ä¼¯ç‰¹-é»„å˜æ¢ï¼ˆHHTï¼‰è¡¨ç¤ºã€‚è¯¥é¡¹ç›®å°†æ€§åˆ«å’ŒåŠ¨ä½œè´¨é‡åˆ†ç±»åˆ†ç¦»ä¸ºä¸“é—¨çš„æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„å‡†ç¡®æ€§ã€‚

### ğŸ¯ æ¦‚è¿°

è¯¥é¡¹ç›®å®ç°äº†åŒé‡æ·±åº¦å­¦ä¹ æµç¨‹ï¼š
1. **æ·±åº¦å­¦ä¹ CNN** ç”¨äºåŠ¨ä½œè´¨é‡ï¼ˆå…¨ç¨‹ã€åŠç¨‹ã€æ— æ•ˆï¼‰- 3ç±»
2. **SVMåˆ†ç±»å™¨** ç”¨äºæ€§åˆ«åˆ†ç±»ï¼ˆç”·ã€å¥³ï¼‰- 2ç±»

**ä¸»è¦ç‰¹ç‚¹ï¼š**
- âœ… **å¯é…ç½®çš„CNNæ¶æ„**ï¼ˆ1-8å±‚ï¼Œçµæ´»æ·±åº¦ï¼‰- **æ–°åŠŸèƒ½ï¼**
- âœ… **è‡ªé€‚åº”åˆ†ç±»å¤´**æ ¹æ®ç½‘ç»œæ·±åº¦è‡ªåŠ¨è°ƒæ•´
- âœ… **æ‰¹å½’ä¸€åŒ–** + Kaimingåˆå§‹åŒ–ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§
- âœ… **å®‰å…¨çš„æ£€æŸ¥ç‚¹ä¿å­˜**é˜²æ­¢ç£ç›˜ç©ºé—´ä¸è¶³å¯¼è‡´çš„æŸå
- âœ… **å­¦ä¹ ç‡é¢„çƒ­** + ä½™å¼¦é€€ç«è°ƒåº¦
- âœ… **æ¢¯åº¦è£å‰ª**é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- âœ… **æ ‡ç­¾å¹³æ»‘**æé«˜æ³›åŒ–èƒ½åŠ›
- âœ… æ¯ä¸ªä»»åŠ¡çš„**å•ç‹¬ä¼˜åŒ–æ¨¡å‹**

**ğŸ“– æœ€æ–°ï¼š[é‡æ„æ€»ç»“](REFACTORING_SUMMARY.md)** - **æ–°ï¼** æ‰©å±•æ¶æ„å’Œè®­ç»ƒä¼˜åŒ–çš„å®Œæ•´ç»†èŠ‚ã€‚

**ğŸ“– [åŒåˆ†ç±»å™¨ç³»ç»ŸæŒ‡å—](DUAL_CLASSIFIER_GUIDE.md)** - å…³äºåŒåˆ†ç±»å™¨ç³»ç»Ÿçš„å®Œæ•´æ–‡æ¡£ã€‚

### ğŸ—ï¸ æ¨¡å‹æ¶æ„

**å¯é…ç½®çš„CNNç¼–ç å™¨ç»“æ„ï¼ˆ1-8å±‚ï¼‰ï¼š** - **å¯é…ç½®ï¼**
- **çµæ´»æ·±åº¦** ä»1åˆ°8ä¸ªå·ç§¯å±‚ï¼ˆé»˜è®¤ï¼š5å±‚ï¼‰
- æ¯ä¸ªå·ç§¯å±‚åŒ…å«ï¼š
  - Conv2Dï¼ˆkernel=3, stride=2, padding=1, bias=Falseï¼‰
  - **æ‰¹å½’ä¸€åŒ–**ï¼ˆè®­ç»ƒç¨³å®šæ€§ï¼‰
  - LeakyReLU æ¿€æ´»å‡½æ•°ï¼ˆslope=0.2ï¼‰
  - **Kaimingåˆå§‹åŒ–**ï¼ˆæ­£ç¡®çš„æ¢¯åº¦æµåŠ¨ï¼‰
- **æ®‹å·®è¿æ¥**åœ¨æ›´æ·±å±‚ä¸­ï¼ˆç¬¬6å±‚åŠä»¥ä¸Šï¼‰
- æœ«å°¾ä½¿ç”¨**å…¨å±€å¹³å‡æ± åŒ–**
- **é€šé“é€’å¢åºåˆ—**ï¼š64 â†’ 128 â†’ 256 â†’ 512 â†’ 1024 â†’ 2048 â†’ ...
- è¾“å‡ºï¼š**ç‰¹å¾å‘é‡**ï¼ˆå¤§å°å–å†³äºå±‚æ•°ï¼‰
  - 3å±‚ï¼š256ç»´
  - 5å±‚ï¼š1024ç»´
  - 7å±‚ï¼š2048ç»´
  - 8å±‚ï¼š2048ç»´

**è‡ªé€‚åº”åˆ†ç±»å¤´ï¼š**
- æ ¹æ®ç¼–ç å™¨æ·±åº¦è‡ªåŠ¨è°ƒæ•´ä¸­é—´å±‚å¤§å°
- ç¡®ä¿æ— è®ºç½‘ç»œé…ç½®å¦‚ä½•éƒ½èƒ½å¹³æ»‘çš„æ¢¯åº¦æµåŠ¨

**åˆ†ç±»å™¨é€‰é¡¹ï¼š**
1. **åŠ¨ä½œè´¨é‡CNN**ï¼šå¯é…ç½®CNN â†’ Dropout â†’ è‡ªé€‚åº”å…¨è¿æ¥å±‚ â†’ 3ç±»
2. **æ€§åˆ«SVM**ï¼šCNNç‰¹å¾ â†’ StandardScaler â†’ RBF SVM â†’ 2ç±»

### ğŸ“Š åˆ†ç±»ä»»åŠ¡

**åŒåˆ†ç±»å™¨ç³»ç»Ÿï¼š**
- **åŠ¨ä½œè´¨é‡**ï¼š3ç±»ï¼ˆå…¨ç¨‹ã€åŠç¨‹ã€æ— æ•ˆï¼‰- æ·±åº¦å­¦ä¹ CNN
- **æ€§åˆ«**ï¼š2ç±»ï¼ˆç”·æ€§ã€å¥³æ€§ï¼‰- SVM

**ä¸ºä»€ä¹ˆä½¿ç”¨åŒåˆ†ç±»å™¨ï¼Ÿ**
- é€šè¿‡ç‰¹å®šä»»åŠ¡ä¼˜åŒ–è·å¾—æ›´å¥½çš„å‡†ç¡®æ€§
- æ¯ä¸ªç®€å•ä»»åŠ¡æ›´å¿«æ”¶æ•›
- æ›´ç¨³å®šçš„è®­ç»ƒåŠ¨æ€
- æ›´å®¹æ˜“è°ƒè¯•å’Œæ”¹è¿›

**ç±»åˆ«æ˜ å°„ï¼š**
| ä»»åŠ¡ | ç±»åˆ« | æ¨¡å‹ç±»å‹ |
|------|------|----------|
| åŠ¨ä½œè´¨é‡ | å…¨ç¨‹ã€åŠç¨‹ã€æ— æ•ˆ | æ·±åº¦CNNï¼ˆå¯é…ç½®1-8å±‚ï¼‰**[å¯é…ç½®]** |
| æ€§åˆ« | ç”·ã€å¥³ | SVMï¼ˆRBFæ ¸ï¼‰|

### ğŸ†• æœ€è¿‘æ”¹è¿›

**2025-12-22ï¼ˆæœ€æ–°æ›´æ–°ï¼‰ï¼š**
1. **å¯é…ç½®çš„ç½‘ç»œæ·±åº¦** - å±‚æ•°ç°åœ¨å¯ä½œä¸ºè¶…å‚æ•°é…ç½®ï¼ˆ1-8å±‚ï¼‰
2. **è‡ªé€‚åº”æ¶æ„** - åˆ†ç±»å¤´æ ¹æ®ç¼–ç å™¨æ·±åº¦è‡ªåŠ¨è°ƒæ•´
3. **å‚æ•°éªŒè¯** - è¾“å…¥éªŒè¯ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ­£ç¡®ç»´åº¦
4. **å®‰å…¨çš„æ£€æŸ¥ç‚¹ä¿å­˜** - åŸå­å†™å…¥å’Œç£ç›˜ç©ºé—´æ£€æŸ¥é˜²æ­¢æŸå
5. **å‘½ä»¤è¡Œæ§åˆ¶** - æ–°å¢ `--num_encoder_layers` å’Œ `--base_channels` å‚æ•°

**ä¹‹å‰çš„æ”¹è¿›ï¼ˆ2025-12-22ï¼‰ï¼š**

**è§£å†³çš„é—®é¢˜ï¼š** ä¹‹å‰çš„ç¬”è®°æœ¬å­˜åœ¨æŸå¤±å‡ ä¹ä¸ä¸‹é™ã€å‡†ç¡®ç‡ä¸æå‡çš„é—®é¢˜ã€‚

**å…³é”®è§£å†³æ–¹æ¡ˆï¼š**
1. **æ‰©å±•ç½‘ç»œ** - ä»3-5å±‚æ‰©å±•åˆ°**å¯é…ç½®æ·±åº¦**ï¼ˆé»˜è®¤7å±‚ï¼‰
2. **æ›´å¥½çš„åˆå§‹åŒ–** - Kaimingåˆå§‹åŒ–é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
3. **æ‰¹å½’ä¸€åŒ–** - æ›¿æ¢InstanceNormï¼Œå®ç°æ›´å¿«æ›´ç¨³å®šçš„è®­ç»ƒ
4. **å­¦ä¹ ç‡ç­–ç•¥** - é™ä½å­¦ä¹ ç‡(0.0001) + é¢„çƒ­(5è½®) + ä½™å¼¦é€€ç«
5. **æ¢¯åº¦è£å‰ª** - é˜²æ­¢æ·±åº¦ç½‘ç»œä¸­çš„æ¢¯åº¦çˆ†ç‚¸
6. **æ ‡ç­¾å¹³æ»‘** - æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢è¿‡åº¦è‡ªä¿¡
7. **æ®‹å·®è¿æ¥** - æ”¹å–„æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æµåŠ¨
8. **AdamWä¼˜åŒ–å™¨** - å¸¦æƒé‡è¡°å‡çš„æ›´å¥½æ­£åˆ™åŒ–

**è¯¦è§ [REFACTORING_SUMMARY.md](REFACTORING_SUMMARY.md) è·å–å®Œæ•´ç»†èŠ‚ã€‚**

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### æ–¹å¼ 1ï¼šKaggle ç¬”è®°æœ¬ï¼ˆæœ€ç®€å•ï¼‰

1. å°† `semg_hht_cnn_classifier.ipynb` ä¸Šä¼ åˆ° Kaggle
2. æ·»åŠ  **HILBERTMATRIX_NPZ** æ•°æ®é›†åˆ°ç¬”è®°æœ¬
3. å¯ç”¨ GPU åŠ é€Ÿå™¨ï¼ˆè®¾ç½® â†’ åŠ é€Ÿå™¨ â†’ GPUï¼‰
4. è¿è¡Œæ‰€æœ‰å•å…ƒæ ¼

ç¬”è®°æœ¬ä¼šè‡ªåŠ¨æ£€æµ‹ Kaggle ç¯å¢ƒå¹¶ä» `/kaggle/input/hilbertmatrix-npz/hht_matrices/` åŠ è½½æ•°æ®ã€‚

#### æ–¹å¼ 2ï¼šå‘½ä»¤è¡Œè®­ç»ƒï¼ˆæœ¬åœ°/æœåŠ¡å™¨ï¼‰

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# ä½¿ç”¨æ‚¨çš„æ•°æ®è®­ç»ƒï¼ˆæ–°çš„åŒåˆ†ç±»å™¨ç³»ç»Ÿï¼‰
python train.py --data_dir ./data --checkpoint_dir ./checkpoints --epochs 100

# ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°å’Œå¯é…ç½®ç½‘ç»œæ·±åº¦çš„é«˜çº§è®­ç»ƒ
python train.py \
    --data_dir ./data \
    --checkpoint_dir ./checkpoints \
    --epochs 100 \
    --batch_size 16 \
    --learning_rate 0.001 \
    --num_encoder_layers 5 \
    --base_channels 64 \
    --test_size 0.2

# ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œè®­ç»ƒï¼ˆ7å±‚ï¼Œæ›´å¤šç‰¹å¾ï¼‰
python train.py \
    --data_dir ./data \
    --num_encoder_layers 7 \
    --base_channels 64 \
    --epochs 100

# ä½¿ç”¨æ›´æµ…çš„ç½‘ç»œè®­ç»ƒï¼ˆ3å±‚ï¼Œæ›´å¿«è®­ç»ƒï¼‰
python train.py \
    --data_dir ./data \
    --num_encoder_layers 3 \
    --base_channels 64 \
    --epochs 100

# ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
python train.py --data_dir ./data --checkpoint_dir ./checkpoints --resume

# è¿è¡Œæ¨ç†
python inference.py --checkpoint ./checkpoints/final --input ./new_data/
```

**æ–°å‚æ•°ï¼š**
- `--num_encoder_layers`: CNNå±‚æ•°ï¼ˆ1-8ï¼Œé»˜è®¤ï¼š5ï¼‰
  - æ›´å¤šå±‚ = æ›´æ·±ç½‘ç»œï¼Œæ›´å¥½çš„ç‰¹å¾æå–ï¼Œè®­ç»ƒæ›´æ…¢
  - æ›´å°‘å±‚ = è®­ç»ƒæ›´å¿«ï¼Œå†…å­˜ä½¿ç”¨æ›´å°‘
- `--base_channels`: åŸºç¡€é€šé“æ•°ï¼ˆé»˜è®¤ï¼š64ï¼‰
  - æ›´é«˜çš„å€¼ = æ›´å¤šå‚æ•°ï¼Œæ›´å¥½çš„å®¹é‡

è¯¦ç»†è¯´æ˜è¯·å‚è§ [åŒåˆ†ç±»å™¨ç³»ç»ŸæŒ‡å—](DUAL_CLASSIFIER_GUIDE.md)ã€‚

### ğŸ“ æ•°æ®æ ¼å¼

**æ–‡ä»¶å‘½åè§„èŒƒï¼š**
```
è‚Œè‚‰åç§°_åŠ¨ä½œç±»å‹_æ€§åˆ«_ç¼–å·.npz
```

ç¤ºä¾‹ï¼š
- `BICEPS_fatiguetest_M_006.npz` â†’ ç”·æ€§ï¼Œå®Œæ•´åŠ¨ä½œ
- `TRICEPS_half_F_012.npz` â†’ å¥³æ€§ï¼ŒåŠç¨‹åŠ¨ä½œ
- `FOREARM_invalid_M_003.npz` â†’ ç”·æ€§ï¼Œæ— æ•ˆåŠ¨ä½œ
- `Test1_1_015.npz` â†’ æœªæ ‡æ³¨æµ‹è¯•æ–‡ä»¶ï¼ˆä»¥ "Test" å¼€å¤´ï¼‰

**æ–‡ä»¶å†…å®¹ï¼š**
æ¯ä¸ª `.npz` æ–‡ä»¶åŒ…å«ä¸€ä¸ª 256Ã—256 çš„ HHT çŸ©é˜µï¼Œä½¿ç”¨é”® `'hht'` å­˜å‚¨ã€‚

### ğŸ”§ æ¨¡å—åŠŸèƒ½

**1. `train.py` - ç”Ÿäº§è®­ç»ƒè„šæœ¬**
- ä»ç›®å½•åŠ è½½ .npz æ–‡ä»¶
- è§£ææ–‡ä»¶åæå–æ€§åˆ«å’ŒåŠ¨ä½œæ ‡ç­¾
- è®­ç»ƒ CNN ç¼–ç å™¨æå–ç‰¹å¾
- åœ¨æå–çš„ç‰¹å¾ä¸Šè®­ç»ƒ SVM åˆ†ç±»å™¨
- ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆç¼–ç å™¨ã€ç¼©æ”¾å™¨ã€SVMã€å…ƒæ•°æ®ï¼‰
- åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
- å¯¹æµ‹è¯•æ–‡ä»¶ï¼ˆä»¥ "Test" å¼€å¤´çš„æ–‡ä»¶ï¼‰è¿è¡Œæ¨ç†

**2. `inference.py` - æ¨ç†è„šæœ¬**
- ä»æ£€æŸ¥ç‚¹åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
- å¤„ç†å•ä¸ªæ–‡ä»¶æˆ–æ‰¹é‡æ–‡ä»¶
- è¾“å‡ºé¢„æµ‹ç»“æœå’Œç½®ä¿¡åº¦åˆ†æ•°
- å°†ç»“æœä¿å­˜ä¸º JSON

**3. `generate_sample_data.py` - æ•°æ®ç”Ÿæˆå™¨**
- åˆ›å»ºç”¨äºæµ‹è¯•çš„åˆæˆ 256Ã—256 HHT çŸ©é˜µ
- ç”Ÿæˆæ­£ç¡®çš„æ–‡ä»¶åæ ¼å¼
- ç”¨äºæµ‹è¯•æµç¨‹

**4. Jupyter ç¬”è®°æœ¬ `semg_hht_cnn_classifier.ipynb`**
- äº¤äº’å¼æ¢ç´¢å’Œå¯è§†åŒ–
- ä¸ Kaggle æ•°æ®é›†é›†æˆ
- åˆ†æ­¥è®­ç»ƒå·¥ä½œæµç¨‹
- é€‚åˆå®éªŒ

### ğŸ“ˆ è®­ç»ƒæµç¨‹

1. **æ•°æ®åŠ è½½**ï¼šåŠ è½½æ‰€æœ‰ .npz æ–‡ä»¶ï¼Œè¿‡æ»¤æµ‹è¯•æ–‡ä»¶ï¼ˆä»¥ "Test" å¼€å¤´ï¼‰
2. **ç‰¹å¾æå–**ï¼šCNN ç¼–ç å™¨å¤„ç† 256Ã—256 çŸ©é˜µ â†’ 256 ç»´å‘é‡
3. **å½’ä¸€åŒ–**ï¼šStandardScaler å½’ä¸€åŒ–ç‰¹å¾
4. **SVM è®­ç»ƒ**ï¼šRBF æ ¸ SVM åœ¨å½’ä¸€åŒ–ç‰¹å¾ä¸Šè®­ç»ƒ
5. **éªŒè¯**ï¼šè®¡ç®—å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1 åˆ†æ•°
6. **æµ‹è¯•æ¨ç†**ï¼šé¢„æµ‹æµ‹è¯•æ–‡ä»¶çš„æ ‡ç­¾
7. **ä¿å­˜æ£€æŸ¥ç‚¹**ï¼šä¿å­˜å®Œæ•´æ¨¡å‹çŠ¶æ€

---

## ğŸ“š Additional Resources | å…¶ä»–èµ„æº

- **Detailed Training Guide | è¯¦ç»†è®­ç»ƒæŒ‡å—**: [TRAINING_GUIDE.md](TRAINING_GUIDE.md)
- **Scripts Documentation | è„šæœ¬æ–‡æ¡£**: [SCRIPTS_README.md](SCRIPTS_README.md)
- **Example Workflow | ç¤ºä¾‹å·¥ä½œæµ**: [example_workflow.sh](example_workflow.sh)

## ğŸ“„ License | è®¸å¯è¯

MIT License - See main repository for details.
MIT è®¸å¯è¯ - è¯¦è§ä¸»ä»“åº“ã€‚

## ğŸ¤ Contributing | è´¡çŒ®

Contributions are welcome! Please maintain the CNN architecture and update documentation.
æ¬¢è¿è´¡çŒ®ï¼è¯·ä¿æŒ CNN æ¶æ„ä¸å˜å¹¶æ›´æ–°æ–‡æ¡£ã€‚
